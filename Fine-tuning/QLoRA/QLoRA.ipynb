{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRAについて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリの最新バージョンをインストール\n",
    "%pip install -q -U bitsandbytes\n",
    "%pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "%pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "%pip install -q -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. bitsandbytes\n",
    "\n",
    "```python\n",
    "%pip install -q -U bitsandbytes\n",
    "```\n",
    "\n",
    "- 概要:\n",
    "- 計算を高速化するためのライブラリで. 特にモデルの量子化（少ないビットでモデルの計算を行う手法）をサポートします. \n",
    "- メモリ消費量を減らしながら. 高速かつ効率的にモデルを動かすことができます. \n",
    "- 用途:\n",
    "- 4-bit や 8-bit 量子化のサポート. \n",
    "- GPU メモリの節約に役立ちます. \n",
    "\n",
    "2. transformers\n",
    "\n",
    "```python\n",
    "%pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "```\n",
    "\n",
    "- 概要:\n",
    "- Hugging Face が提供するライブラリで. さまざまな最先端の自然言語処理（NLP）モデルを簡単に利用できます. \n",
    "- GPT. BERT. OPT などのモデルを扱う際に必要です. \n",
    "- 用途:\n",
    "- モデルのロード. トークナイザーの適用. 学習や推論の実行. \n",
    "- ポイント:\n",
    "- git+https://github.com/... と記載されているのは. GitHubの最新コードを直接取得してインストールするためです. \n",
    "\n",
    "3. peft\n",
    "\n",
    "```python\n",
    "%pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "```\n",
    "\n",
    "- 概要:\n",
    "- Parameter-Efficient Fine-Tuning（PEFT）をサポートするライブラリです. \n",
    "- モデル全体を再学習するのではなく. 一部のパラメータだけを調整することで. 効率的にモデルの微調整を可能にします. \n",
    "- 用途:\n",
    "- QLoRA（量子化を活用した効率的な微調整）や他の効率的な微調整手法を使用. \n",
    "\n",
    "4. accelerate\n",
    "\n",
    "```python\n",
    "%pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "```\n",
    "\n",
    "- 概要:\n",
    "- 分散処理やマルチGPU環境での計算を簡単に行うためのライブラリ. \n",
    "- モデルのトレーニングや推論を高速化する. \n",
    "- 用途:\n",
    "- GPUやTPUのリソースを効果的に活用して. トレーニングや推論のパフォーマンスを向上. \n",
    "\n",
    "コマンドの解説\n",
    "\n",
    "- %pip: Jupyter Notebook や Google Colab で pip を実行するためのマジックコマンド. \n",
    "- -q: 実行ログを抑制して. インストール状況を簡潔に表示. \n",
    "- -U: ライブラリを「アップデート」するオプション. 既にインストールされている場合でも最新バージョンに更新. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Hugging Faceのtransformersライブラリから. 事前学習済みの\"facebook/opt-350m\"モデルをロード\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\". load_in_4bit=True. device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ライブラリのインポート\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "```\n",
    "\n",
    "- transformers ライブラリ:\n",
    "- 機械学習モデル（特にNLPモデル）を簡単にロード・実行できるライブラリ. \n",
    "- AutoModelForCausalLM は「因果言語モデル」（Causal Language Model）をロードするためのクラスです. \n",
    "\n",
    "2. モデルのロード\n",
    "\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"facebook/opt-350m\",\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "- from_pretrained():\n",
    "- Hugging Face モデルハブから. 事前学習済みモデルをロードします. \n",
    "- \"facebook/opt-350m\" は. Facebook が提供する OPT モデルの 350M パラメータ版. \n",
    "- 引数の詳細:\n",
    "\t1.\t\"facebook/opt-350m\":\n",
    "- ロードするモデル名（またはローカルディレクトリのパス）. \n",
    "- このモデルは. 因果言語モデルとして設計されており. 次に続く単語や文章を予測するタスクに適しています. \n",
    "\t2.\tload_in_4bit=True:\n",
    "- モデルのパラメータを 4bit量子化 してロードします. \n",
    "- モデルのサイズを小さくし. メモリ使用量を大幅に削減することで. 低スペックなGPUでもモデルを扱えるようにします. \n",
    "- 量子化により. 若干の性能低下（精度の劣化）が発生する可能性がありますが. 計算効率が向上します. \n",
    "\t3.\tdevice_map=\"auto\":\n",
    "- デバイスマッピング（CPU. GPU. TPU などの選択）を自動で設定します. \n",
    "- 使用可能なデバイス（たとえばGPU）を探し. 適切に割り当てます. \n",
    "- GPUがある環境では. 自動的にGPUにモデルを配置し. 計算を高速化します. \n",
    "\n",
    "OPT モデルについて\n",
    "\n",
    "- OPT (Open Pretrained Transformer):\n",
    "- Facebook（現Meta）が開発した. 大規模な自然言語処理モデル. \n",
    "- GPT-3に匹敵する性能を持ちつつ. 軽量化と効率化を意識して設計されています. \n",
    "- OPT-350M:\n",
    "- OPTシリーズの中で. 350M（350百万）パラメータを持つ小規模版. \n",
    "- 軽量かつ適度な性能を持つため. リソースの限られた環境で使いやすいモデル. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer. AutoModelForCausalLM\n",
    "\n",
    "# モデルとトークナイザーのロード\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\". load_in_4bit=True. device_map=\"auto\")\n",
    "\n",
    "# 入力の準備\n",
    "# 質問を入力\n",
    "input_text = \"What is QLoRA?\"\n",
    "# 入力をトークン化し. GPUに転送\n",
    "input_ids = tokenizer(input_text. return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "# モデルを使ってテキスト生成\n",
    "# 最大長50でテキスト生成\n",
    "outputs = model.generate(input_ids. max_length=50)\n",
    "# 生成されたトークンを元に戻す\n",
    "generated_text = tokenizer.decode(outputs[0]. skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 必要なライブラリとモジュールのインポート\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer. AutoModelForCausalLM\n",
    "```\n",
    "\n",
    "- AutoTokenizer: テキストをトークン化（モデルが理解できる形式に変換）したり. トークンを元のテキストに戻すためのクラス. \n",
    "- AutoModelForCausalLM: 因果言語モデル（Causal Language Model）を扱うためのクラス. \n",
    "\n",
    "2. モデルとトークナイザーのロード\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\". load_in_4bit=True. device_map=\"auto\")\n",
    "```\n",
    "\n",
    "- トークナイザー:\n",
    "- \"facebook/opt-350m\" 用のトークナイザーをロードします. \n",
    "- テキストをモデルが理解可能なトークン（数値）に変換するために必要です. \n",
    "- モデル:\n",
    "- OPT-350M モデルをロードします. \n",
    "- 4bit量子化を使用してメモリ効率を向上させ. デバイスの自動割り当て（device_map=\"auto\"） を行います. \n",
    "\n",
    "3. 入力テキストの準備\n",
    "\n",
    "```python\n",
    "input_text = \"What is QLoRA?\"\n",
    "input_ids = tokenizer(input_text. return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "```\n",
    "\n",
    "- 入力テキスト:\n",
    "- ユーザーが与える質問やプロンプト（この場合は “What is QLoRA?”）. \n",
    "- tokenizer():\n",
    "- テキストをトークン化し. PyTorchテンソル形式（return_tensors=\"pt\"）で出力. \n",
    "- .to(\"cuda\"): トークン化したデータを GPU（CUDAデバイス） に転送して. 計算を高速化. \n",
    "\n",
    "4. モデルを使ったテキスト生成\n",
    "\n",
    "```python\n",
    "outputs = model.generate(input_ids. max_length=50)\n",
    "```\n",
    "\n",
    "- model.generate():\n",
    "- モデルを使って. 次の単語や文章を予測し. テキストを生成します. \n",
    "- 引数:\n",
    "- input_ids: トークナイザーによって変換された入力. \n",
    "- max_length=50: 生成されるテキストの最大トークン数（この例では最大50トークン）. \n",
    "- モデルは. 入力に続く最も可能性の高い単語を選びながらテキストを生成します. \n",
    "\n",
    "5. 生成されたトークンをテキストに戻す\n",
    "\n",
    "```python\n",
    "generated_text = tokenizer.decode(outputs[0]. skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "- tokenizer.decode():\n",
    "- モデルが出力したトークン（数値列）を元のテキストに戻します. \n",
    "- skip_special_tokens=True: 特殊トークン（例: <EOS>）を省略して読みやすい形式にします. \n",
    "\n",
    "6. 結果を表示\n",
    "\n",
    "```python\n",
    "print(generated_text)\n",
    "```\n",
    "\n",
    "- 生成されたテキストを表示します. \n",
    "\n",
    "このコードの流れ\n",
    "\n",
    "1.\tモデルとトークナイザーのロード: Hugging Face から事前学習済みモデルをロード. \n",
    "2.\t入力テキストを準備: トークン化し. GPUに転送. \n",
    "3.\tテキスト生成: 入力に基づいてモデルが続きのテキストを生成. \n",
    "4.\t生成テキストを復元: モデルの出力を人間が読める形式に変換. \n",
    "5.\t結果を表示: 生成された回答やテキストを出力. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q markupsafe==2.0.1\n",
    "%pip install -q --upgrade gradio\n",
    "%pip install -q transformers datasets peft\n",
    "%pip install -q --upgrade peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 安全な文字列処理ライブラリ markupsafe のインストール\n",
    "\n",
    "```python\n",
    "%pip install -q markupsafe==2.0.1\n",
    "```\n",
    "\n",
    "- markupsafe:\n",
    "- Flask や Jinja2 などのウェブ関連ライブラリで使用される文字列操作ライブラリ. \n",
    "- 安全にHTMLやXMLにエスケープされた文字列を生成する機能を提供します. \n",
    "- ==2.0.1:\n",
    "- バージョンを2.0.1に固定してインストールします. 特定のライブラリの互換性を保つために使用されることが多いです. \n",
    "\n",
    "2. Gradioのアップグレード\n",
    "\n",
    "```python\n",
    "%pip install -q --upgrade gradio\n",
    "```\n",
    "\n",
    "- gradio:\n",
    "- 機械学習モデルを簡単にウェブアプリ化できるライブラリ. \n",
    "- ユーザーインターフェースを作成し. モデルのデモを素早く構築できます. \n",
    "- --upgrade:\n",
    "- Gradioを最新バージョンにアップグレードします. \n",
    "- 使用例:\n",
    "- 画像生成モデルやテキスト生成モデルのインターフェース作成. \n",
    "- ウェブブラウザ経由でモデルをテストするためのUI構築. \n",
    "\n",
    "3. Transformers. Datasets. PEFT のインストール\n",
    "\n",
    "```python\n",
    "%pip install -q transformers datasets peft\n",
    "```\n",
    "\n",
    "- transformers:\n",
    "- Hugging Face が提供するライブラリで. BERT. GPT. OPT など多数の事前学習済みモデルをサポート. \n",
    "- 用途: テキスト生成. 分類. 翻訳など. \n",
    "- datasets:\n",
    "- Hugging Face のデータセットライブラリ. \n",
    "- 数百種類の自然言語処理（NLP）データセットを簡単に利用可能. \n",
    "- 用途: データのロード. 前処理. データ拡張など. \n",
    "- peft:\n",
    "- Parameter-Efficient Fine-Tuning の略. \n",
    "- 大規模言語モデル（LLM）の微調整を効率化するためのライブラリ. \n",
    "- 低リソース環境やカスタマイズタスクで有効. \n",
    "\n",
    "4. PEFTのアップグレード\n",
    "\n",
    "```python\n",
    "%pip install -q --upgrade peft\n",
    "```\n",
    "\n",
    "- --upgrade:\n",
    "- PEFTライブラリを最新バージョンにアップデートします. \n",
    "- モデルの微調整機能が最新の状態で利用可能になります. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "#from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pandasのインポート\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "- Pandas:\n",
    "- データ解析用ライブラリ. \n",
    "- 用途:\n",
    "- CSVやExcelなどのデータファイルを読み書きする. \n",
    "- データの前処理（フィルタリング. 集計. 欠損値処理など）を行う. \n",
    "\n",
    "2. PyTorchのインポート\n",
    "\n",
    "```python\n",
    "import torch\n",
    "```\n",
    "\n",
    "- PyTorch:\n",
    "- 機械学習モデルの構築やトレーニングをサポートするライブラリ. \n",
    "- Hugging Face Transformersのモデルは内部的にPyTorchを使用します. \n",
    "\n",
    "3. Transformers関連のインポート\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM. AutoTokenizer. Trainer. TrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "```\n",
    "\n",
    "- AutoModelForCausalLM:\n",
    "- 自然言語生成（Causal Language Modeling）タスク用の事前学習済みモデルをロードするためのクラス. \n",
    "- 例: GPT系モデル（GPT-3. GPT-Neoなど）. \n",
    "- AutoTokenizer:\n",
    "- モデルに対応するトークナイザー（テキストをトークンに変換するツール）を自動で取得します. \n",
    "- Trainer:\n",
    "- モデルのトレーニング（微調整）を簡単に実行するためのラッパー. \n",
    "- バッチ処理. 学習率スケジューリング. 評価機能などを備えています. \n",
    "- TrainingArguments:\n",
    "- トレーニングプロセスの設定（エポック数. 学習率. バッチサイズなど）を指定するためのクラス. \n",
    "- DataCollatorForSeq2Seq:\n",
    "- シーケンス生成タスクにおけるデータの前処理をサポート. \n",
    "- ミニバッチ作成時にパディングやトークンの整列を自動で行います. \n",
    "\n",
    "4. Datasetsライブラリのインポート\n",
    "\n",
    "```python\n",
    "from datasets import Dataset\n",
    "```\n",
    "\n",
    "- Datasets:\n",
    "- データセットの読み込み. 前処理. 分割を簡単に行うためのHugging Faceライブラリ. \n",
    "- Datasetクラスを使用して. カスタムデータや既存のデータセットを簡単に管理可能. \n",
    "\n",
    "5. モデル微調整関連のインポート\n",
    "\n",
    "```python\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig. get_peft_model\n",
    "```\n",
    "\n",
    "- BitsAndBytesConfig:\n",
    "- モデルを効率的にロードするための設定を提供. \n",
    "- 特に. 量子化（quantization）を使用してメモリ使用量を削減する場合に有用. \n",
    "- 例: 4-bit 量子化を設定. \n",
    "- PEFT（Parameter-Efficient Fine-Tuning）関連:\n",
    "- LoraConfig:\n",
    "- LoRA（Low-Rank Adaptation）微調整の設定を指定. \n",
    "- 大規模モデルの一部のパラメータだけを微調整し. 効率的に学習可能. \n",
    "- get_peft_model:\n",
    "- LoRA設定を使用してモデルを微調整可能な状態に変換する関数. \n",
    "\n",
    "6. コメントアウトされている行\n",
    "\n",
    "```python\n",
    "#from peft import LoraConfig. get_peft_model. prepare_model_for_int8_training. BitsAndBytesConfig\n",
    "```\n",
    "\n",
    "- prepare_model_for_int8_training:\n",
    "- モデルを8-bit量子化（低精度演算）でトレーニングする準備を行う関数. \n",
    "- コメントアウトされている理由:\n",
    "- 現時点で必要ない. もしくはこのコードでは未使用. \n",
    "\n",
    "このコードの目的\n",
    "\n",
    "- Hugging Faceライブラリを活用して. 事前学習済みモデルのロードや微調整を行う準備. \n",
    "- 特にLoRAや量子化などを使い. 大規模モデルを効率的に扱うための設定を含む. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルID\n",
    "model_id = \"facebook/opt-350m\"\n",
    "\n",
    "# 4ビット量子化の設定\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# 量子化されたfacebook/opt-350mモデルを読み込む\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id. quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは. Hugging FaceのTransformersライブラリを使用して. facebook/opt-350mモデルを**8ビット量子化（8-bit quantization）**で読み込むプロセスを示しています. 量子化を行うことで. 計算リソースやメモリ使用量を大幅に削減しながら. 大規模言語モデルを扱うことが可能です. \n",
    "\n",
    "コードの詳細解説\n",
    "\n",
    "1. モデルIDの設定\n",
    "\n",
    "```python\n",
    "model_id = \"facebook/opt-350m\"\n",
    "```\n",
    "\n",
    "- model_id:\n",
    "- Hugging Faceのモデルハブに公開されている. 事前学習済みモデルのIDを指定します. \n",
    "- ここでは. Facebookが提供する OPT-350M モデル（350Mパラメータ）を使用します. \n",
    "\n",
    "2. 8ビット量子化の設定\n",
    "\n",
    "```python\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "```\n",
    "\n",
    "- BitsAndBytesConfig:\n",
    "- **量子化（quantization）**や計算効率化の設定を行うクラス. \n",
    "- load_in_8bit=True:\n",
    "- モデルを 8-bit精度 でロードします. \n",
    "- 通常の32-bit浮動小数点数（FP32）に比べてメモリ使用量を大幅に削減. \n",
    "- メリット:\n",
    "- 軽量化: モデルサイズと計算負荷を削減. \n",
    "- 効率化: 同じGPUメモリでより大きなモデルを扱える. \n",
    "\n",
    "3. 量子化されたモデルのロード\n",
    "\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id. quantization_config=quantization_config)\n",
    "```\n",
    "\n",
    "- AutoModelForCausalLM.from_pretrained:\n",
    "- 指定したモデルIDから. Causal Language Modeling（因果言語モデリング）用の事前学習済みモデルをロードします. \n",
    "- 引数:\n",
    "- model_id:\n",
    "- 使用するモデルの名前（ID）. \n",
    "- quantization_config=quantization_config:\n",
    "- 上記で設定した8-bit量子化の設定を適用. \n",
    "- 動作:\n",
    "- モデルパラメータを8-bit量子化した形式でメモリにロード. \n",
    "- GPUやCPUで効率的に推論や微調整を実行. \n",
    "\n",
    "このコードの全体像\n",
    "\n",
    "\t1.\tHugging Faceの facebook/opt-350m モデルを指定. \n",
    "\t2.\tBitsAndBytesConfig を用いて 8-bit量子化の設定を適用. \n",
    "\t3.\t量子化された形式でモデルをロードし. 推論やトレーニングに備える. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "PeftModelForCausalLM(\n",
    "  (base_model): LoraModel(\n",
    "    (model): OPTForCausalLM(\n",
    "      (model): OPTModel(\n",
    "        (decoder): OPTDecoder(\n",
    "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
    "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
    "          (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n",
    "          (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
    "          (layers): ModuleList(\n",
    "            (0-23): 24 x OPTDecoderLayer(\n",
    "              (self_attn): OPTSdpaAttention(\n",
    "                (k_proj): lora.Linear4bit(\n",
    "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
    "                  (lora_dropout): ModuleDict(\n",
    "                    (default): Dropout(p=0.1, inplace=False)\n",
    "                  )\n",
    "                  (lora_A): ModuleDict(\n",
    "                    (default): Linear(in_features=1024, out_features=64, bias=False)\n",
    "                  )\n",
    "                  (lora_B): ModuleDict(\n",
    "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
    "                  )\n",
    "                  (lora_embedding_A): ParameterDict()\n",
    "                  (lora_embedding_B): ParameterDict()\n",
    "                  (lora_magnitude_vector): ModuleDict()\n",
    "                )\n",
    "                (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
    "                (q_proj): lora.Linear4bit(\n",
    "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
    "                  (lora_dropout): ModuleDict(\n",
    "                    (default): Dropout(p=0.1, inplace=False)\n",
    "                  )\n",
    "                  (lora_A): ModuleDict(\n",
    "                    (default): Linear(in_features=1024, out_features=64, bias=False)\n",
    "                  )\n",
    "                  (lora_B): ModuleDict(\n",
    "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
    "                  )\n",
    "                  (lora_embedding_A): ParameterDict()\n",
    "                  (lora_embedding_B): ParameterDict()\n",
    "                  (lora_magnitude_vector): ModuleDict()\n",
    "                )\n",
    "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
    "              )\n",
    "              (activation_fn): ReLU()\n",
    "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
    "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
    "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
    "            )\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "      (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
    "    )\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTForCausalLM アーキテクチャ概要\n",
    "\n",
    "OPTForCausalLM は. Facebookが開発した OPT (Open Pretrained Transformer) モデルファミリーの一部で. 主に 因果言語モデリング (Causal Language Modeling) に使用されます. このアーキテクチャは. 軽量化と効率的な微調整を重視した設計が特徴です. \n",
    "\n",
    "1. 主な目的\n",
    "\n",
    "- 因果言語モデリング: トークン列を入力し. 次に出現するトークンを予測. \n",
    "- 自然言語生成: テキスト生成や会話エージェント. コード補完など. \n",
    "\n",
    "2. アーキテクチャの構成\n",
    "\n",
    "(1) Embedding 層\n",
    "\n",
    "- embed_tokens: トークンを512次元のベクトルに変換. \n",
    "- embed_positions: トークン位置を考慮した2050次元の埋め込み. \n",
    "\n",
    "(2) Decoder\n",
    "\n",
    "- Transformerアーキテクチャ:\n",
    "- デコーダ専用構造で24層の OPTDecoderLayer を含む. \n",
    "- 隠れ層サイズ: 1024\n",
    "- トークン埋め込み次元: 512\n",
    "\n",
    "(3) Decoder Layer (OPTDecoderLayer)\n",
    "\n",
    "\t1.\t注意メカニズム (Self-Attention):\n",
    "- self_attn: Scaled Dot-Product Attention. \n",
    "- LoRA (Low-Rank Adaptation):\n",
    "- 軽量な微調整手法を採用し. k_proj と q_proj のプロジェクションに適用. \n",
    "- パラメータ数を抑えながら効率的なモデル調整を実現. \n",
    "\t2.\t全結合層:\n",
    "- fc1: 入力次元を1024から4096に拡張. \n",
    "- fc2: 中間次元を1024に縮小. \n",
    "\t3.\t正規化:\n",
    "- 注意層後: self_attn_layer_norm. \n",
    "- 出力層後: final_layer_norm. \n",
    "\t4.\t活性化関数:\n",
    "- ReLU: 全結合層の活性化関数として使用. \n",
    "\n",
    "(4) 最後の線形層 (lm_head)\n",
    "\n",
    "- デコーダの出力（512次元）を辞書サイズ（50272次元）に変換. \n",
    "- 各トークンの出現確率を計算. \n",
    "\n",
    "3. 特徴\n",
    "\n",
    "\t1.\tLoRAの採用:\n",
    "- 低ランク分解 (Low-Rank Decomposition) による効率的な微調整. \n",
    "- 計算コストとメモリ消費を削減しつつ. 新しいタスクへの適応が可能. \n",
    "\t2.\t軽量設計:\n",
    "- パラメータ効率を重視し. 大規模モデルでの実用性を向上. \n",
    "\t3.\t柔軟性:\n",
    "- モジュール化された設計により. 特定の層や部分のカスタマイズが可能. \n",
    "\n",
    "4. 用途例\n",
    "\n",
    "- テキスト生成: 自然な文章やストーリー生成. \n",
    "- 対話システム: 会話エージェントやチャットボット. \n",
    "- 微調整タスク: 特定分野（法律. 医療など）のテキスト生成. \n",
    "\n",
    "5. 技術仕様\n",
    "\n",
    "|項目 |\t値 |\n",
    "| --- | --- |\n",
    "| モデル層数 | 24層 |\n",
    "| 隠れ層サイズ | 1024 |\n",
    "| 埋め込み次元 | 512 | \n",
    "| トークン辞書サイズ | 50272 |\n",
    "| 活性化関数 | ReLU |\n",
    "| 使用技術 | LoRA. Scaled Dot-Product Attention |\n",
    "\n",
    "1. 利点\n",
    "\n",
    "- 計算効率: モデル全体を再学習する必要なく. 一部のパラメータで適応が可能. \n",
    "- スケーラビリティ: 大規模モデルにも対応しやすい. \n",
    "- 多用途: 各種生成タスクに対応可能な汎用性. \n",
    "\n",
    "このアーキテクチャは. 効率性と適応性を兼ね備えた設計であり. 大規模モデルの微調整や生成タスクに最適です. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRAの設定\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    target_modules=[\"q_proj\". \"k_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRAの設定についての詳細な説明\n",
    "\n",
    "LoRA（Low-Rank Adaptation）は. 大規模な事前学習済みモデルを効率的に微調整（fine-tuning）する手法です. この手法は. 計算コストやメモリ使用量を削減しながら. 高い性能を達成できるのが特徴です. このコードで指定されている各設定の詳細をさらに掘り下げて説明します. \n",
    "\n",
    "LoRAの基本的な仕組み\n",
    "\n",
    "1.\t通常. 大規模モデルの重み（パラメータ）は非常に大きな行列で構成されます. \n",
    "2.\tLoRAは. その大きな行列を 低ランク分解 という手法で分解します. \n",
    "- 具体的には. 行列 $W$ を. 2つの小さな行列 $A$ と $B$ に分解します. \n",
    "- これにより. 学習するパラメータの数が大幅に減少します. \n",
    "3.\t既存のモデルの重みは固定され. LoRAで学習するのは $A$ と $B$ のみです. \n",
    "4.\tこの方法では. 大規模な行列の更新が不要になり. 計算効率が向上します. \n",
    "\n",
    "コード解説\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    target_modules=[\"q_proj\". \"k_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "```\n",
    "\n",
    "1. lora_alpha=16\n",
    "\n",
    "- 役割:\n",
    "- LoRAによって学習された行列 ￼ と ￼ の出力をスケーリングするための係数. \n",
    "- 学習の安定性を確保し. 微調整の影響を適切にモデル全体に反映させるために使用します. \n",
    "- 動作の詳細:\n",
    "- 出力結果は以下のようにスケーリングされます：\n",
    "$W^{\\prime} = W + \\alpha \\cdot (A \\cdot B)$\n",
    "\n",
    "￼\n",
    "- $W^{\\prime}$￼: 元の重み（事前学習済みの値. 固定）. \n",
    "- $(A \\cdot B)$￼: LoRAで学習した低ランク行列の積. \n",
    "- $\\alpha$￼: スケーリング係数. \n",
    "- 値の選び方:\n",
    "- 一般的には ￼ を大きくするとLoRAの影響が増え. 小さくすると元のモデルの重みが優先されます. \n",
    "\n",
    "1. lora_dropout=0.1\n",
    "\n",
    "- 役割:\n",
    "- 過学習を防ぐためのドロップアウト（Dropout）をLoRA層に導入します. \n",
    "- 動作の詳細:\n",
    "- 学習中. 一定割合（ここでは10%）のユニットを無効化して. 汎化性能（未知のデータへの適応能力）を向上させます. \n",
    "- ドロップアウトはランダムに適用されるため. 毎回異なるニューロンが無効化されます. \n",
    "- 適切な値:\n",
    "- 通常は 0.1 ～ 0.3 の範囲で設定されます. \n",
    "\n",
    "2. r=64\n",
    "\n",
    "- 役割:\n",
    "- LoRAで学習する行列のランク（次元）を指定します. \n",
    "- 動作の詳細:\n",
    "- 元の行列 $W$ を. 以下のように低ランク行列 $A$ と $B$ に分解します：\n",
    "$W \\approx A \\cdot B$\n",
    "\n",
    "￼\n",
    "- $A$￼: サイズは  $\\text{original\\_rows} \\times r$ ￼. \n",
    "- $B$￼: サイズは $ r \\times \\text{original\\_columns} $￼. \n",
    "- ここで. $r$ が小さいほどパラメータ数が減少し. 計算効率が上がります. \n",
    "- 選び方:\n",
    "- 小さい $r$ は計算量を削減しますが. モデルの表現力が制限される可能性があります. \n",
    "- 大規模モデルでは $r$ を 32 ～ 128 程度に設定するのが一般的です. \n",
    "\n",
    "1. target_modules=[\"q_proj\". \"k_proj\"]\n",
    "\n",
    "- 役割:\n",
    "- Transformerモデル内でLoRAを適用する層を指定します. \n",
    "- 動作の詳細:\n",
    "- ここでは q_proj（Queryプロジェクション）と k_proj（Keyプロジェクション）の層をターゲットにしています. \n",
    "- これらはAttention機構の計算に重要な役割を果たす部分で. ここにLoRAを適用することでモデル全体の性能に大きな影響を与えられます. \n",
    "\n",
    "2. bias=\"none\"\n",
    "\n",
    "- 役割:\n",
    "- バイアス項（bias）をLoRAで学習するかどうかを指定します. \n",
    "- 設定の詳細:\n",
    "- \"none\":\n",
    "- バイアス項は固定され. 学習されません. \n",
    "- 他のオプション：\n",
    "- \"all\": バイアス項を含めて学習. \n",
    "- \"lora_only\": LoRA部分のみにバイアスを学習. \n",
    "\n",
    "3. task_type=\"CAUSAL_LM\"\n",
    "\n",
    "- 役割:\n",
    "- LoRAを適用するタスクの種類を指定します. \n",
    "- 詳細:\n",
    "- \"CAUSAL_LM\" は因果言語モデル（Causal Language Modeling）の略で. 次の単語を予測するタスクを指します. \n",
    "- 具体例：\n",
    "- 入力：「The cat sat on the」\n",
    "- 出力：「mat.」\n",
    "- GPTやOPTのようなモデルがこのタスクに使用されます. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target_modules=[\"q_proj\". \"k_proj\"] は, LoRA (Low-Rank Adaptation) の適用対象として, 特定のモジュール（層）を指定する設定です. この指定により, LoRA はモデルの一部に限定して適用されます. これを理解するには, まず以下の背景を押さえる必要があります. \n",
    "\n",
    "背景: Transformerと自己注意機構\n",
    "\n",
    "Transformerモデル（例: GPT, OPT）では, 自己注意機構 (Self-Attention) が重要な役割を果たします. この自己注意機構は以下の3つの行列で構成されます：\n",
    "\n",
    "1.\tQuery (q_proj):\n",
    "- 入力特徴量を「質問 (query)」ベクトルに変換. \n",
    "- どの部分が重要かを問う役割を持つ. \n",
    "2.\tKey (k_proj):\n",
    "- 入力特徴量を「鍵 (key)」ベクトルに変換. \n",
    "- 各トークンがどれだけ関連性を持つかを示す役割を持つ. \n",
    "3.\tValue (v_proj):\n",
    "- 入力特徴量を「値 (value)」ベクトルに変換. \n",
    "- 実際に出力に反映される情報を保持. \n",
    "\n",
    "q_proj. k_proj. v_proj はそれぞれ Linear レイヤー（全結合層）で実装されています. \n",
    "\n",
    "LoRAの概要\n",
    "\n",
    "LoRA は, 大きなモデルを効率的に微調整するための手法で, 特定のパラメータ（モジュール）に低ランク行列を追加して学習します. この低ランク行列によるパラメータ追加により, 以下が可能になります：\n",
    "\n",
    "1.\t計算負荷の削減:\n",
    "- モデル全体ではなく, 選択された層（部分的なパラメータ）にのみ変更を加える. \n",
    "2.\tモデルの効率的な更新:\n",
    "- モデル全体を再学習せずに特定の部分を微調整できる. \n",
    "\n",
    "target_modules=[\"q_proj\". \"k_proj\"]\n",
    "\n",
    "この指定では, 自己注意機構の以下の部分をLoRAで微調整対象にすることを意味します：\n",
    "\n",
    "1.\tq_proj (Query projection):\n",
    "- モデルが「どの情報に注目するべきか」を決定する部分. \n",
    "2.\tk_proj (Key projection):\n",
    "- モデルが「どの情報が関連するか」を評価する部分. \n",
    "\n",
    "この設定により, 自己注意機構全体ではなく, q_projとk_projに限定してLoRAの適用が行われます. これには以下の利点があります：\n",
    "\n",
    "利点\n",
    "\n",
    "1.\tパラメータの効率化:\n",
    "- モデル全体を調整せず, 重要な部分（自己注意のQueryとKey部分）に焦点を当てる. \n",
    "2.\t学習時間の短縮:\n",
    "- LoRAの調整対象を限定することで, 計算負荷が軽減される. \n",
    "3.\t性能の維持:\n",
    "- Valueやその他の層を固定するため, 元のモデルの性能を損なうリスクが小さい. \n",
    "\n",
    "全体像\n",
    "\n",
    "以下に, この設定を視覚的に整理します：\n",
    "\n",
    "```plaintext\n",
    "Transformer内の自己注意機構：\n",
    "------------------------------------------------\n",
    "[Input] --> q_proj (調整対象: LoRA) --> Query\n",
    "         --> k_proj (調整対象: LoRA) --> Key\n",
    "         --> v_proj (固定)          --> Value\n",
    "[Output] <-- Attention計算\n",
    "```\n",
    "\n",
    "LoRAを適用することで, q_proj と k_proj に対して低ランク行列を導入し, これらの部分の微調整を効率化します. \n",
    "\n",
    "まとめ\n",
    "\n",
    "- target_modules の役割: LoRAの適用対象となるモジュールを指定します. \n",
    "- q_proj と k_proj を選択する理由: 自己注意機構の中でも, 特に情報の選択や関連性評価に関わる部分であり, モデルの応答生成に大きく影響するからです. \n",
    "\n",
    "これにより, 少ないリソースで高性能な微調整が可能になります. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRAアダプタをモデルに適用\n",
    "model = get_peft_model(model. lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRAアダプタをモデルに適用\n",
    "```python\n",
    "model = get_peft_model(model. lora_config)\n",
    "```\n",
    "\n",
    "1. コードの全体的な概要\n",
    "\n",
    "このコードは. 既存の大規模言語モデルにLoRA（Low-Rank Adaptation）を適用する処理です. \n",
    "\n",
    "- get_peft_model 関数を使用して. 事前学習済みモデル（model）に LoRAアダプタ を組み込みます. \n",
    "- これにより. 元のモデルのパラメータを固定しながら. LoRAのパラメータのみを学習可能な形に変換します. \n",
    "\n",
    "2. 各要素の説明\n",
    "\n",
    "(1) get_peft_model\n",
    "\n",
    "- 役割:\n",
    "- LoRAアダプタをモデルに注入（integrate）し. モデルの一部にLoRAを適用可能にします. \n",
    "- この関数は. 元のモデル構造を変更するのではなく. LoRA用の追加パラメータを一部の層に追加します. \n",
    "- 具体的な動作:\n",
    "- target_modules（例: [\"q_proj\". \"k_proj\"]）で指定された部分にLoRAアダプタを適用. \n",
    "- 元のモデルの重みは固定され. LoRAで導入したパラメータ（低ランク行列 ￼ と ￼ ）のみが学習対象になります. \n",
    "\n",
    "(2) model\n",
    "\n",
    "- 元のモデル:\n",
    "- ここでは. 事前にロード済みの言語モデル（例: facebook/opt-350m）. \n",
    "- AutoModelForCausalLM などで取得されたPyTorchモデル. \n",
    "- LoRA適用後のモデル:\n",
    "- get_peft_model を適用すると. モデルの一部がLoRA用の層に置き換えられます. \n",
    "- モデル構造はLoRAをサポートする形に変更されますが. 基本的な動作（例: テキスト生成）は維持されます. \n",
    "\n",
    "(3) lora_config\n",
    "\n",
    "- LoRAの設定:\n",
    "- このコードでは. 以下のようなパラメータを持つ設定が事前に用意されています:\n",
    "- r: LoRAで使用する低ランク行列のランク（次元数）. \n",
    "- lora_alpha: LoRA出力をスケーリングする係数. \n",
    "- lora_dropout: ドロップアウト率. \n",
    "- target_modules: LoRAを適用する対象モジュール（例: q_proj. k_proj）. \n",
    "- task_type: モデルのタスクタイプ（例: CAUSAL_LM）. \n",
    "- 適用時の役割:\n",
    "- この設定に基づいて. LoRAの適用範囲や学習対象が決まります. \n",
    "\n",
    "3. LoRA適用後のモデルの特徴\n",
    "\n",
    "\t1.\tパラメータの固定\n",
    "- 元の事前学習済みモデルのパラメータは固定され. 更新されません. \n",
    "- 更新対象はLoRAで導入された追加の低ランク行列 ￼ と ￼ のみです. \n",
    "\t2.\t効率的な学習\n",
    "- 低ランク行列に限定して学習を行うため. 必要なメモリ量や計算負荷が大幅に減少します. \n",
    "- 特に. GPUメモリが制約となる環境で効果的です. \n",
    "\t3.\t動作の互換性\n",
    "- LoRAアダプタを適用しても. 元のモデルのインターフェースやAPIはそのまま利用可能です. \n",
    "- 例えば. model.generate() を使ったテキスト生成などが通常通り実行できます. \n",
    "\n",
    "4. 実行後に確認すべきこと\n",
    "\n",
    "(1) LoRAが適用されたことの確認\n",
    "\n",
    "get_peft_model を適用後. 以下のコードでモデルの設定が正しいか確認できます：\n",
    "\n",
    "# モデルに適用されたLoRAの情報を表示\n",
    "print(model)\n",
    "\n",
    "(2) LoRAで学習可能なパラメータの確認\n",
    "\n",
    "LoRA部分のみが学習対象となっていることを確認するには以下を実行します：\n",
    "\n",
    "# 学習対象のパラメータを表示\n",
    "for name. param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "\n",
    "(3) メモリ消費量の削減\n",
    "\n",
    "LoRAを適用したモデルをロードした際に. 従来よりもメモリ使用量が減少しているかを確認できます. \n",
    "\n",
    "5. 実行の次のステップ\n",
    "\n",
    "LoRAアダプタを適用したモデルを使用して以下のような操作を行います：\n",
    "\n",
    "\t1.\t微調整（Fine-tuning）\n",
    "- LoRAを適用したモデルに新しいデータを用いて学習を行います. \n",
    "\t2.\t推論（Inference）\n",
    "- LoRA付きのモデルを使ってタスク（例: テキスト生成）を実行します. \n",
    "\n",
    "LoRAは. 大規模モデルを効率的に微調整するための強力な手法です. このコードにより. 計算コストを抑えながら高い性能を発揮できるカスタムモデルが作成可能です. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイザーを読み込む\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoTokenizer.from_pretrained(model_id) のコード解説\n",
    "\n",
    "# トークナイザーを読み込む\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "```\n",
    "\n",
    "このコードは. 指定されたモデルの トークナイザー をロードするための処理です. \n",
    "\n",
    "1. トークナイザーとは？\n",
    "\n",
    "トークナイザー（Tokenizer）は. 自然言語処理（NLP）の前処理において重要な役割を果たします. テキストを以下のように処理します：\n",
    "\n",
    "- テキストの分割（トークン化）：テキストを単語. サブワード. または文字単位の「トークン」に分解します. \n",
    "- 数値化：トークンをモデルが理解できる形式（数値のインデックス）に変換します. \n",
    "- 特殊トークンの処理：文章の始まりや終わり. パディング. マスクなどの特殊トークンを追加します. \n",
    "\n",
    "2. AutoTokenizer.from_pretrained(model_id) の役割\n",
    "\n",
    "(1) AutoTokenizer\n",
    "\n",
    "- AutoTokenizer は. Hugging Faceの transformers ライブラリで提供されるクラスで. 事前学習済みモデルに合わせて適切なトークナイザーを自動的に選んでくれます. \n",
    "- AutoTokenizer を使用することで. 事前にモデルに合わせたトークナイザーを手動で選ぶ手間を省くことができます. 例えば. BertTokenizer や GPT2Tokenizer など. モデルに適したトークナイザーが内部で自動的に選ばれます. \n",
    "\n",
    "(2) from_pretrained(model_id)\n",
    "\n",
    "- from_pretrained メソッドは. 指定した model_id（モデルの名前やパス）からトークナイザーの設定をロードします. \n",
    "- model_id には. 事前学習済みモデルの名前（例: \"facebook/opt-350m\"）を指定します. このモデルに合わせて. 対応するトークナイザーも自動的にロードされます. \n",
    "\n",
    "(3) どうして model_id を使うのか？\n",
    "\n",
    "- モデルに関連したトークナイザーの設定や語彙（vocabulary）は. モデルごとに異なります. 例えば. BERT と GPT-2 では. トークン化の方法が異なります（BERTはWordPiece. GPT-2はByte-Pair Encoding）. そのため. モデルを使う際にはそのモデルに最適化されたトークナイザーを使う必要があります. \n",
    "\n",
    "3. 実際にロードされるもの\n",
    "\n",
    "AutoTokenizer.from_pretrained(model_id) を呼び出すと. 以下のようなものがロードされます：\n",
    "\n",
    "- 語彙（vocabulary）: モデルに適した語彙ファイルがロードされます. この語彙ファイルには. モデルが理解できるすべてのトークンが含まれています. \n",
    "- トークナイザー設定: トークン化方法や特殊トークン（例えば. 文の始まりを示す [CLS]. 文の終わりを示す [SEP] など）の設定が含まれます. \n",
    "- 追加の設定（オプション）: モデルに合わせた特殊トークンの設定や. その他の必要なパラメータ（最大長. パディングの方法など）が含まれます. \n",
    "\n",
    "4. 具体的な使用例\n",
    "\n",
    "例えば. facebook/opt-350m モデルの場合. このコードは以下のように動作します：\n",
    "\n",
    "- facebook/opt-350m モデルに適したトークナイザー（例えば. Byte-Pair Encoding 方式）がロードされ. tokenizer 変数に格納されます. \n",
    "- tokenizer はその後. テキストのトークン化や逆トークン化（数値をトークンに戻す）に使用できます. \n",
    "\n",
    "# 例：トークン化と逆トークン化\n",
    "```python\n",
    "input_text = \"What is QLoRA?\"\n",
    "input_ids = tokenizer(input_text. return_tensors=\"pt\").input_ids\n",
    "decoded_text = tokenizer.decode(input_ids[0]. skip_special_tokens=True)\n",
    "\n",
    "print(\"トークン化されたID:\". input_ids)\n",
    "print(\"復元されたテキスト:\". decoded_text)\n",
    "```\n",
    "\n",
    "5. AutoTokenizer を使う利点\n",
    "\n",
    "- 簡潔さ: モデルに合わせたトークナイザーを手動で選ぶ必要がなく. コードがシンプルになります. \n",
    "- 柔軟性: AutoTokenizer は多くのモデルに対応しており. 同じコードで異なるモデルにも適応できます. \n",
    "- 拡張性: 新しいモデルがHugging Faceに追加されるたびに. AutoTokenizer は自動的に新しいモデルのトークナイザーをサポートします. \n",
    "\n",
    "6. model_id とは何か？\n",
    "\n",
    "model_id は. 事前学習済みモデルの識別子であり. 以下のような形式です：\n",
    "\n",
    "- 例: \"facebook/opt-350m\" や \"bert-base-uncased\" など. \n",
    "- 詳細:\n",
    "- model_id はHugging Face Hubで公開されているモデルの名前です. \n",
    "- 自分のローカルファイルシステムに保存されたモデルを指定する場合も. ファイルパスを指定することができます. \n",
    "\n",
    "まとめ\n",
    "\n",
    "このコードの目的は. 指定した model_id に対応するトークナイザーをロードし. テキストをトークン化してモデルが処理できる形式に変換することです. トークナイザーはNLPの前処理に欠かせないものであり. AutoTokenizer を使うことで. 異なるモデル間で簡単にトークナイザーを切り替えることができます. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの準備\n",
    "data = {\n",
    "    \"input_text\": [\"What is QLoRA？\"] * 5,\n",
    "    \"output_text\": [\n",
    "        \"QLoRA is a technique for efficient fine-tuning that enables high-performance model training while reducing memory usage through 4-bit quantization.\",\n",
    "        \"QLoRA. or Quantized Low-Rank Adaptation. is an advanced method that leverages low-rank matrix factorization and quantization for fine-tuning large models efficiently.\",\n",
    "        \"QLoRA is a strategy that combines low-rank adaptation and 4-bit quantization. optimizing the model training process by significantly reducing memory overhead.\",\n",
    "        \"QLoRA enhances the fine-tuning of large models by applying 4-bit quantization and low-rank adaptations. allowing for faster training with reduced computational resources.\",\n",
    "        \"QLoRA facilitates efficient model tuning by incorporating low-rank updates and utilizing 4-bit quantization. making it feasible to fine-tune large models on limited hardware.\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**日本語訳**\n",
    "\n",
    "**1. QLoRA is a technique for efficient fine-tuning that enables high-performance model training while reducing memory usage through 4-bit quantization.**\n",
    "- QLoRAは. 4ビット量子化によるメモリ使用量の削減と高性能モデルのトレーニングを可能にする効率的なファインチューニング手法です.\n",
    "\n",
    "**2. QLoRA. or Quantized Low-Rank Adaptation. is an advanced method that leverages low-rank matrix factorization and quantization for fine-tuning large models efficiently.**\n",
    "- QLoRA（Quantized Low-Rank Adaptation）は. 低ランク行列分解と量子化を活用して. 大規模モデルのファインチューニングを効率的に行うための高度な手法です.\n",
    "\n",
    "**3. QLoRA is a strategy that combines low-rank adaptation and 4-bit quantization. optimizing the model training process by significantly reducing memory overhead.**\n",
    "- QLoRAは. 低ランク適応と4ビット量子化を組み合わせた戦略であり. メモリオーバーヘッドを大幅に削減することでモデルのトレーニングプロセスを最適化します.\n",
    "\n",
    "**4. QLoRA enhances the fine-tuning of large models by applying 4-bit quantization and low-rank adaptations. allowing for faster training with reduced computational resources.**\n",
    "- QLoRAは. 4ビット量子化と低ランク適応を適用することで. 大規模モデルのファインチューニングを強化し. 計算資源を削減しながらより高速なトレーニングを可能にします.\n",
    "\n",
    "**5. QLoRA facilitates efficient model tuning by incorporating low-rank updates and utilizing 4-bit quantization. making it feasible to fine-tune large models on limited hardware.**\n",
    "- QLoRAは. 低ランク更新と4ビット量子化を組み込むことで. 効率的なモデルチューニングを可能にし. 限られたハードウェア上で大規模モデルのファインチューニングを実現します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset.from_dict(data) のコードは. datasets ライブラリを使用して. Pythonの辞書型データ (data) から Dataset オブジェクトを作成するためのものです. これにより. データが適切に整形され. Hugging Faceの datasets ライブラリでのデータ操作やモデル学習に使用できる形式に変換されます. \n",
    "\n",
    "詳細解説\n",
    "\n",
    "```python\n",
    "dataset = Dataset.from_dict(data)\n",
    "```\n",
    "\n",
    "このコードで行われていることを詳しく見ていきましょう. \n",
    "\n",
    "1. Dataset.from_dict(data) メソッド\n",
    "\n",
    "- 目的: Dataset.from_dict は. Pythonの辞書型（dict）データから. Hugging Face datasets ライブラリで使える Dataset オブジェクトを生成するためのメソッドです. \n",
    "- 引数 data: ここで渡される data は. 通常. 辞書型のデータです. この辞書のキーはカラム名に対応し. 値はリストや配列. またはタプルで. データの各サンプルに対応します. 具体的には. 各カラムに対する値（リスト）を持つ辞書を渡す形になります. \n",
    "\n",
    "2. datasets ライブラリ\n",
    "\n",
    "- datasets ライブラリ: Hugging Faceの datasets ライブラリは. 大規模なデータセットを簡単に操作. 処理. 分析できるツールを提供します. これを使うと. データを効率的にロードしたり. 前処理を行ったり. モデルのトレーニングに使ったりできます. \n",
    "\n",
    "3. data の形式\n",
    "\n",
    "- 辞書型データ (dict):\n",
    "- data は通常. 各カラム名（例えば “text”. “label” など）をキーとして持ち. それぞれのカラムに対応する値としてリストやNumPy配列. またはPandasのSeriesなどを指定します. \n",
    "- 例えば. 次のようなデータが data に渡されることが考えられます. \n",
    "\n",
    "```python\n",
    "data = {\n",
    "    \"text\": [\"I love programming\". \"Hugging Face is great\". \"I am learning NLP\"],\n",
    "    \"label\": [1. 0. 1]\n",
    "}\n",
    "```\n",
    "\n",
    "この場合. \"text\" カラムにはテキストデータが. \"label\" カラムにはラベルが含まれており. これを Dataset.from_dict(data) に渡すことで. これらのデータが Dataset オブジェクトとして生成されます. \n",
    "\n",
    "4. Dataset オブジェクト\n",
    "\n",
    "- Dataset は. データを効率的に操作するためのHugging Faceライブラリ内のクラスです. これにより. 例えばデータのスライシング（データの一部を取り出す）. シャッフル（データの順序をランダム化する）. バッチ処理（複数のサンプルをまとめて処理する）などが簡単に行えます. \n",
    "\n",
    "例\n",
    "\n",
    "例えば. 以下のようにテキストとラベルのペアからデータセットを作成する場合：\n",
    "\n",
    "```python\n",
    "from datasets import Dataset\n",
    "\n",
    "# 辞書型データ\n",
    "data = {\n",
    "    \"text\": [\"I love programming\". \"Hugging Face is great\". \"I am learning NLP\"],\n",
    "    \"label\": [1. 0. 1]\n",
    "}\n",
    "\n",
    "# Datasetオブジェクトを作成\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# 結果確認\n",
    "print(dataset)\n",
    "```\n",
    "\n",
    "出力結果:\n",
    "\n",
    "```python\n",
    "Dataset({\n",
    "    features: ['text'. 'label'],\n",
    "    num_rows: 3\n",
    "})\n",
    "```\n",
    "\n",
    "ここで. dataset は Dataset オブジェクトで. text と label という2つのカラムを持つデータセットが作成されます. \n",
    "\n",
    "主な利点\n",
    "\n",
    "- 効率的なデータ操作: datasets ライブラリは. 効率的なデータ処理（データのシャッフル. バッチ処理. フィルタリング. 変換など）を提供します. \n",
    "- メモリ管理: 大規模なデータセットを扱う際にメモリを効率的に使えるように設計されており. メモリに収まりきらないデータもディスクから直接読み込んで処理できます. \n",
    "\n",
    "まとめ\n",
    "\n",
    "Dataset.from_dict(data) は. Pythonの辞書型データからHugging Faceの Dataset オブジェクトを作成する方法です. この方法を使うことで. 辞書で管理されているデータを簡単にデータセットとして利用できるようになります. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットのトークナイズ\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは. dataset.map() メソッドを使用して. データセットの全体に対してトークナイゼーション（トークン化）を適用するためのものです. 具体的に何が行われるのかを. 以下で詳細に説明します. \n",
    "\n",
    "コードの説明\n",
    "\n",
    "```python\n",
    "tokenized_dataset = dataset.map(tokenize_function. batched=True)\n",
    "```\n",
    "\n",
    "1. dataset.map() メソッド\n",
    "\n",
    "- dataset.map() は. datasets ライブラリで提供されるメソッドで. 指定した関数（この場合は tokenize_function）をデータセット全体に適用します. \n",
    "- map() メソッドは. データセットの各サンプルに関数を適用することにより. 元のデータを変換した新しいデータセットを返します. 例えば. トークナイゼーション. データ前処理. フィルタリング. 変換などの処理に使用されます. \n",
    "\n",
    "2. tokenize_function\n",
    "\n",
    "- tokenize_function は. 前のコードで定義された関数です. この関数は. データセットのサンプル（examples）を入力として受け取り. 入力テキスト（input_text）と出力テキスト（output_text）をトークン化します. \n",
    "- トークン化後は. モデルが理解できる形式（input_ids や attention_mask など）に変換されます. \n",
    "\n",
    "3. batched=True\n",
    "\n",
    "- batched=True は. 関数がバッチ単位でデータを処理することを指定します. つまり. データセットの一部をまとめて処理し. 個々のサンプルではなく. 複数のサンプルを一度に関数に渡します. \n",
    "- これにより. トークナイゼーションを効率的に処理できるため. 速度向上が期待できます. batched=True によって. tokenize_function に渡される examples 引数はリストまたは辞書の形式で. 複数のサンプルが一度に処理されます. \n",
    "\n",
    "4. トークナイズされたデータセット\n",
    "\n",
    "dataset.map() の結果として. トークナイゼーションが適用された新しいデータセットが返されます. このデータセットには. 元の入力テキストと出力テキストが. トークン化された形式で格納されています. これにより. 各サンプルのテキストは. モデルの入力として使用可能な形式になります. \n",
    "\n",
    "具体的に. トークナイゼーション後には以下のようなキーが含まれるデータが生成されます（tokenize_function が行っている処理に基づく）:\n",
    "\n",
    "- input_ids: トークン化された入力テキスト. \n",
    "- attention_mask: パディングが追加されている場合. パディング部分を示すマスク. モデルがどのトークンを処理すべきか. または無視すべきかを示します. \n",
    "- labels: 出力テキスト（教師データ）のトークン化結果. 通常. 生成タスクの場合. 出力としてこれが使用されます. \n",
    "\n",
    "5. 処理後のデータセットの利用\n",
    "\n",
    "トークナイズされたデータセット（tokenized_dataset）は. トレーニング. 評価. または推論のために使用できます. このデータセットは. すでにモデルが理解できる形式に変換されているため. すぐに Trainer などのトレーニングツールに渡して使用することができます. \n",
    "\n",
    "まとめ\n",
    "\n",
    "このコードの目的は. dataset 内のすべてのサンプルに対して tokenize_function を適用し. 入力と出力のテキストをトークン化することです. batched=True を使用することで. バッチ処理で効率よくトークナイゼーションを行い. トークン化されたデータセット（tokenized_dataset）を得ることができます. この新しいデータセットは. モデルのトレーニングや推論に使用するために準備されたものです. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データコラトレータ\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer. model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは. トレーニングの際にデータを適切な形式に整えるための データコラトレータ（DataCollatorForSeq2Seq） を作成しています. データコラトレータは. ミニバッチの作成時に入力データを適切にパディングし. バッチ内のサンプルが同じ長さを持つように調整します. これにより. モデルが効率的に学習できるようになります. \n",
    "\n",
    "以下で. このコードの詳細について説明します. \n",
    "\n",
    "コードの構成\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer. model=model)\n",
    "\n",
    "1. DataCollatorForSeq2Seq\n",
    "\n",
    "- DataCollatorForSeq2Seq は. Hugging Face の transformers ライブラリのクラスで. シーケンス生成タスク（例えば. 翻訳. 要約. 質問応答など）のためのデータコラトレータです. このクラスは. トレーニング時に シーケンスデータ（入力文と出力文）を適切な形式に整えるために使用されます. \n",
    "- シーケンスデータは通常. 異なる長さのテキストから成り立っていますが. ミニバッチを作成するには. すべてのサンプルが同じ長さである必要があります. DataCollatorForSeq2Seq は. 以下のような処理を行います：\n",
    "- パディング: 各サンプルを指定された最大長さに合わせてパディングします. これにより. バッチ内のすべてのサンプルが同じ長さになります. \n",
    "- ラベルの調整: 出力シーケンスに関しても同様にパディングを行い. モデルにとって適切な形式になります. \n",
    "\n",
    "2. 引数: tokenizer と model\n",
    "\n",
    "- tokenizer:\n",
    "- tokenizer は. トークン化を担当するオブジェクトで. テキストをトークン化したり. トークンIDに変換したり. 逆にトークンIDをテキストに戻したりする機能を提供します. \n",
    "- DataCollatorForSeq2Seq は. tokenizer を使用して. 入力シーケンスと出力シーケンスをパディングし. トークンIDを正しく扱います. \n",
    "- model:\n",
    "- model 引数は. 使うモデルのインスタンスです. これを渡すことで. データコラトレータは. モデルに適したデータ整形を行うために必要な設定を調整できます. 例えば. 特定のモデルが要求するパディングの方法やその他の設定に従うことができます. \n",
    "\n",
    "3. データコラトレータの役割\n",
    "\n",
    "- DataCollatorForSeq2Seq は. トレーニング中に使用される データのバッチ化 を適切に行います. トークナイズされた入力と出力のペアを受け取り. モデルに適した形に調整します. \n",
    "- パディング: ミニバッチ内の全てのテキストの長さを揃えます. これは. GPUを効率的に使用するために重要です. \n",
    "- バッチ処理: バッチ単位でデータを供給するため. トレーニングが効率的に進みます. \n",
    "\n",
    "4. トレーニング時の利用\n",
    "\n",
    "- この data_collator は. Trainer クラスで使用されることが多いです. Trainer は. データローダー（DataLoader）とともに data_collator を使って. トレーニング時に適切なバッチを生成します. \n",
    "\n",
    "まとめ\n",
    "\n",
    "このコードは. シーケンス生成タスク（例えば. QAや翻訳など）におけるデータの前処理を行うための データコラトレータ を作成しています. DataCollatorForSeq2Seq を使用すると. 入力と出力のシーケンスが異なる長さを持つ場合でも. 効率的にパディングと調整を行い. ミニバッチの作成をサポートします. これにより. トレーニング中にGPUを最大限に活用できるようになります. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習の設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは. Hugging Face の transformers ライブラリで使用する 学習設定（TrainingArguments）を定義しています. TrainingArguments は. トレーニングの設定を行うクラスで. トレーニング時の様々なパラメータを管理します. 以下は各設定の詳細です. \n",
    "\n",
    "コード\n",
    "\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "```\n",
    "\n",
    "各パラメータの詳細\n",
    "\n",
    "1.\toutput_dir\n",
    "- output_dir は. トレーニング結果やモデルの保存先ディレクトリを指定します. トレーニング終了後に. 最終的なモデルやチェックポイントがこのディレクトリに保存されます. \n",
    "- ここでは. \"./results\" と指定されており. トレーニング結果は results フォルダ内に保存されます. \n",
    "2.\tevaluation_strategy\n",
    "- evaluation_strategy は. 評価のタイミングを設定します. 以下のようなオプションがあります：\n",
    "- \"no\": 評価を行わない\n",
    "- \"epoch\": 各エポック終了時に評価を行う\n",
    "- \"steps\": 指定したステップごとに評価を行う\n",
    "- ここでは \"no\" が指定されており. 評価は行われません. \n",
    "3.\tlearning_rate\n",
    "- learning_rate は. 学習率（モデルの重み更新の速さ）を設定します. 1e-4（0.0001）という小さな学習率が指定されています. これにより. 学習が安定しやすくなります. \n",
    "4.\tper_device_train_batch_size\n",
    "- per_device_train_batch_size は. トレーニングの際に各デバイス（GPUなど）で使用するバッチサイズを設定します. ここでは 2 と指定されています. つまり. 各GPUで2つのサンプルを同時に処理します. \n",
    "5.\tnum_train_epochs\n",
    "- num_train_epochs は. モデルのトレーニングを行うエポック数を指定します. エポックは. データセット全体を一回通すことを指します. ここでは 3 回のエポックで学習を行うことが設定されています. \n",
    "6.\tweight_decay\n",
    "- weight_decay は. 重みの減衰（L2正則化）を設定します. これにより. モデルの過学習を防ぐことができます. 0.01 と指定されており. 適度な正則化が行われます. \n",
    "7.\tlogging_dir\n",
    "- logging_dir は. トレーニング中のログ情報を保存するディレクトリを指定します. ここでは './logs' と指定されており. トレーニング中に発生するログがこのディレクトリに保存されます. \n",
    "8.\tlogging_steps\n",
    "- logging_steps は. 何ステップごとにログを記録するかを設定します. ここでは 10 が指定されており. 10ステップごとにログを出力します. \n",
    "9.\tsave_strategy\n",
    "- save_strategy は. モデルのチェックポイントを保存するタイミングを設定します. 以下のオプションがあります：\n",
    "- \"no\": チェックポイントを保存しない\n",
    "- \"epoch\": 各エポック終了時にチェックポイントを保存する\n",
    "- \"steps\": 指定したステップごとにチェックポイントを保存する\n",
    "- ここでは \"epoch\" が指定されており. 各エポックが終了するたびにチェックポイントが保存されます. \n",
    "\n",
    "まとめ\n",
    "\n",
    "このコードは. トレーニングの設定を行うために TrainingArguments を使用しています. 主に次のポイントを設定しています：\n",
    "\n",
    "- 出力先やログ出力先の指定\n",
    "- 学習率やバッチサイズなどの基本的なハイパーパラメータの設定\n",
    "- 評価の頻度やチェックポイントの保存タイミングの設定\n",
    "\n",
    "この設定を使用すると. 指定されたパラメータに従ってトレーニングが実行され. 結果やモデルは指定されたディレクトリに保存されます. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーナーの設定\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは. Hugging Face の transformers ライブラリの Trainer クラスを使用して. モデルのトレーニングの設定を行っています. Trainer は. モデルのトレーニングプロセスを簡単に管理できる高レベルなAPIで. データセットの準備や評価. ログ出力. モデルの保存などを簡素化します. \n",
    "\n",
    "コード\n",
    "```python\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "```\n",
    "\n",
    "各引数の詳細\n",
    "\n",
    "1.\tmodel\n",
    "- model は. トレーニングに使用するモデルです. この例では. 前のコードで定義された model（facebook/opt-350m）が指定されています. Trainer は. 指定されたモデルを使ってトレーニングを行います. \n",
    "2.\targs\n",
    "- args には. トレーニングの設定を指定するために TrainingArguments オブジェクトが渡されます. これにより. バッチサイズや学習率. エポック数など. トレーニングの詳細な設定が提供されます. このコードでは. 前述の training_args が指定されています. \n",
    "3.\ttrain_dataset\n",
    "- train_dataset には. トレーニングに使用するデータセットを指定します. この例では. トークナイズされたデータセット（tokenized_dataset）が指定されています. tokenized_dataset は. 前述のトークナイズ関数を通じて. テキストデータをトークンに変換したデータセットです. \n",
    "4.\tdata_collator\n",
    "- data_collator は. データのバッチ処理を行うためのオブジェクトです. このコードでは. DataCollatorForSeq2Seq を使って. トークン化されたデータをバッチごとに処理しています. data_collator は. バッチ内のデータを適切にまとめ. モデルに渡す形式に変換します. この場合. シーケンスデータを処理するために DataCollatorForSeq2Seq が使用されています. \n",
    "\n",
    "Trainer クラスの主な役割\n",
    "\n",
    "- モデルのトレーニング: Trainer は. 与えられたモデルを指定された設定（TrainingArguments）に基づいてトレーニングします. \n",
    "- 評価: 設定に応じて. トレーニング中にモデルの評価を実行します（ここでは評価は行われない設定になっています）. \n",
    "- ログの管理: トレーニング中のログを指定されたディレクトリ（logging_dir）に記録し. 進行状況を追跡できます. \n",
    "- チェックポイントの保存: 指定された方法でモデルのチェックポイント（中間結果）を保存します. \n",
    "- 学習データのバッチ処理: data_collator によって. データを適切な形式に変換し. モデルに供給する準備をします. \n",
    "\n",
    "まとめ\n",
    "\n",
    "このコードで作成された Trainer インスタンスは. 指定されたモデル. トレーニング設定. データセット. バッチ処理の設定を元に. トレーニングを開始する準備が整います. Trainer を使うことで. トレーニングの流れが簡単に管理でき. 細かい設定や操作を意識することなく. 高レベルなトレーニングプロセスを実行できます. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トレーニングの出力結果\n",
    "この出力は, Hugging FaceのTrainerまたは類似のトレーニングスクリプトを使用した際のトレーニング結果を示しています. それぞれの値がトレーニングプロセスの進行状況や性能指標を表しています. \n",
    "\n",
    "出力の項目の説明\n",
    "\n",
    "1.\tglobal_step: 9\n",
    "- トレーニング中に実行された全体の更新ステップ数（グローバルステップ数）. \n",
    "- モデルが9回のパラメータ更新を行ったことを意味します. \n",
    "2.\ttraining_loss: 12.783854166666666\n",
    "- トレーニング中に記録された平均損失値. \n",
    "- 損失関数（例: クロスエントロピー）が示す値で, 低いほどモデルの予測精度が高いことを意味します. \n",
    "- 値が大きい場合, モデルがまだ学習不足, もしくはハイパーパラメータ（学習率など）を調整する必要がある可能性があります. \n",
    "3.\tmetrics: {...}\n",
    "- トレーニングのパフォーマンス指標を含む辞書形式のデータ. \n",
    "metrics内の各項目\n",
    "\n",
    "- train_runtime: 2.5007\n",
    "- トレーニングに要した総時間（秒）. \n",
    "- train_samples_per_second: 5.998\n",
    "- 1秒あたりに処理されたトレーニングサンプル数. \n",
    "- train_steps_per_second: 3.599\n",
    "- 1秒あたりに実行されたトレーニングステップ数. \n",
    "- 高いほど学習が効率的であることを示します. \n",
    "- total_flos: 3567161180160.0\n",
    "- 総フロップ数 (Floating Point Operations). \n",
    "- モデルがトレーニング中に実行した浮動小数点演算の量を示します. \n",
    "- フロップ数が大きいほど, モデルの計算量が多いことを意味します. \n",
    "- train_loss: 12.783854166666666\n",
    "- 上記のtraining_lossと同じ値. \n",
    "- トレーニングプロセス全体で記録された平均損失値. \n",
    "- epoch: 3.0\n",
    "- トレーニングが完了したエポック数. \n",
    "- データセット全体を3回繰り返してトレーニングを実施したことを意味します. \n",
    "\n",
    "この結果から分かること\n",
    "\n",
    "1.\t損失値 (training_loss)\n",
    "- 現時点の損失値が12.78と比較的高めなので, モデルがまだ最適化の途中である可能性があります. \n",
    "- 学習率の調整やエポック数の増加, データの正規化などを試す価値があります. \n",
    "2.\tトレーニング速度\n",
    "- 秒あたり6サンプル処理（train_samples_per_second）と3.6ステップ実行（train_steps_per_second）は, トレーニングが効率的に進んでいることを示しています. \n",
    "3.\tエポック数\n",
    "- エポック数は3で, データセット全体を3回繰り返してトレーニングした状態です. \n",
    "- 追加のエポックを試すことで損失値がさらに下がる可能性があります. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM. AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 保存したモデルとトークナイザーを読み込む\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"./QLoRA_fine_tuned_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./QLoRA_fine_tuned_model\". local_files_only=True)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"./QLoRA_fine_tuned_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./QLoRA_fine_tuned_model\". local_files_only=True)\n",
    "\n",
    "# 推論を行うための入力テキスト\n",
    "input_text = \"What is QLoRA?\"\n",
    "\n",
    "# テキストをトークナイズ\n",
    "inputs = tokenizer(input_text. return_tensors=\"pt\". padding=True. truncation=True. max_length=128)\n",
    "\n",
    "# 推論を実行\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=128.  # 出力の最大長\n",
    "        num_beams=5.     # ビームサーチの探索幅\n",
    "        no_repeat_ngram_size=2.  # 重複を避けるために2-gramを繰り返さない\n",
    "        early_stopping=True  # 終了条件\n",
    "    )\n",
    "\n",
    "# 出力をデコードして結果を表示\n",
    "output_text = tokenizer.decode(outputs[0]. skip_special_tokens=True)\n",
    "print(f\"モデルの回答: {output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは. 事前にファインチューニングされたモデルを使用して. 与えられたテキストに基づく推論（テキスト生成）を行うためのものです. transformers ライブラリと PyTorch を使用して. 指定したモデルから推論を実行し. 生成されたテキストを表示します. \n",
    "\n",
    "コードの詳細な解説\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM. AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 保存したモデルとトークナイザーを読み込む\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"./QLoRA_fine_tuned_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./QLoRA_fine_tuned_model\". local_files_only=True)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"./QLoRA_fine_tuned_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./QLoRA_fine_tuned_model\". local_files_only=True)\n",
    "```\n",
    "\n",
    "1.\tモデルとトークナイザーの読み込み\n",
    "- AutoModelForCausalLM.from_pretrained(\"./QLoRA_fine_tuned_model\". local_files_only=True) で. ローカルに保存されているファインチューニングされたモデル（QLoRA_fine_tuned_model）を読み込んでいます. \n",
    "- local_files_only=True オプションを使うことで. インターネットからモデルをダウンロードするのではなく. ローカルに保存されたモデルのみを読み込むことを指定しています. \n",
    "- 同様に. AutoTokenizer.from_pretrained を使って. モデルに合わせたトークナイザーも読み込んでいます. \n",
    "\n",
    "# 推論を行うための入力テキスト\n",
    "```python\n",
    "input_text = \"What is QLoRA?\"\n",
    "```\n",
    "\n",
    "# テキストをトークナイズ\n",
    "```python\n",
    "inputs = tokenizer(input_text. return_tensors=\"pt\". padding=True. truncation=True. max_length=128)\n",
    "```\n",
    "\n",
    "2.\t入力テキストの準備\n",
    "- input_text に. 推論のための入力文「What is QLoRA?」を指定します. \n",
    "- tokenizer(input_text. return_tensors=\"pt\". padding=True. truncation=True. max_length=128) では. input_text をトークナイズ（トークン化）して. モデルに入力できる形式に変換します. \n",
    "- return_tensors=\"pt\" は. PyTorch のテンソル形式で返すことを指定しています. \n",
    "- padding=True は. 入力シーケンスを指定した最大長（ここでは128）でパディングして長さを揃えるためのオプションです. \n",
    "- truncation=True は. 入力が最大長を超える場合にトークンを切り詰めるためのオプションです. \n",
    "- max_length=128 は. 入力テキストの最大長を128に制限します. \n",
    "\n",
    "# 推論を実行\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=128.  # 出力の最大長\n",
    "        num_beams=5.     # ビームサーチの探索幅\n",
    "        no_repeat_ngram_size=2.  # 重複を避けるために2-gramを繰り返さない\n",
    "        early_stopping=True  # 終了条件\n",
    "    )\n",
    "```\n",
    "\n",
    "3.\t推論の実行\n",
    "- with torch.no_grad(): は. 推論時に勾配計算を無効にすることでメモリ消費を削減し. 計算速度を向上させるために使われます. \n",
    "- model.generate(...) メソッドを使って. 指定した条件でテキスト生成を行います. 主な引数は以下の通りです：\n",
    "- inputs[\"input_ids\"] は. トークン化された入力文（input_ids）です. \n",
    "- inputs[\"attention_mask\"] は. パディングを考慮するためのマスクです. これにより. パディングされた部分を無視してモデルが計算を行うことができます. \n",
    "- max_length=128 は. 生成されるテキストの最大長を128トークンに制限します. \n",
    "- num_beams=5 は. ビームサーチの探索幅を5に設定しています. ビームサーチは. 最も確からしい次の単語を選ぶ際に複数の候補を同時に探索することで. より良い結果を得るための手法です. \n",
    "- no_repeat_ngram_size=2 は. 2-gram（連続する2つの単語）が繰り返されることを防ぐためのオプションです. これにより. 生成されるテキストに意味のない繰り返しが生じにくくなります. \n",
    "- early_stopping=True は. 生成中に終了条件が満たされた場合. 生成を早期に停止するオプションです. \n",
    "\n",
    "# 出力をデコードして結果を表示\n",
    "output_text = tokenizer.decode(outputs[0]. skip_special_tokens=True)\n",
    "print(f\"モデルの回答: {output_text}\")\n",
    "\n",
    "4.\t生成されたテキストのデコードと表示\n",
    "- tokenizer.decode(outputs[0]. skip_special_tokens=True) は. 生成されたトークン列（outputs[0]）を人間が読めるテキスト形式にデコードするためのメソッドです. skip_special_tokens=True は. 特殊なトークン（例：パディングトークンや開始/終了トークンなど）を除外するオプションです. \n",
    "- 最後に. print(f\"モデルの回答: {output_text}\") で. モデルが生成したテキストを表示します. \n",
    "\n",
    "まとめ\n",
    "\n",
    "このコードは. 保存されたファインチューニング済みモデルを使って. 与えられた入力テキストに基づいてテキスト生成を行い. その結果を表示するものです. トークン化やテキスト生成の設定. ビームサーチによる最適なテキストの選択. 繰り返しの回避など. 様々なテクニックを駆使して. 品質の高い生成結果を得ることを目指しています. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
