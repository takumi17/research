{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install torch\n",
    "%pip -q install transformers\n",
    "%pip -q install bitsandbytes\n",
    "%pip -q install peft\n",
    "%pip -q install trl\n",
    "%pip -q install datasets\n",
    "%pip -q install tensorboard\n",
    "%pip -q install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import bitsandbytes as bnb\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import torchinfo\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingfaceトークンの設定（gemma2を使用するのに必要なため）\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# モデルのリポジトリIDを設定\n",
    "repo_id = \"google/gemma-2-2b-jpn-it\"\n",
    "\n",
    "# データセットのパス\n",
    "dataset_path = \"./zmn.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonlファイルを読み込む\n",
    "json_data = []\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        json_data.append(json.loads(line))\n",
    "\n",
    "# DatasetオブジェクトにJSONデータを変換\n",
    "dataset = Dataset.from_list(json_data)\n",
    "\n",
    "# プロンプトフォーマット\n",
    "PROMPT_FORMAT = \"\"\"<start_of_turn>user\n",
    "{system}\n",
    "\n",
    "{instruction}\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{output}\n",
    "<end_of_turn>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 統一されたデータ形式で処理を行うため\n",
    "- JSONデータは辞書やリストの形式で保存されていますが、これでは直接機械学習モデルのトレーニングに利用できません。\n",
    "- Hugging FaceのDatasetオブジェクトは、効率的にデータを処理・操作できる専用の形式です。\n",
    "  - バッチ処理\n",
    "  - メモリ効率の良いデータ管理\n",
    "  - データのシャッフルや分割\n",
    "- Dataset形式に変換することで、Hugging Faceエコシステムの利便性を最大限に活用できます。\n",
    "\n",
    "2.\tトークナイザやモデルと簡単に連携させるため\n",
    "3.\t大規模データの処理に適している\n",
    "4.\tトレーニングデータの管理や前処理が簡単になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの内容をプロンプトにセット → textフィールドとして作成する関数\n",
    "def generate_text_field(data):\n",
    "    messages = data[\"messages\"]\n",
    "    system = \"\"\n",
    "    instruction = \"\"\n",
    "    output = \"\"\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            system = message[\"content\"]\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            instruction = message[\"content\"]\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            output = message[\"content\"]  \n",
    "    full_prompt = PROMPT_FORMAT.format(system=system, instruction=instruction, output=output) \n",
    "    return {\"text\": full_prompt}\n",
    "\n",
    "# データセットに（generate_text_fieldの処理を用いて）textフィールドを追加\n",
    "train_dataset = dataset.map(generate_text_field)\n",
    "\n",
    "# messagesフィールドを削除\n",
    "train_dataset = train_dataset.remove_columns([\"messages\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**コードの役割**\n",
    "1. generate_text_field 関数\n",
    "\n",
    "この関数は、各データサンプルから必要な情報を抽出し、プロンプト形式のテキストを作成します。\n",
    "\n",
    "主な流れ：\n",
    "- messages フィールドの解析:\n",
    "  - data[\"messages\"]は一連の対話データを含むリストと仮定されます。\n",
    "  - 各メッセージの役割（role）に基づいて、その内容（content）を変数に保存します：\n",
    "    - system: システムメッセージ（背景や設定情報）\n",
    "    - instruction: ユーザーからの指示や入力\n",
    "    - output: アシスタントの応答\n",
    "- プロンプト形式の生成:\n",
    "  - 収集したデータを特定のフォーマット文字列（PROMPT_FORMAT）に挿入し、最終的なプロンプトを作成します。\n",
    "  - 例\n",
    "\n",
    "```python\n",
    "PROMPT_FORMAT = \"{system}\\n\\nInstruction: {instruction}\\n\\nResponse: {output}\"\n",
    "```\n",
    "\n",
    "- 上記フォーマットに基づいて、以下のような完全なプロンプトを生成：\n",
    "\n",
    "```python\n",
    "System Message: You are an AI assistant.\n",
    "Instruction: Write a poem about the sea.\n",
    "Response: The sea is deep and blue...\n",
    "```\n",
    "\n",
    "- 戻り値:\n",
    "  - 辞書形式で\"text\"キーにプロンプトをセットして返します：\n",
    "\n",
    "```python\n",
    "return {\"text\": full_prompt}\n",
    "```\n",
    "\n",
    "1. dataset.map(generate_text_field)\n",
    "\n",
    "この部分は、generate_text_field関数をデータセット内の全てのデータサンプルに適用し、新しいフィールド（\"text\"）をデータセットに追加します。\n",
    "- map関数の動作:\n",
    "  - データセット内の各サンプルを引数として関数に渡し、結果を反映した新しいデータセットを返します。\n",
    "  - 上記のgenerate_text_fieldによって、各サンプルに\"text\"フィールドが追加されます。\n",
    "\n",
    "- 例\n",
    "\n",
    "元のデータサンプル\n",
    "```python\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a poem about the sea.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The sea is deep and blue...\"}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "処理後のデータサンプル\n",
    "```python\n",
    "{\n",
    "    \"messages\": [...],\n",
    "    \"text\": \"You are an AI assistant.\\n\\nInstruction: Write a poem about the sea.\\n\\nResponse: The sea is deep and blue...\"\n",
    "}\n",
    "```\n",
    "\n",
    "**なぜこの処理を行うのか？**\n",
    "1.\t言語モデルに適した形式を作るため\n",
    "- プロンプト形式でテキストを構築することで、言語モデルが学習時に入力を理解しやすくなります。\n",
    "- 特に指示（Instruction）と応答（Response）の明確な分離は、指示追従型モデルの学習に適しています。\n",
    "2.\tトレーニングデータの標準化\n",
    "- データセット全体を統一的なフォーマット（PROMPT_FORMAT）に整えることで、モデルのトレーニングが安定します。\n",
    "- 例えば、InstructionとResponseの明確な対応を学習できるようになります。\n",
    "3.\t高効率なトレーニングデータの準備\n",
    "- モデルにとって重要な情報（system、instruction、output）のみを抽出してプロンプト化することで、冗長な情報を省き、効率的なトレーニングが可能になります。\n",
    "4.\t多目的な使用を想定\n",
    "- この形式は、テキスト生成モデルや指示追従型タスク（例：ChatGPTのようなモデル）の微調整に適しています。\n",
    "- 生成タスク: プロンプトから自然な応答を生成する能力を訓練。\n",
    "- 指示理解タスク: 指示内容と関連した応答を生成する能力を強化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 量子化のConfigを設定\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # 4ビット量子化を使用\n",
    "    bnb_4bit_quant_type=\"nf4\", # 4ビット量子化の種類にnf4（NormalFloat4）を使用\n",
    "    bnb_4bit_use_double_quant=True, # 二重量子化を使用\n",
    "    bnb_4bit_compute_dtype=torch.float16 # 量子化のデータ型をfloat16に設定\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bnb_4bit_quant_type=\"nf4\"`\n",
    "\n",
    "- NF4（NormalFloat4） という量子化の形式を指定します。\n",
    "- NF4は「データの重要な部分をうまく残す」特別な方法で、精度をできるだけ保ちながら量子化する技術です。\n",
    "- なぜ使うのか？\n",
    "  - 通常の4ビット量子化よりも性能が良いとされています。\n",
    "\n",
    "`bnb_4bit_use_double_quant=True`\n",
    "\n",
    "- 二重量子化（二段階の圧縮） を使います。\n",
    "- 1回量子化した後に、もう一度量子化をかけることで、さらにメモリを節約できます。\n",
    "\n",
    " `bnb_4bit_compute_dtype=torch.float16`\n",
    "\n",
    "- 計算時のデータ型をfloat16（16ビット浮動小数点）に設定します。\n",
    "- 4ビット量子化では、計算するときに直接4ビットを使うのではなく、少し高精度なfloat16を使います。\n",
    "  - 理由は、計算が安定しやすくなるためです。\n",
    "- 例え話:\n",
    "  - 荷物を運ぶときに、元は小さいけど運ぶときだけ少し大きめの箱を使うイメージ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=repo_id, # モデルのリポジトリIDをセット\n",
    "    device_map={\"\": \"cuda\"}, # 使用デバイスを設定\n",
    "    quantization_config=quantization_config, # 量子化のConfigをセット\n",
    "    attn_implementation=\"eager\", # 注意機構に\"eager\"を設定（Gemma2モデルの学習で推奨されているため）\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**このコードの目的**\n",
    "\n",
    "- 機械学習モデルを「事前学習済み（pretrained）」の状態で読み込み、効率的に実行できるようにします。\n",
    "- この設定では、量子化を適用し、GPU（CUDA）を利用することで高速化と省メモリ化を目指しています。\n",
    "\n",
    "**コードの詳しい説明**\n",
    "\n",
    "1. AutoModelForCausalLM.from_pretrained()\n",
    "\n",
    "この関数は、事前学習済みの言語モデルを読み込むために使われます。\n",
    "- AutoModelForCausalLM:\n",
    "  - Hugging Faceのライブラリで提供されるクラス。\n",
    "  - **Causal Language Model（因果言語モデル）**をロードするための自動設定クラスです。\n",
    "  - 例：ChatGPTのような対話型AIや、文章生成モデルに使われます。\n",
    "- from_pretrained():\n",
    "  - モデルを指定されたリポジトリからダウンロードしてロードします。\n",
    "  - リポジトリとは？\n",
    "    - モデルのデータが保存されている場所（クラウド上のフォルダのようなもの）。\n",
    "    - 例：Hugging Face Hub上の公開モデル。\n",
    "\n",
    "2. attn_implementation=\"eager\"\n",
    "\n",
    "- attn_implementation:\n",
    "  - 注意機構（Attention Mechanism）の実装方法を指定します。\n",
    "  - \"eager\"は、Gemma-2モデルで推奨されている設定。\n",
    "- 注意機構とは？\n",
    "  - モデルが重要な単語や情報に集中するための仕組み。\n",
    "  - 例：質問に応答するときに、質問文のキーワードに注意を払う。\n",
    "- eagerの利点:\n",
    "  - シンプルで安定した動作が期待できるため、特定のモデル（例：Gemma-2）では推奨されています。\n",
    "\n",
    "eagerは、Gemma-2モデルなどの特定の機械学習モデルで推奨されている注意機構（Attention Mechanism）の実装方法です。この設定により、モデルの注意（Attention）の計算が「簡潔かつ安定」した方法で実行されます。\n",
    "\n",
    "**Attention Mechanism（注意機構）とは？**\n",
    "\n",
    "- 役割: モデルが重要な情報（単語やトークン）に「集中」できるようにする仕組み。\n",
    "- 例: 「猫はかわいい」という文で、「猫」と「かわいい」の関連性に注目しながら意味を学ぶ。\n",
    "\n",
    "\n",
    "**eager の意味**\n",
    "\n",
    "eagerは、Attention計算を実行する方法の一つで、「即時実行モード」に相当します。\n",
    "- **即時実行（Eager Execution）**とは？\n",
    "  - コードを1行ずつ実行する、直感的でデバッグしやすい方式。\n",
    "  - Pythonプログラムの通常の動作と似ています。\n",
    "  - 注意の計算が「リアルタイム」に行われるため、動作が分かりやすく、エラーが発生した場合も原因を特定しやすい。\n",
    "- 他のモードとの違い\n",
    "  - 多くのモデルでは、Attention計算を高速化するために「最適化されたバックエンド」を使うことがあります（例: Flash Attention）。\n",
    "  - しかし、これらの高速化手法は、特定のハードウェアやライブラリに依存することが多く、安定性に問題が生じる場合があります。\n",
    "  - eagerは単純で安定しており、特に新しいモデルやデバイスで推奨されることがあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# キャッシュを無効化（メモリ使用量を削減）\n",
    "model.config.use_cache = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは、モデルの設定を変更して、キャッシュ機能を無効化するものです。これにより、特にトレーニング時や推論時に、モデルが使用するメモリ量を削減できます。\n",
    "\n",
    "コードの詳細\n",
    "\n",
    "`model.config.use_cache = False`\n",
    "\n",
    "- model.config:\n",
    "  - モデルの動作に関する設定を格納するオブジェクト。\n",
    "  - 例：バッチサイズ、注意機構の種類、キャッシュの利用など。\n",
    "- use_cache:\n",
    "  - キャッシュ（生成済みの計算結果の再利用）を有効または無効にする設定。\n",
    "  - デフォルト値は通常Trueで、推論時に計算の効率を上げるために使用されます。\n",
    "- Falseに設定する効果:\n",
    "  - キャッシュを無効化することで、メモリ使用量を減らす。\n",
    "  - 特に、モデルがメモリに余裕がない場合（大きなモデルを小さなGPUで使う場合など）に有効。\n",
    "\n",
    "キャッシュ（Cache）とは？\n",
    "\n",
    "キャッシュは、既に計算した結果を再利用する仕組みです。\n",
    "- 例：文章生成タスク\n",
    "\t1.\tモデルが文章の最初の部分を計算します。\n",
    "\t2.\t次の単語を予測するとき、以前の計算結果をキャッシュとして再利用することで効率化します。\n",
    "\t3.\tキャッシュを利用することで、計算の重複を防ぎ、高速化を実現します。\n",
    "- 推論時（生成タスク）での利点:\n",
    "  - 次の単語を予測する際、過去の文脈を再計算する必要がないため、時間を大幅に節約できます。\n",
    "\n",
    "キャッシュを無効化する理由\n",
    "\n",
    "1.\tトレーニング時の不要性:\n",
    "  - キャッシュは推論（文章生成）では役立ちますが、トレーニングでは通常不要です。\n",
    "  - 各バッチで入力データが異なるため、キャッシュを保持しても再利用する機会がほとんどありません。\n",
    "2.\tメモリの節約:\n",
    "  - キャッシュを保持するためには追加のメモリが必要です。\n",
    "  - モデルが大きい場合や、GPUメモリに制約がある場合は、キャッシュを無効にすることでメモリ不足を防ぎます。\n",
    "3.\t動作の安定性:\n",
    "  - 大規模なモデルや複雑なトレーニングでは、キャッシュを保持すると計算が不安定になる場合があります。\n",
    "  - キャッシュを無効化することで、これらの問題を回避できます。\n",
    "\n",
    "例えで理解する\n",
    "\n",
    "- キャッシュあり（use_cache=True）:\n",
    "  - 本を読んでいるときに、前のページのメモを取っておいて、それを再度参照して内容を思い出す。\n",
    "  - 新しいページを読むときには、過去のページのメモが役立つ。\n",
    "- キャッシュなし（use_cache=False）:\n",
    "  - 本を読むときに、メモを取らずに毎回最初から全部読む。\n",
    "  - 時間はかかるけど、メモを保存するスペース（メモリ）は不要になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テンソル並列ランクを１に設定（テンソル並列化を使用しない）\n",
    "model.config.pretraining_tp = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**コードの詳細**\n",
    "\n",
    "`model.config.pretraining_tp = 1`\n",
    "\n",
    "このコードは、モデルの設定において**テンソル並列化（Tensor Parallelism）**のランクを1に設定するものです。テンソル並列化を無効化（使用しない）することを意味します。\n",
    "\n",
    "テンソル並列化とは？\n",
    "\n",
    "- テンソル並列化は、大規模なニューラルネットワークモデルを複数のGPUに分割して計算を並列化する方法の一つです。\n",
    "- 通常、大規模モデルでは重い計算を分散するため、モデルの重みや演算をGPU間で分割して負荷を分散します。\n",
    "- テンソル並列化は、モデルの**テンソル（行列やベクトルなどのデータ構造）**を分割し、複数のデバイスで並行して計算を行う手法です。\n",
    "\n",
    "pretraining_tp の役割\n",
    "\n",
    "- **pretraining_tp**は、モデルがテンソル並列化を利用する際の「並列ランク数」を指定します。\n",
    "  - ランク数: モデルのテンソルを分割する数。\n",
    "  - 例: ランクが2なら、モデルのテンソルが2つのGPUで並列計算されます。\n",
    "- pretraining_tp=1:\n",
    "  - モデルがテンソル並列化を使用しない設定。\n",
    "  - 全てのテンソル計算が1つのデバイス（通常は1つのGPU）で実行されます。\n",
    "\n",
    "このコードの目的\n",
    "\n",
    "- テンソル並列化を無効化することで、モデルの動作を簡略化し、単一GPU上で計算を行うようにする。\n",
    "- メモリや計算リソースの制約がある環境では、この設定が適している場合があります。\n",
    "\n",
    "なぜテンソル並列化を無効化するのか?\n",
    "\n",
    "1.\tリソース制約:\n",
    "  - ユーザーが単一のGPUを使っている場合、テンソル並列化を有効にすると動作しないか、エラーが発生する可能性があります。\n",
    "  - 並列化を無効化することで、シンプルな設定で実行できます。\n",
    "2.\t小規模モデルの場合:\n",
    "  - モデルが小さい場合、テンソル並列化は必要ありません。\n",
    "  - 並列化のオーバーヘッド（通信コスト）がかえってパフォーマンスを低下させることがあります。\n",
    "3.\tデバッグや開発の容易さ:\n",
    "  - 並列化を無効化することで、コードの実行が単純化され、エラーが発生した際にデバッグが容易になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイザーの準備\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=repo_id, # モデルのリポジトリIDをセット\n",
    "    attn_implementation=\"eager\", # 注意機構に\"eager\"を設定（Gemma2モデルの学習で推奨されているため）\n",
    "    add_eos_token=True, # EOSトークンの追加を設定\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パディングトークンが設定されていない場合、EOSトークンを設定\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# パディングを右側に設定(fp16を使う際のオーバーフロー対策)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# モデルから4ビット量子化された線形層の名前を取得する関数\n",
    "def find_all_linear_names(model):\n",
    "    target_class = bnb.nn.Linear4bit\n",
    "    linear_layer_names = set()\n",
    "    for name_list, module in model.named_modules():\n",
    "        if isinstance(module, target_class):\n",
    "            names = name_list.split('.')\n",
    "            layer_name = names[-1] if len(names) > 1 else names[0]\n",
    "            linear_layer_names.add(layer_name)\n",
    "    if 'lm_head' in linear_layer_names:\n",
    "        linear_layer_names.remove('lm_head')\n",
    "    return list(linear_layer_names)\n",
    "\n",
    "# モジュールのリストとして線形層の名前を取得\n",
    "target_modules = find_all_linear_names(model)\n",
    "\n",
    "# LoRAのConfigを設定\n",
    "Lora_config = LoraConfig(\n",
    "    lora_alpha=8, # LoRAによる学習の影響力を調整（スケーリング)\n",
    "    lora_dropout=0.1, # ドロップアウト率\n",
    "    r=4, # 低ランク行列の次元数\n",
    "    bias=\"none\", # バイアスのパラメータ更新\n",
    "    task_type=\"CAUSAL_LM\", # タスクの種別\n",
    "    target_modules=target_modules # LoRAを適用するモジュールのリスト\n",
    ")\n",
    "\n",
    "# 学習パラメータを設定\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./train_logs\", # ログの出力ディレクトリ\n",
    "    fp16=True, # fp16を使用\n",
    "    logging_strategy='epoch', # 各エポックごとにログを保存（デフォルトは\"steps\"）\n",
    "    save_strategy='epoch', # 各エポックごとにチェックポイントを保存（デフォルトは\"steps\"）\n",
    "    num_train_epochs=3, # 学習するエポック数\n",
    "    per_device_train_batch_size=1, # （GPUごと）一度に処理するバッチサイズ\n",
    "    gradient_accumulation_steps=4, # 勾配を蓄積するステップ数\n",
    "    optim=\"paged_adamw_32bit\", # 最適化アルゴリズム\n",
    "    learning_rate=5e-4, # 初期学習率\n",
    "    lr_scheduler_type=\"cosine\", # 学習率スケジューラの種別\n",
    "    max_grad_norm=0.3, # 勾配の最大ノルムを制限（クリッピング）\n",
    "    warmup_ratio=0.03, # 学習を増加させるウォームアップ期間の比率\n",
    "    weight_decay=0.001, # 重み減衰率\n",
    "    group_by_length=True,# シーケンスの長さが近いものをまとめてバッチ化\n",
    "    report_to=\"tensorboard\" # TensorBoard使用してログを生成（\"./train_logs\"に保存）\n",
    ")\n",
    "\n",
    "# SFTパラメータの設定\n",
    "trainer = SFTTrainer(\n",
    "    model=model, # モデルをセット\n",
    "    tokenizer=tokenizer, # トークナイザーをセット\n",
    "    train_dataset=train_dataset, # データセットをセット\n",
    "    dataset_text_field=\"text\", # 学習に使用するデータセットのフィールド\n",
    "    peft_config=Lora_config, # LoRAのConfigをセット\n",
    "    args=training_arguments, # 学習パラメータをセット\n",
    "    max_seq_length=512, # 入力シーケンスの最大長を設定\n",
    ")\n",
    "\n",
    "# 正規化層をfloat32に変換(学習を安定させるため)\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "# モデルの学習\n",
    "trainer.train()\n",
    "\n",
    "# 学習したアダプターを保存\n",
    "trainer.model.save_pretrained(\"./QLoRA_sample_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# huggingfaceトークンの設定（gemma2を使用するのに必要なため）\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# アダプターのパス\n",
    "adapter_path = \"./QLoRA_sample_model\"\n",
    "\n",
    "# モデルのリポジトリIDを設定\n",
    "repo_id = \"google/gemma-2-2b-jpn-it\"\n",
    "\n",
    "# ベースモデルとアダプターの読み込み\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=adapter_path, \n",
    "    device_map={\"\": \"cuda\"}, \n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# トークナイザーの準備\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=repo_id, \n",
    ")\n",
    "\n",
    "# パディングトークンが設定されていない場合、EOSトークンを設定\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# パディングを右側に設定(fp16を使う際のオーバーフロー対策)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "question_list = [\n",
    "    \"名前を教えてください\",\n",
    "    \"日本の首都はどこですか\", \n",
    "    \"ジョークを言ってください\", \n",
    "    \"東北の観光地について教えてください\" \n",
    "]\n",
    "\n",
    "# 各質問に対して回答を生成\n",
    "for i, question in enumerate(question_list, 1):\n",
    "    print(f\"\\nchat_{i}----------------------------------------------------\")\n",
    "    print(f\"質問: {question}\")\n",
    "    \n",
    "    # チャットメッセージの設定\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    # トークナイザーのチャットテンプレートを適用\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # プロンプトをトークン化してテンソルに変換（GPUに転送）\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # 回答を生成\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        attention_mask=model_inputs.attention_mask,\n",
    "        max_new_tokens=300\n",
    "    )\n",
    "    \n",
    "    # 生成された回答を抽出\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    # トークンIDを文字列に変換\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"回答: {response}\")\n",
    "    print(\"----------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
