{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install torch\n",
    "%pip -q install transformers\n",
    "%pip -q install bitsandbytes\n",
    "%pip -q install peft\n",
    "%pip -q install trl\n",
    "%pip -q install datasets\n",
    "%pip -q install tensorboard\n",
    "%pip -q install pandas openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import bitsandbytes as bnb\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import torchinfo\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingfaceトークンの設定（gemma2を使用するのに必要なため）\n",
    "os.environ[\"HF_TOKEN\"] = \"内緒\"\n",
    "\n",
    "# モデルのリポジトリIDを設定\n",
    "repo_id = \"google/gemma-2-2b-jpn-it\"\n",
    "\n",
    "# データセットのパス\n",
    "dataset_path = \"./zmn.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonlファイルを読み込む\n",
    "json_data = []\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        json_data.append(json.loads(line))\n",
    "\n",
    "# DatasetオブジェクトにJSONデータを変換\n",
    "dataset = Dataset.from_list(json_data)\n",
    "\n",
    "# プロンプトフォーマット\n",
    "PROMPT_FORMAT = \"\"\"<start_of_turn>user\n",
    "{system}\n",
    "\n",
    "{instruction}\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{output}\n",
    "<end_of_turn>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 統一されたデータ形式で処理を行うため\n",
    "- JSONデータは辞書やリストの形式で保存されていますが、これでは直接機械学習モデルのトレーニングに利用できません。\n",
    "- Hugging FaceのDatasetオブジェクトは、効率的にデータを処理・操作できる専用の形式です。\n",
    "  - バッチ処理\n",
    "  - メモリ効率の良いデータ管理\n",
    "  - データのシャッフルや分割\n",
    "- Dataset形式に変換することで、Hugging Faceエコシステムの利便性を最大限に活用できます。\n",
    "\n",
    "2.\tトークナイザやモデルと簡単に連携させるため\n",
    "3.\t大規模データの処理に適している\n",
    "4.\tトレーニングデータの管理や前処理が簡単になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの内容をプロンプトにセット → textフィールドとして作成する関数\n",
    "def generate_text_field(data):\n",
    "    messages = data[\"messages\"]\n",
    "    system = \"\"\n",
    "    instruction = \"\"\n",
    "    output = \"\"\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            system = message[\"content\"]\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            instruction = message[\"content\"]\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            output = message[\"content\"]  \n",
    "    full_prompt = PROMPT_FORMAT.format(system=system, instruction=instruction, output=output) \n",
    "    return {\"text\": full_prompt}\n",
    "\n",
    "# データセットに（generate_text_fieldの処理を用いて）textフィールドを追加\n",
    "train_dataset = dataset.map(generate_text_field)\n",
    "\n",
    "# messagesフィールドを削除\n",
    "train_dataset = train_dataset.remove_columns([\"messages\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**コードの役割**\n",
    "1. generate_text_field 関数\n",
    "\n",
    "この関数は、各データサンプルから必要な情報を抽出し、プロンプト形式のテキストを作成します。\n",
    "\n",
    "主な流れ：\n",
    "- messages フィールドの解析:\n",
    "  - data[\"messages\"]は一連の対話データを含むリストと仮定されます。\n",
    "  - 各メッセージの役割（role）に基づいて、その内容（content）を変数に保存します：\n",
    "    - system: システムメッセージ（背景や設定情報）\n",
    "    - instruction: ユーザーからの指示や入力\n",
    "    - output: アシスタントの応答\n",
    "- プロンプト形式の生成:\n",
    "  - 収集したデータを特定のフォーマット文字列（PROMPT_FORMAT）に挿入し、最終的なプロンプトを作成します。\n",
    "  - 例\n",
    "\n",
    "```python\n",
    "PROMPT_FORMAT = \"{system}\\n\\nInstruction: {instruction}\\n\\nResponse: {output}\"\n",
    "```\n",
    "\n",
    "- 上記フォーマットに基づいて、以下のような完全なプロンプトを生成：\n",
    "\n",
    "```python\n",
    "System Message: You are an AI assistant.\n",
    "Instruction: Write a poem about the sea.\n",
    "Response: The sea is deep and blue...\n",
    "```\n",
    "\n",
    "- 戻り値:\n",
    "  - 辞書形式で\"text\"キーにプロンプトをセットして返します：\n",
    "\n",
    "```python\n",
    "return {\"text\": full_prompt}\n",
    "```\n",
    "\n",
    "1. dataset.map(generate_text_field)\n",
    "\n",
    "この部分は、generate_text_field関数をデータセット内の全てのデータサンプルに適用し、新しいフィールド（\"text\"）をデータセットに追加します。\n",
    "- map関数の動作:\n",
    "  - データセット内の各サンプルを引数として関数に渡し、結果を反映した新しいデータセットを返します。\n",
    "  - 上記のgenerate_text_fieldによって、各サンプルに\"text\"フィールドが追加されます。\n",
    "\n",
    "- 例\n",
    "\n",
    "元のデータサンプル\n",
    "```python\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a poem about the sea.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The sea is deep and blue...\"}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "処理後のデータサンプル\n",
    "```python\n",
    "{\n",
    "    \"messages\": [...],\n",
    "    \"text\": \"You are an AI assistant.\\n\\nInstruction: Write a poem about the sea.\\n\\nResponse: The sea is deep and blue...\"\n",
    "}\n",
    "```\n",
    "\n",
    "**なぜこの処理を行うのか？**\n",
    "1.\t言語モデルに適した形式を作るため\n",
    "- プロンプト形式でテキストを構築することで、言語モデルが学習時に入力を理解しやすくなります。\n",
    "- 特に指示（Instruction）と応答（Response）の明確な分離は、指示追従型モデルの学習に適しています。\n",
    "2.\tトレーニングデータの標準化\n",
    "- データセット全体を統一的なフォーマット（PROMPT_FORMAT）に整えることで、モデルのトレーニングが安定します。\n",
    "- 例えば、InstructionとResponseの明確な対応を学習できるようになります。\n",
    "3.\t高効率なトレーニングデータの準備\n",
    "- モデルにとって重要な情報（system、instruction、output）のみを抽出してプロンプト化することで、冗長な情報を省き、効率的なトレーニングが可能になります。\n",
    "4.\t多目的な使用を想定\n",
    "- この形式は、テキスト生成モデルや指示追従型タスク（例：ChatGPTのようなモデル）の微調整に適しています。\n",
    "- 生成タスク: プロンプトから自然な応答を生成する能力を訓練。\n",
    "- 指示理解タスク: 指示内容と関連した応答を生成する能力を強化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 量子化のConfigを設定\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # 4ビット量子化を使用\n",
    "    bnb_4bit_quant_type=\"nf4\", # 4ビット量子化の種類にnf4（NormalFloat4）を使用\n",
    "    bnb_4bit_use_double_quant=True, # 二重量子化を使用\n",
    "    bnb_4bit_compute_dtype=torch.float16 # 量子化のデータ型をfloat16に設定\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bnb_4bit_quant_type=\"nf4\"`\n",
    "\n",
    "- NF4（NormalFloat4） という量子化の形式を指定します。\n",
    "- NF4は「データの重要な部分をうまく残す」特別な方法で、精度をできるだけ保ちながら量子化する技術です。\n",
    "- なぜ使うのか？\n",
    "  - 通常の4ビット量子化よりも性能が良いとされています。\n",
    "\n",
    "`bnb_4bit_use_double_quant=True`\n",
    "\n",
    "- 二重量子化（二段階の圧縮） を使います。\n",
    "- 1回量子化した後に、もう一度量子化をかけることで、さらにメモリを節約できます。\n",
    "\n",
    " `bnb_4bit_compute_dtype=torch.float16`\n",
    "\n",
    "- 計算時のデータ型をfloat16（16ビット浮動小数点）に設定します。\n",
    "- 4ビット量子化では、計算するときに直接4ビットを使うのではなく、少し高精度なfloat16を使います。\n",
    "  - 理由は、計算が安定しやすくなるためです。\n",
    "- 例え話:\n",
    "  - 荷物を運ぶときに、元は小さいけど運ぶときだけ少し大きめの箱を使うイメージ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=repo_id, # モデルのリポジトリIDをセット\n",
    "    device_map={\"\": \"cuda\"}, # 使用デバイスを設定\n",
    "    quantization_config=quantization_config, # 量子化のConfigをセット\n",
    "    attn_implementation=\"eager\", # 注意機構に\"eager\"を設定（Gemma2モデルの学習で推奨されているため）\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**このコードの目的**\n",
    "\n",
    "- 機械学習モデルを「事前学習済み（pretrained）」の状態で読み込み、効率的に実行できるようにします。\n",
    "- この設定では、量子化を適用し、GPU（CUDA）を利用することで高速化と省メモリ化を目指しています。\n",
    "\n",
    "**コードの詳しい説明**\n",
    "\n",
    "1. AutoModelForCausalLM.from_pretrained()\n",
    "\n",
    "この関数は、事前学習済みの言語モデルを読み込むために使われます。\n",
    "- AutoModelForCausalLM:\n",
    "  - Hugging Faceのライブラリで提供されるクラス。\n",
    "  - **Causal Language Model（因果言語モデル）**をロードするための自動設定クラスです。\n",
    "  - 例：ChatGPTのような対話型AIや、文章生成モデルに使われます。\n",
    "- from_pretrained():\n",
    "  - モデルを指定されたリポジトリからダウンロードしてロードします。\n",
    "  - リポジトリとは？\n",
    "    - モデルのデータが保存されている場所（クラウド上のフォルダのようなもの）。\n",
    "    - 例：Hugging Face Hub上の公開モデル。\n",
    "\n",
    "2. attn_implementation=\"eager\"\n",
    "\n",
    "- attn_implementation:\n",
    "  - 注意機構（Attention Mechanism）の実装方法を指定します。\n",
    "  - \"eager\"は、Gemma-2モデルで推奨されている設定。\n",
    "- 注意機構とは？\n",
    "  - モデルが重要な単語や情報に集中するための仕組み。\n",
    "  - 例：質問に応答するときに、質問文のキーワードに注意を払う。\n",
    "- eagerの利点:\n",
    "  - シンプルで安定した動作が期待できるため、特定のモデル（例：Gemma-2）では推奨されています。\n",
    "\n",
    "eagerは、Gemma-2モデルなどの特定の機械学習モデルで推奨されている注意機構（Attention Mechanism）の実装方法です。この設定により、モデルの注意（Attention）の計算が「簡潔かつ安定」した方法で実行されます。\n",
    "\n",
    "**Attention Mechanism（注意機構）とは？**\n",
    "\n",
    "- 役割: モデルが重要な情報（単語やトークン）に「集中」できるようにする仕組み。\n",
    "- 例: 「猫はかわいい」という文で、「猫」と「かわいい」の関連性に注目しながら意味を学ぶ。\n",
    "\n",
    "\n",
    "**eager の意味**\n",
    "\n",
    "eagerは、Attention計算を実行する方法の一つで、「即時実行モード」に相当します。\n",
    "- **即時実行（Eager Execution）**とは？\n",
    "  - コードを1行ずつ実行する、直感的でデバッグしやすい方式。\n",
    "  - Pythonプログラムの通常の動作と似ています。\n",
    "  - 注意の計算が「リアルタイム」に行われるため、動作が分かりやすく、エラーが発生した場合も原因を特定しやすい。\n",
    "- 他のモードとの違い\n",
    "  - 多くのモデルでは、Attention計算を高速化するために「最適化されたバックエンド」を使うことがあります（例: Flash Attention）。\n",
    "  - しかし、これらの高速化手法は、特定のハードウェアやライブラリに依存することが多く、安定性に問題が生じる場合があります。\n",
    "  - eagerは単純で安定しており、特に新しいモデルやデバイスで推奨されることがあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# キャッシュを無効化（メモリ使用量を削減）\n",
    "model.config.use_cache = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは、モデルの設定を変更して、キャッシュ機能を無効化するものです。これにより、特にトレーニング時や推論時に、モデルが使用するメモリ量を削減できます。\n",
    "\n",
    "コードの詳細\n",
    "\n",
    "`model.config.use_cache = False`\n",
    "\n",
    "- model.config:\n",
    "  - モデルの動作に関する設定を格納するオブジェクト。\n",
    "  - 例：バッチサイズ、注意機構の種類、キャッシュの利用など。\n",
    "- use_cache:\n",
    "  - キャッシュ（生成済みの計算結果の再利用）を有効または無効にする設定。\n",
    "  - デフォルト値は通常Trueで、推論時に計算の効率を上げるために使用されます。\n",
    "- Falseに設定する効果:\n",
    "  - キャッシュを無効化することで、メモリ使用量を減らす。\n",
    "  - 特に、モデルがメモリに余裕がない場合（大きなモデルを小さなGPUで使う場合など）に有効。\n",
    "\n",
    "キャッシュ（Cache）とは？\n",
    "\n",
    "キャッシュは、既に計算した結果を再利用する仕組みです。\n",
    "- 例：文章生成タスク\n",
    "\t1.\tモデルが文章の最初の部分を計算します。\n",
    "\t2.\t次の単語を予測するとき、以前の計算結果をキャッシュとして再利用することで効率化します。\n",
    "\t3.\tキャッシュを利用することで、計算の重複を防ぎ、高速化を実現します。\n",
    "- 推論時（生成タスク）での利点:\n",
    "  - 次の単語を予測する際、過去の文脈を再計算する必要がないため、時間を大幅に節約できます。\n",
    "\n",
    "キャッシュを無効化する理由\n",
    "\n",
    "1.\tトレーニング時の不要性:\n",
    "  - キャッシュは推論（文章生成）では役立ちますが、トレーニングでは通常不要です。\n",
    "  - 各バッチで入力データが異なるため、キャッシュを保持しても再利用する機会がほとんどありません。\n",
    "2.\tメモリの節約:\n",
    "  - キャッシュを保持するためには追加のメモリが必要です。\n",
    "  - モデルが大きい場合や、GPUメモリに制約がある場合は、キャッシュを無効にすることでメモリ不足を防ぎます。\n",
    "3.\t動作の安定性:\n",
    "  - 大規模なモデルや複雑なトレーニングでは、キャッシュを保持すると計算が不安定になる場合があります。\n",
    "  - キャッシュを無効化することで、これらの問題を回避できます。\n",
    "\n",
    "例えで理解する\n",
    "\n",
    "- キャッシュあり（use_cache=True）:\n",
    "  - 本を読んでいるときに、前のページのメモを取っておいて、それを再度参照して内容を思い出す。\n",
    "  - 新しいページを読むときには、過去のページのメモが役立つ。\n",
    "- キャッシュなし（use_cache=False）:\n",
    "  - 本を読むときに、メモを取らずに毎回最初から全部読む。\n",
    "  - 時間はかかるけど、メモを保存するスペース（メモリ）は不要になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テンソル並列ランクを１に設定（テンソル並列化を使用しない）\n",
    "model.config.pretraining_tp = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**コードの詳細**\n",
    "\n",
    "`model.config.pretraining_tp = 1`\n",
    "\n",
    "このコードは、モデルの設定において **テンソル並列化（Tensor Parallelism）** のランクを1に設定するものです。テンソル並列化を無効化（使用しない）することを意味します。\n",
    "\n",
    "テンソル並列化とは？\n",
    "\n",
    "- テンソル並列化は、大規模なニューラルネットワークモデルを複数のGPUに分割して計算を並列化する方法の一つです。\n",
    "- 通常、大規模モデルでは重い計算を分散するため、モデルの重みや演算をGPU間で分割して負荷を分散します。\n",
    "- テンソル並列化は、モデルの **テンソル（行列やベクトルなどのデータ構造）** を分割し、複数のデバイスで並行して計算を行う手法です。\n",
    "\n",
    "pretraining_tp の役割\n",
    "\n",
    "- **pretraining_tp**は、モデルがテンソル並列化を利用する際の「並列ランク数」を指定します。\n",
    "  - ランク数: モデルのテンソルを分割する数。\n",
    "  - 例: ランクが2なら、モデルのテンソルが2つのGPUで並列計算されます。\n",
    "- pretraining_tp=1:\n",
    "  - モデルがテンソル並列化を使用しない設定。\n",
    "  - 全てのテンソル計算が1つのデバイス（通常は1つのGPU）で実行されます。\n",
    "\n",
    "このコードの目的\n",
    "\n",
    "- テンソル並列化を無効化することで、モデルの動作を簡略化し、単一GPU上で計算を行うようにする。\n",
    "- メモリや計算リソースの制約がある環境では、この設定が適している場合があります。\n",
    "\n",
    "なぜテンソル並列化を無効化するのか?\n",
    "\n",
    "1.\tリソース制約:\n",
    "  - ユーザーが単一のGPUを使っている場合、テンソル並列化を有効にすると動作しないか、エラーが発生する可能性があります。\n",
    "  - 並列化を無効化することで、シンプルな設定で実行できます。\n",
    "2.\t小規模モデルの場合:\n",
    "  - モデルが小さい場合、テンソル並列化は必要ありません。\n",
    "  - 並列化のオーバーヘッド（通信コスト）がかえってパフォーマンスを低下させることがあります。\n",
    "3.\tデバッグや開発の容易さ:\n",
    "  - 並列化を無効化することで、コードの実行が単純化され、エラーが発生した際にデバッグが容易になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイザーの準備\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=repo_id, # モデルのリポジトリIDをセット\n",
    "    attn_implementation=\"eager\", # 注意機構に\"eager\"を設定（Gemma2モデルの学習で推奨されているため）\n",
    "    add_eos_token=True, # EOSトークンの追加を設定\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは、Hugging Faceライブラリを使用して、**事前学習済みトークナイザー（tokenizer）**をロードし、必要な設定を行うものです。\n",
    "\n",
    "**トークナイザーとは？**\n",
    "\n",
    "トークナイザーは、テキストデータをモデルが理解できる形式に変換するツールです。\n",
    "- 役割:\n",
    "\t1.\tテキストをトークン（単語や文字単位の小さな単位）に分割する。\n",
    "\t2.\tトークンを数値データ（ID）に変換する。\n",
    "\t3.\tモデルが出力した数値（トークンID）を再びテキストに戻す。\n",
    "- 例:\n",
    "  - 入力: \"Hello world!\"\n",
    "  - 出力（トークン分割）: [\"Hello\", \"world\", \"!\"]\n",
    "  - 出力（数値化）: [101, 7592, 999]（トークンごとのID）\n",
    "\n",
    "コードの各パラメータについて\n",
    "\n",
    "1. pretrained_model_name_or_path=repo_id\n",
    "\n",
    "- 説明:\n",
    "  - トークナイザーをロードするためのモデルの名前やパスを指定します。\n",
    "  - repo_idには事前学習済みモデルのリポジトリID（例: \"gemma-2-2b\"など）が含まれます。\n",
    "  - トークナイザーは、このモデルに適合するように設計されています。\n",
    "- 目的:\n",
    "  - モデルに対応した適切なトークナイザーを利用するため。\n",
    "\n",
    "1. attn_implementation=\"eager\"\n",
    "\n",
    "- 説明:\n",
    "  - 注意機構（Attention Mechanism）を制御する設定。\n",
    "  - \"eager\"は、Gemma2モデルで推奨される注意機構の実装方式。\n",
    "  - モデルの挙動や互換性を保つため、トークナイザーにもこの設定を指定します。\n",
    "- 目的:\n",
    "  - モデルの動作と整合性を保つため。\n",
    "\n",
    "1. add_eos_token=True\n",
    "\n",
    "- 説明:\n",
    "  - トークナイザーが入力テキストの最後に **EOSトークン（End of Sequence）** を自動的に追加します。\n",
    "  - EOSトークンは、テキストの終了を示す特殊なトークンです。\n",
    "- 理由:\n",
    "  - モデルが入力データの終了を明確に理解するため。\n",
    "  - 特に、テキスト生成タスクでは重要です。モデルはEOSトークンを出力すると、文章生成を終了します。\n",
    "- 例:\n",
    "  - 入力テキスト: \"Hello world!\"\n",
    "  - トークン化結果（EOS追加なし）: [101, 7592, 999]\n",
    "  - トークン化結果（EOS追加あり）: [101, 7592, 999, 102]（102がEOSトークン）\n",
    "\n",
    "トークナイザーを準備する理由\n",
    "\n",
    "1.\tモデルとの整合性:\n",
    "  - モデルに適したトークナイザーを利用することで、入力形式が正確になり、エラーを防ぎます。\n",
    "2.\tテキストデータの前処理:\n",
    "  - 自然言語データはそのままではモデルが理解できないため、数値データに変換する必要があります。\n",
    "3.\t学習・推論の効率化:\n",
    "  - モデルが効率的に動作するよう、トークナイザーがテキストを適切に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パディングトークンが設定されていない場合、EOSトークンを設定\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは、トークナイザー（tokenizer）において **パディングトークン（pad_token）** が設定されていない場合、**終了トークン（eos_token）** をパディングトークンとして設定する処理です。\n",
    "\n",
    "各トークンの説明\n",
    "\n",
    "1. パディングトークン（pad_token）\n",
    "\n",
    "- パディングトークンは、入力の長さを揃えるために使用される特殊なトークンです。\n",
    "- 入力データ（テキスト）が異なる長さの場合、モデルに一括処理させるため、短い入力をパディングトークンで補います。\n",
    "例:\n",
    "  - 入力A: [\"Hello\", \"world\", \"!\"] → 長さ: 3\n",
    "  - 入力B: [\"Hi\", \"!\"] → 長さ: 2\n",
    "  - パディング後:\n",
    "    - A: [\"Hello\", \"world\", \"!\", \"<pad>\"] → 長さ: 4\n",
    "    - B: [\"Hi\", \"!\", \"<pad>\", \"<pad>\"] → 長さ: 4\n",
    "\n",
    "1. 終了トークン（eos_token）\n",
    "\n",
    "- EOSトークン（End of Sequence）は、テキストの終了を示すための特殊なトークンです。\n",
    "- 主に、テキスト生成タスクやデコーダー部分で利用されます。\n",
    "例:\n",
    "  - 入力: [\"Hello\", \"world\", \"!\"]\n",
    "  - EOS追加後: [\"Hello\", \"world\", \"!\", \"<eos>\"]\n",
    "\n",
    "コードの目的\n",
    "\n",
    "1.\tパディングトークンが設定されていない場合への対処\n",
    "  - 一部のトークナイザーでは、初期状態でpad_tokenが設定されていないことがあります。\n",
    "  - このコードでは、pad_tokenが未設定の場合に、eos_tokenを代用として設定しています。\n",
    "2.\tモデルの動作を安定化\n",
    "  - パディングが必要な場面（例: バッチ処理）でエラーを防ぎ、モデルが正常に動作するようにします。\n",
    "\n",
    "なぜEOSトークンを代用するのか？\n",
    "\n",
    "- パディングトークンと終了トークンの役割の類似性:\n",
    "  - どちらも「特別な意味を持つトークン」で、通常の単語や記号とは異なります。\n",
    "  - パディングトークンが未設定の場合、終了トークンを一時的に代用しても大きな問題はありません。\n",
    "- 実用上の簡便さ:\n",
    "  - パディングトークンを新たに作成するよりも、既に存在する終了トークンを使う方が簡単でエラーを減らせます。\n",
    "\n",
    "例えで理解する\n",
    "\n",
    "- パディングトークンの役割:\n",
    "  - 同じ長さに揃えるために、ノートのページを空白で埋めるイメージ。\n",
    "  - ノートAは3ページ、ノートBは2ページだとすると、2つを揃えるためにBに空白ページを1枚追加。\n",
    "- EOSトークンを代用する理由:\n",
    "  - パディング用の空白ページがない場合、終了ページ（例えば「これで終了」という記載）を代用するイメージ。\n",
    "  - 完全な空白ではないが、十分に意味が通じるため問題がない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パディングを右側に設定(fp16を使う際のオーバーフロー対策)\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは、トークナイザー（tokenizer）に対して、パディングを入力シーケンスの右側に追加するように設定するものです。これにより、トークナイザーが入力データを統一した長さに揃える際、必要なパディングトークンを右側に挿入します。\n",
    "\n",
    "**パディングとは？**\n",
    "\n",
    "- 目的:\n",
    "  - モデルが異なる長さの入力を扱う際に一括処理できるように、短い入力にパディングトークン（<pad>など）を追加します。\n",
    "- 例:\n",
    "\n",
    "入力1: [\"Hello\", \"world\"] 　\n",
    "<br>\n",
    "入力2: [\"Hi\"]\n",
    "\n",
    "→ パディング後（右側に追加）:\n",
    "\n",
    "入力1: [\"Hello\", \"world\"]　　\n",
    "<br>\n",
    "入力2: [\"Hi\", \"<pad>\"]\n",
    "\n",
    "**padding_sideの役割**\n",
    "\n",
    "padding_sideは、どちら側にパディングトークンを追加するかを指定する属性です。\n",
    "- \"right\"（右側）: 入力シーケンスの末尾にパディングを追加。\n",
    "- \"left\"（左側）: 入力シーケンスの先頭にパディングを追加。\n",
    "\n",
    "例: \"right\" vs \"left\"\n",
    "\n",
    "- 入力データ:\n",
    "\n",
    "[\"Hello\", \"world\"]\n",
    "<br>\n",
    "[\"Hi\"]\n",
    "\n",
    "\n",
    "- padding_side = \"right\"（右側）:\n",
    "\n",
    "[\"Hello\", \"world\"]\n",
    "<br>\n",
    "[\"Hi\", \"<pad>\"]\n",
    "\n",
    "\n",
    "- padding_side = \"left\"（左側）:\n",
    "\n",
    "[\"<pad>\", \"Hi\"]\n",
    "\n",
    "コードの目的\n",
    "\n",
    "1. fp16オーバーフロー対策\n",
    "\n",
    "- fp16（半精度浮動小数点演算）:\n",
    "  - モデルの計算において、メモリ効率を向上させるために使用されます。\n",
    "  - ただし、数値が非常に大きくなると「オーバーフロー」が発生し、計算結果が不正確になるリスクがあります。\n",
    "- 右側にパディングを追加する理由:\n",
    "  - モデルのアテンション（Attention）計算では、入力シーケンスの重要な部分（トークン）に焦点を当てます。\n",
    "  - パディングトークンを右側に追加することで、重要な情報を含む部分がシーケンスの先頭に集まりやすくなります。\n",
    "  - これにより、オーバーフローの影響を最小限に抑えることができます。\n",
    "\n",
    "1. パフォーマンス向上\n",
    "\n",
    "- 多くの自然言語処理（NLP）モデルでは、右側パディングが効率的に処理されるよう最適化されています。\n",
    "- 特にトランスフォーマーモデルでは、左側にパディングを追加するよりも右側の方が一般的で、計算負荷を軽減できます。\n",
    "\n",
    "例えで理解する\n",
    "\n",
    "- パディングを右側に追加する理由:\n",
    "  - 重要なテキスト部分が入力の「前側」に集まるようにすることで、モデルが最初に重要な部分を処理しやすくします。\n",
    "  - 例えば、映画を編集する際に、重要なシーンを最初に配置して目立たせるイメージです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルから4ビット量子化された線形層の名前を取得する関数\n",
    "def find_all_linear_names(model):\n",
    "    target_class = bnb.nn.Linear4bit\n",
    "    linear_layer_names = set()\n",
    "    for name_list, module in model.named_modules():\n",
    "        if isinstance(module, target_class):\n",
    "            names = name_list.split('.')\n",
    "            layer_name = names[-1] if len(names) > 1 else names[0]\n",
    "            linear_layer_names.add(layer_name)\n",
    "    if 'lm_head' in linear_layer_names:\n",
    "        linear_layer_names.remove('lm_head')\n",
    "    return list(linear_layer_names)\n",
    "\n",
    "# モジュールのリストとして線形層の名前を取得\n",
    "target_modules = find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**コードの概要**\n",
    "\n",
    "このコードは、モデル内にある4ビット量子化された線形層（Linear4bit）の名前をリストとして取得する関数です。以下に、その動作や背景を詳しく説明します。\n",
    "\n",
    "**目的**\n",
    "\n",
    "- 量子化された線形層を見つける:\n",
    "- モデル内のすべてのモジュールをチェックし、Linear4bitに該当するモジュールの名前を収集。\n",
    "- 線形層（Linear）は、ニューラルネットワークの基本的な計算単位。\n",
    "\n",
    "**引数**\n",
    "\n",
    "- model: モデルオブジェクト。PyTorchで定義されたニューラルネットワーク。\n",
    "\n",
    "**戻り値**\n",
    "\n",
    "- 4ビット量子化された線形層の名前のリスト（list）。\n",
    "\n",
    "**コードの動作の流れ**\n",
    "\n",
    "1.\ttarget_class の設定\n",
    "\n",
    "`target_class = bnb.nn.Linear4bit`\n",
    "\n",
    "- bnb.nn.Linear4bit:\n",
    "- Bits and Bytesライブラリが提供する「4ビット量子化された線形層」のクラス。\n",
    "- 通常の線形層よりもメモリ効率が高い。\n",
    "\n",
    "2.\t線形層の名前を格納するセットを初期化\n",
    "\n",
    "`linear_layer_names = set()`\n",
    "\n",
    "- 名前を一意に保存するためにセット（set）を使用。\n",
    "- 重複した名前を自動的に排除。\n",
    "\n",
    "3.\tモデル内のモジュールを探索\n",
    "\n",
    "```python\n",
    "for name_list, module in model.named_modules():\n",
    "    if isinstance(module, target_class):\n",
    "        names = name_list.split('.')\n",
    "        layer_name = names[-1] if len(names) > 1 else names[0]\n",
    "        linear_layer_names.add(layer_name)\n",
    "```\n",
    "\n",
    "- model.named_modules():\n",
    "- モデル内のすべてのモジュール（レイヤー）を名前付きで取得します。\n",
    "- 例: (\"layer1.fc\", Linear4bit) のようなタプルが取得される。\n",
    "- isinstance(module, target_class):\n",
    "- モジュールがLinear4bitクラスに属している場合のみ処理。\n",
    "- 名前を分割・抽出:\n",
    "- 名前（name_list）を.で分割し、最後の部分（names[-1]）を取得。\n",
    "- ネストされた名前空間でも正しく処理される。\n",
    "\n",
    "4.\tlm_head を除外\n",
    "\n",
    "if 'lm_head' in linear_layer_names:\n",
    "    linear_layer_names.remove('lm_head')\n",
    "\n",
    "- lm_head:\n",
    "- モデルの出力部分に該当するレイヤー。\n",
    "- 多くの場合、最終出力に関連するため、量子化の対象外とされる。\n",
    "\n",
    "5.\t結果をリストで返す\n",
    "\n",
    "return list(linear_layer_names)\n",
    "\n",
    "- 結果をセット（set）からリスト（list）に変換して返す。\n",
    "\n",
    "**このコードを使う理由**\n",
    "\n",
    "1.\t効率的なモジュール検索\n",
    "- モデル全体から特定のモジュール（この場合は4ビット量子化された線形層）を簡単に特定。\n",
    "- モジュール数が多い大規模モデルで特に有用。\n",
    "2.\tLoRA（Low-Rank Adaptation）との統合\n",
    "- この関数はLoRA（低ランク適応手法）を適用する際に、適用対象となる線形層を動的に選択するために使われることが多い。\n",
    "- 例: 以下のように、LoRA設定で使用します。\n",
    "\n",
    "```python\n",
    "target_modules = find_all_linear_names(model)\n",
    "lora_config = LoraConfig(target_modules=target_modules, ...)\n",
    "```\n",
    "\n",
    "\n",
    "3.\t柔軟性\n",
    "- モジュール名を動的に取得することで、異なるモデル構造にも対応可能。\n",
    "\n",
    "**例えで理解する**\n",
    "\n",
    "- モデルは「巨大な工場」、モジュールは「工場内の機械」。\n",
    "- この関数は、「特定の種類の機械（4ビット線形層）」だけを探し出し、その名前をリスト化する「点検リスト作成ツール」のようなものです。\n",
    "- 例えば、ある特定の作業（LoRA適用など）を「指定の機械」だけに行うために必要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLoRAの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRAのConfigを設定\n",
    "Lora_config = LoraConfig(\n",
    "    lora_alpha=8, # LoRAによる学習の影響力を調整（スケーリング)\n",
    "    lora_dropout=0.1, # ドロップアウト率\n",
    "    r=4, # 低ランク行列の次元数\n",
    "    bias=\"none\", # バイアスのパラメータ更新\n",
    "    task_type=\"CAUSAL_LM\", # タスクの種別\n",
    "    target_modules=target_modules # LoRAを適用するモジュールのリスト\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは、**LoRA（Low-Rank Adaptation）** の設定を定義するためのものです。\n",
    "LoRAは、モデルの微調整（fine-tuning）を効率的に行う手法で、特定の部分（通常は線形層）を低ランク行列に置き換え、学習するパラメータ数を大幅に削減します。\n",
    "\n",
    "コードの目的\n",
    "\n",
    "1.\tLoRAを使ったモデル微調整の設定:\n",
    "    - モデルの特定のモジュール（層）に対して、LoRAを適用します。\n",
    "    - 必要なハイパーパラメータを指定して、LoRAの動作を調整します。\n",
    "2.\t計算効率を向上:\n",
    "    - モデル全体を微調整する代わりに、LoRAで軽量かつ効率的な学習を可能にします。\n",
    "3.\t柔軟な適用:\n",
    "    - 特定のモジュール（target_modules）だけを対象にすることで、重要な部分だけを学習対象にできます。\n",
    "\n",
    "各パラメータの解説\n",
    "\n",
    "1. lora_alpha=8\n",
    "\n",
    "- LoRAスケーリング係数:\n",
    "  - LoRAの更新部分（低ランク行列）の出力に掛けるスケール係数。\n",
    "  - この値が大きいほど、LoRAによるモデルの出力への影響が大きくなります。\n",
    "  - 例え: スピーカーの音量調節のようなものです。音量を上げたり下げたりして、学習の「影響力」を調整します。\n",
    "\n",
    "1. lora_dropout=0.1\n",
    "\n",
    "- ドロップアウト率:\n",
    "  - ドロップアウトは、過学習を防ぐためにランダムにニューロンを無効化する手法。\n",
    "  - 0.1は、10%のニューロンを無効化する設定を意味します。\n",
    "  - LoRA層の一部出力を無効化することで、汎化性能（未学習データへの適応能力）を向上させます。\n",
    "\n",
    "1. r=4\n",
    "\n",
    "- 低ランク行列のランク（次元数）:\n",
    "  - LoRAで追加される低ランク行列の次元を指定します。\n",
    "  - この値が小さいほど、学習パラメータ数が減り、計算効率が高まります。\n",
    "  - 例え: 「大きなデータを小さな表に圧縮して、それだけを学習する」イメージです。\n",
    "\n",
    "1. bias=\"none\"\n",
    "\n",
    "- バイアスパラメータの扱い:\n",
    "  - none: バイアスは学習しない設定。\n",
    "  - 他に \"all\"（全てのバイアスを学習する）や \"lora_only\"（LoRA層だけ学習）などの選択肢もあります。\n",
    "  - バイアスを固定することで、モデルの安定性を保ちつつ計算コストを削減します。\n",
    "\n",
    "1. task_type=\"CAUSAL_LM\"\n",
    "\n",
    "- タスクタイプ:\n",
    "  - CAUSAL_LMは、因果的言語モデル（Causal Language Modeling）の略。\n",
    "  - 次の単語を予測するタスク（例: GPTモデル）に対応した設定。\n",
    "  - この設定を基に、LoRAが適用される方法が調整されます。\n",
    "\n",
    "1. target_modules=target_modules\n",
    "\n",
    "- LoRAを適用するモジュール（層）:\n",
    "  - 事前に定義したtarget_modulesに基づいて、LoRAを適用する層を限定します。\n",
    "  - 通常は、量子化された線形層（Linear4bit）を対象とします。\n",
    "  - 例え: 部分的に改良する対象を選ぶようなもの（例: 家の中で修理が必要な家具だけを選ぶ）。\n",
    "\n",
    "なぜこの設定を行うのか？\n",
    "\n",
    "1.\t効率的な学習:\n",
    "  - モデル全体ではなく、一部の層にLoRAを適用することで、学習時間やメモリ使用量を削減します。\n",
    "2.\t汎化性能を向上:\n",
    "  - ドロップアウトや低ランク行列を使用することで、過学習を防ぎ、未知のデータへの対応力を向上させます。\n",
    "3.\tカスタマイズ性:\n",
    "  - パラメータを調整することで、タスクやモデルに応じた最適な微調整が可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習パラメータを設定\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./train_logs\", # ログの出力ディレクトリ\n",
    "    fp16=True, # fp16を使用\n",
    "    logging_strategy='epoch', # 各エポックごとにログを保存（デフォルトは\"steps\"）\n",
    "    save_strategy='epoch', # 各エポックごとにチェックポイントを保存（デフォルトは\"steps\"）\n",
    "    num_train_epochs=3, # 学習するエポック数\n",
    "    per_device_train_batch_size=1, # （GPUごと）一度に処理するバッチサイズ\n",
    "    gradient_accumulation_steps=4, # 勾配を蓄積するステップ数\n",
    "    optim=\"paged_adamw_32bit\", # 最適化アルゴリズム\n",
    "    learning_rate=5e-4, # 初期学習率\n",
    "    lr_scheduler_type=\"cosine\", # 学習率スケジューラの種別\n",
    "    max_grad_norm=0.3, # 勾配の最大ノルムを制限（クリッピング）\n",
    "    warmup_ratio=0.03, # 学習を増加させるウォームアップ期間の比率\n",
    "    weight_decay=0.001, # 重み減衰率\n",
    "    group_by_length=True,# シーケンスの長さが近いものをまとめてバッチ化\n",
    "    report_to=\"tensorboard\" # TensorBoard使用してログを生成（\"./train_logs\"に保存）\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは、モデルを学習する際に使用する 学習パラメータ（Training Arguments） を設定しています。Hugging Faceのトレーニングループ（Trainerクラス）に渡すことで、モデルの学習が効率的かつ管理しやすくなります。\n",
    "\n",
    "**コードの目的**\n",
    "\n",
    "1.\tトレーニングの動作を管理:\n",
    "  - ログ出力、モデルの保存、学習の進行状況の制御など、学習全体の設定を管理します。\n",
    "2.\t計算資源の効率化:\n",
    "  - fp16やgradient_accumulationでGPUのメモリを節約しながら、大きなモデルの学習を可能にします。\n",
    "3.\t学習効率の向上:\n",
    "  - 適切な学習率スケジューリングや重み減衰（weight decay）を設定し、学習を安定させます。\n",
    "4.\t結果の可視化:\n",
    "  - TensorBoardにログを出力することで、学習中の変化をリアルタイムで確認可能です。\n",
    "\n",
    "**各パラメータの解説**\n",
    "\n",
    "1. output_dir=\"./train_logs\"\n",
    "\n",
    "- モデルやログの保存先ディレクトリ:\n",
    "  - 学習中に生成されるログやチェックポイント（途中保存したモデル）を保存するディレクトリを指定。\n",
    "  - ここにモデルの状態が保存されるため、途中で停止しても再開が可能。\n",
    "\n",
    "1. fp16=True\n",
    "\n",
    "- 半精度浮動小数点（fp16）を使用:\n",
    "  - モデルの重みや計算を半精度（16ビット）で行い、メモリ使用量を減少。\n",
    "  - 高速化も期待できますが、数値精度に若干の影響を及ぼす場合もあります。\n",
    "\n",
    "1. logging_strategy='epoch'\n",
    "\n",
    "- ログの保存頻度を設定:\n",
    "  - 各エポックの終了時にログを出力します。\n",
    "  - デフォルトの”steps”（ステップごと）ではなく、エポック単位で出力することで、頻繁なログ出力を抑制。\n",
    "\n",
    "1. save_strategy='epoch'\n",
    "\n",
    "- チェックポイントの保存頻度を設定:\n",
    "  - モデルの途中経過を保存する頻度を設定。\n",
    "  - 各エポック終了後にモデルの状態を保存します。\n",
    "\n",
    "1. num_train_epochs=3\n",
    "\n",
    "- 学習するエポック数:\n",
    "  - モデル全体のデータセットを3回繰り返して学習します。\n",
    "  - データのサイズや計算資源に応じて調整可能。\n",
    "\n",
    "1. per_device_train_batch_size=1\n",
    "\n",
    "- バッチサイズ:\n",
    "  - 1回の学習ステップで処理するデータの数を設定。\n",
    "  - 各デバイス（例: GPU）でのバッチサイズが1になります。\n",
    "  - モデルやデータの大きさによっては、メモリ制限のため小さくする必要があります。\n",
    "\n",
    "1. gradient_accumulation_steps=4\n",
    "\n",
    "- 勾配の蓄積:\n",
    "  - バッチサイズが小さい場合、4ステップ分の勾配を蓄積してからモデルを更新します。\n",
    "  - 実質的にバッチサイズが「1 × 4 = 4」となるため、メモリ効率を保ちながら学習が安定。\n",
    "\n",
    "1. optim=\"paged_adamw_32bit\"\n",
    "\n",
    "- 最適化アルゴリズム:\n",
    "  - AdamWの32ビット版（効率的な重み減衰を含む）。\n",
    "  - paged_adamwはメモリ節約のため特化したバージョン。\n",
    "\n",
    "1. learning_rate=5e-4\n",
    "\n",
    "- 初期学習率:\n",
    "  - 学習を始める際の学習率を指定。\n",
    "  - 大きすぎると学習が不安定、小さすぎると収束が遅くなります。\n",
    "\n",
    "1.  lr_scheduler_type=\"cosine\"\n",
    "\n",
    "- 学習率スケジューラ:\n",
    "  - 学習率をエポックごとに調整し、コサイン曲線のように減少させる方法。\n",
    "  - 適応的に学習率を調整することで、学習の安定性を向上。\n",
    "\n",
    "1.  max_grad_norm=0.3\n",
    "\n",
    "- 勾配のクリッピング:\n",
    "  - 勾配のノルム（長さ）を0.3に制限。\n",
    "  - 勾配が大きくなりすぎると学習が不安定になるため、その影響を防ぎます。\n",
    "\n",
    "1.  warmup_ratio=0.03\n",
    "\n",
    "- ウォームアップ期間:\n",
    "  - 学習の最初の3%は徐々に学習率を上げる設定。\n",
    "  - 急激な学習率の増加による不安定性を防ぎます。\n",
    "\n",
    "1.  weight_decay=0.001\n",
    "\n",
    "- 重み減衰（正則化）:\n",
    "  - 学習中にモデルの重みを少しずつ減少させることで、過学習を防ぎます。\n",
    "\n",
    "1.  group_by_length=True\n",
    "\n",
    "- シーケンスの長さに基づくバッチ化:\n",
    "  - 入力テキストの長さが近いものをまとめてバッチを作成。\n",
    "  - 効率的な計算を実現します。\n",
    "\n",
    "1.  report_to=\"tensorboard\"\n",
    "\n",
    "- ログ出力先:\n",
    "  - TensorBoardに学習過程の情報を出力。\n",
    "  - 例えば、損失関数の変化や学習率の動きをリアルタイムで可視化可能。\n",
    "\n",
    "\n",
    "この設定を「車の燃費テスト」に例えると：\n",
    "- output_dir: テスト結果を保存するノート。\n",
    "- fp16: 軽量化したエンジンを使用。\n",
    "- gradient_accumulation: 複数回の測定を合算して燃費を計算。\n",
    "- lr_scheduler: 試運転中に徐々にスピードを上げる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFTパラメータの設定\n",
    "trainer = SFTTrainer(\n",
    "    model=model, # モデルをセット\n",
    "    tokenizer=tokenizer, # トークナイザーをセット\n",
    "    train_dataset=train_dataset, # データセットをセット\n",
    "    dataset_text_field=\"text\", # 学習に使用するデータセットのフィールド\n",
    "    peft_config=Lora_config, # LoRAのConfigをセット\n",
    "    args=training_arguments, # 学習パラメータをセット\n",
    "    max_seq_length=512, # 入力シーケンスの最大長を設定\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SFTTrainer（Supervised Fine-Tuning Trainer）とは？\n",
    "\n",
    "SFTTrainerは、**「モデルを特定のタスクに合わせて微調整（fine-tuning）するためのツール」** です。微調整とは、既に訓練されたモデル（例えば言語モデル）に新しいデータを使って学習を追加し、特定の用途に対応できるようにすることです。\n",
    "\n",
    "**簡単な例え**\n",
    "\n",
    "SFTTrainerを「スポーツの個人トレーニングコーチ」に例えましょう：\n",
    "- モデル：すでに基本技術を持っているサッカー選手。\n",
    "- データセット：練習メニュー（ドリブル練習やシュート練習の指示）。\n",
    "- 微調整：シュートの精度やスピードを高めるために、特定の練習を集中して行うこと。\n",
    "\n",
    "SFTTrainerは、この練習プランを作り、選手に適切な練習をさせるコーチの役割です。\n",
    "\n",
    "**SFTTrainerの特徴**\n",
    "\n",
    "1.\t既存のモデルを活用する:\n",
    "- すでに大量のデータで学習済みの言語モデル（例: ChatGPTやBERT）を基にします。\n",
    "- 最初から学習するより効率的で、時間や計算コストを削減できます。\n",
    "2.\t新しいデータで特定のタスクを学習:\n",
    "- 例えば「質問に答える」「文章を要約する」「特定のテーマで文章を生成する」といった新しいスキルを学習させます。\n",
    "3.\t簡単に設定・実行できる:\n",
    "- 必要なデータ、モデル、トレーニングパラメータを指定するだけで、細かい部分を自動で処理します。\n",
    "\n",
    "**SFTTrainerを使う理由**\n",
    "\n",
    "- 計算資源を節約: モデル全体を訓練するのではなく、必要な部分だけを効率的に学習できます。\n",
    "- タスクに特化: たとえば、一般的な文章生成モデルを「特定のトピックについて答えるモデル」に変えることができます。\n",
    "- 簡単なセットアップ: トレーニングループ（学習の進行管理）やログの記録などを自動化してくれます。\n",
    "\n",
    "**どうやって使うのか？**\n",
    "\n",
    "SFTTrainerを使うには、以下の情報を提供します：\n",
    "1.\tモデル: 微調整したい既存の言語モデル。\n",
    "2.\tトークナイザー: テキストデータを数値データに変換するツール。\n",
    "3.\tデータセット: モデルに学習させる具体的な例（質問と答え、入力文と出力文など）。\n",
    "4.\t学習パラメータ: 学習率やエポック数、バッチサイズなどの設定。\n",
    "5.\tLoRAなどの効率化設定: 計算資源を節約しながら学習を最適化する仕組み。\n",
    "\n",
    "例: 実際にやっていること\n",
    "\n",
    "コードの内容をざっくり言うと：\n",
    "\n",
    "- モデル: 言語モデル（文章を生成したり、質問に答えたりできるAI）。\n",
    "- データ: 「質問→答え」のペアが含まれる学習データ。\n",
    "- 目標: モデルに「質問に正確に答えるスキル」を教える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは、SFTTrainer（Supervised Fine-Tuning Trainer）を使って、モデルの微調整（fine-tuning）を実行する準備を整えています。このSFTTrainerは、Hugging FaceのTrainerクラスを拡張したもので、特にLoRAのような効率的な学習方法を使った微調整に適しています。\n",
    "\n",
    "**コードの目的**\n",
    "\n",
    "1.\t微調整用のトレーナー設定:\n",
    "- 指定したモデル、トークナイザー、データセットを使い、学習ループを管理するトレーナーを作成します。\n",
    "2.\tLoRA（Low-Rank Adaptation）を利用した効率的な微調整:\n",
    "- メモリと計算資源を節約しながら、特定のタスク向けにモデルを調整します。\n",
    "3.\tデータセットとパラメータの設定:\n",
    "- 入力テキストの形式や長さ、学習に必要なパラメータを登録します。\n",
    "\n",
    "**各パラメータの解説**\n",
    "\n",
    "1. model=model\n",
    "\n",
    "- 微調整するモデルを指定:\n",
    "  - ここでは、量子化されたモデルを設定しています。\n",
    "  - このモデルが学習対象となり、タスクに合わせて微調整されます。\n",
    "\n",
    "1. tokenizer=tokenizer\n",
    "\n",
    "- トークナイザーを指定:\n",
    "  - テキストデータを数値データに変換するためのツール。\n",
    "  - モデルと連携して、適切な形式の入力データを生成します。\n",
    "\n",
    "1. train_dataset=train_dataset\n",
    "\n",
    "- 学習用データセットを指定:\n",
    "  - モデルが学習するためのデータ。\n",
    "  - 例: テキストデータやラベル付きの文章。\n",
    "\n",
    "1. dataset_text_field=\"text\"\n",
    "\n",
    "- データセット内のテキストが格納されているフィールドを指定:\n",
    "  - データセットの中で、モデルが学習に使うテキストが含まれるカラム名（キー）を指定。\n",
    "  - 例えば、JSONやCSV形式のデータから特定のフィールドを指定します。\n",
    "\n",
    "1. peft_config=Lora_config\n",
    "\n",
    "- LoRAの設定を適用:\n",
    "  - 低ランク行列を使った効率的な学習を行うための設定。\n",
    "  - ここでは、学習におけるLoRAの影響力（lora_alpha）、ドロップアウト率（lora_dropout）、適用するモジュールなどを指定したLora_configが使われます。\n",
    "\n",
    "1. args=training_arguments\n",
    "\n",
    "- 学習パラメータを適用:\n",
    "  - TrainingArgumentsで設定した学習率、エポック数、バッチサイズなどの情報を渡します。\n",
    "  - これにより、学習ループ全体の挙動が制御されます。\n",
    "\n",
    "1. max_seq_length=512\n",
    "\n",
    "- 入力シーケンスの最大長を指定:\n",
    "  - モデルに入力される文章の最大トークン数。\n",
    "  - 長い文章がトークン数512を超えた場合は切り捨てられます。\n",
    "  - モデルの性能やメモリ使用量に応じて調整が必要です。\n",
    "\n",
    "なぜこの設定を行うのか？\n",
    "\n",
    "1.\t効率的な学習:\n",
    "  - LoRAを使ってメモリを節約しながら、高品質な微調整を行うため。\n",
    "  - 長い文章や大量のデータを扱う際に、計算資源を節約する工夫が含まれています。\n",
    "2.\tタスク適応:\n",
    "  - 特定のタスク（例: 質問応答や文章生成）にモデルを調整するため。\n",
    "  - データセットの内容や形式を明示的に指定し、学習効率を高めます。\n",
    "3.\t汎用性とカスタマイズ性:\n",
    "  - SFTTrainerは、LoRAのような特定の学習方法を簡単に組み込むことができ、カスタマイズも容易です。\n",
    "\n",
    "**例えで理解する**\n",
    "\n",
    "この設定を「サッカーの個人トレーニング」に例えると：\n",
    "- model: 練習する選手。\n",
    "- tokenizer: 練習内容を選手が理解できる言語に翻訳するコーチ。\n",
    "- train_dataset: 練習で使用するドリルやシナリオ。\n",
    "- dataset_text_field=\"text\": ボールを使う部分の練習メニュー。\n",
    "- peft_config: 特定のスキル（例: ドリブル）の強化プラン。\n",
    "- args: トレーニングスケジュールや練習の回数。\n",
    "- max_seq_length: 1回の練習で集中する時間（512分以内）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規化層をfloat32に変換(学習を安定させるため)\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの学習\n",
    "trainer.train()\n",
    "\n",
    "# 学習したアダプターを保存\n",
    "trainer.model.save_pretrained(\"./QLoRA_sample_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# huggingfaceトークンの設定（gemma2を使用するのに必要なため）\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# アダプターのパス\n",
    "adapter_path = \"./QLoRA_sample_model\"\n",
    "\n",
    "# モデルのリポジトリIDを設定\n",
    "repo_id = \"google/gemma-2-2b-jpn-it\"\n",
    "\n",
    "# ベースモデルとアダプターの読み込み\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=adapter_path, \n",
    "    device_map={\"\": \"cuda\"}, \n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# トークナイザーの準備\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=repo_id, \n",
    ")\n",
    "\n",
    "# パディングトークンが設定されていない場合、EOSトークンを設定\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# パディングを右側に設定(fp16を使う際のオーバーフロー対策)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "question_list = [\n",
    "    \"名前を教えてください\",\n",
    "    \"日本の首都はどこですか\", \n",
    "    \"ジョークを言ってください\", \n",
    "    \"東北の観光地について教えてください\" \n",
    "]\n",
    "\n",
    "# 各質問に対して回答を生成\n",
    "for i, question in enumerate(question_list, 1):\n",
    "    print(f\"\\nchat_{i}----------------------------------------------------\")\n",
    "    print(f\"質問: {question}\")\n",
    "    \n",
    "    # チャットメッセージの設定\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    # トークナイザーのチャットテンプレートを適用\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # プロンプトをトークン化してテンソルに変換（GPUに転送）\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # 回答を生成\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        attention_mask=model_inputs.attention_mask,\n",
    "        max_new_tokens=300\n",
    "    )\n",
    "    \n",
    "    # 生成された回答を抽出\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    # トークンIDを文字列に変換\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"回答: {response}\")\n",
    "    print(\"----------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
