{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install torch\n",
    "%pip -q install transformers\n",
    "%pip -q install bitsandbytes\n",
    "%pip -q install peft\n",
    "%pip -q install trl\n",
    "%pip -q install datasets\n",
    "%pip -q install tensorboard\n",
    "%pip -q install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import bitsandbytes as bnb\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import torchinfo\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingfaceトークンの設定（gemma2を使用するのに必要なため）\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# モデルのリポジトリIDを設定\n",
    "repo_id = \"google/gemma-2-2b-jpn-it\"\n",
    "\n",
    "# データセットのパス\n",
    "dataset_path = \"./zmn.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonlファイルを読み込む\n",
    "json_data = []\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        json_data.append(json.loads(line))\n",
    "\n",
    "# DatasetオブジェクトにJSONデータを変換\n",
    "dataset = Dataset.from_list(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonlファイルを読み込む\n",
    "json_data = []\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        json_data.append(json.loads(line))\n",
    "\n",
    "# DatasetオブジェクトにJSONデータを変換\n",
    "dataset = Dataset.from_list(json_data)\n",
    "\n",
    "# プロンプトフォーマット\n",
    "PROMPT_FORMAT = \"\"\"<start_of_turn>user\n",
    "{system}\n",
    "\n",
    "{instruction}\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{output}\n",
    "<end_of_turn>\n",
    "\"\"\"\n",
    "\n",
    "# データセットの内容をプロンプトにセット → textフィールドとして作成する関数\n",
    "def generate_text_field(data):\n",
    "    messages = data[\"messages\"]\n",
    "    system = \"\"\n",
    "    instruction = \"\"\n",
    "    output = \"\"\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            system = message[\"content\"]\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            instruction = message[\"content\"]\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            output = message[\"content\"]  \n",
    "    full_prompt = PROMPT_FORMAT.format(system=system, instruction=instruction, output=output) \n",
    "    return {\"text\": full_prompt}\n",
    "\n",
    "# データセットに（generate_text_fieldの処理を用いて）textフィールドを追加\n",
    "train_dataset = dataset.map(generate_text_field)\n",
    "\n",
    "# messagesフィールドを削除\n",
    "train_dataset = train_dataset.remove_columns([\"messages\"]) \n",
    "\n",
    "# 量子化のConfigを設定\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # 4ビット量子化を使用\n",
    "    bnb_4bit_quant_type=\"nf4\", # 4ビット量子化の種類にnf4（NormalFloat4）を使用\n",
    "    bnb_4bit_use_double_quant=True, # 二重量子化を使用\n",
    "    bnb_4bit_compute_dtype=torch.float16 # 量子化のデータ型をfloat16に設定\n",
    ")\n",
    "\n",
    "# モデルの読み込み\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=repo_id, # モデルのリポジトリIDをセット\n",
    "    device_map={\"\": \"cuda\"}, # 使用デバイスを設定\n",
    "    quantization_config=quantization_config, # 量子化のConfigをセット\n",
    "    attn_implementation=\"eager\", # 注意機構に\"eager\"を設定（Gemma2モデルの学習で推奨されているため）\n",
    ")\n",
    "\n",
    "# キャッシュを無効化（メモリ使用量を削減）\n",
    "model.config.use_cache = False \n",
    "\n",
    "# テンソル並列ランクを１に設定（テンソル並列化を使用しない）\n",
    "model.config.pretraining_tp = 1 \n",
    "\n",
    "# トークナイザーの準備\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=repo_id, # モデルのリポジトリIDをセット\n",
    "    attn_implementation=\"eager\", # 注意機構に\"eager\"を設定（Gemma2モデルの学習で推奨されているため）\n",
    "    add_eos_token=True, # EOSトークンの追加を設定\n",
    ")\n",
    "\n",
    "# パディングトークンが設定されていない場合、EOSトークンを設定\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# パディングを右側に設定(fp16を使う際のオーバーフロー対策)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# モデルから4ビット量子化された線形層の名前を取得する関数\n",
    "def find_all_linear_names(model):\n",
    "    target_class = bnb.nn.Linear4bit\n",
    "    linear_layer_names = set()\n",
    "    for name_list, module in model.named_modules():\n",
    "        if isinstance(module, target_class):\n",
    "            names = name_list.split('.')\n",
    "            layer_name = names[-1] if len(names) > 1 else names[0]\n",
    "            linear_layer_names.add(layer_name)\n",
    "    if 'lm_head' in linear_layer_names:\n",
    "        linear_layer_names.remove('lm_head')\n",
    "    return list(linear_layer_names)\n",
    "\n",
    "# モジュールのリストとして線形層の名前を取得\n",
    "target_modules = find_all_linear_names(model)\n",
    "\n",
    "# LoRAのConfigを設定\n",
    "Lora_config = LoraConfig(\n",
    "    lora_alpha=8, # LoRAによる学習の影響力を調整（スケーリング)\n",
    "    lora_dropout=0.1, # ドロップアウト率\n",
    "    r=4, # 低ランク行列の次元数\n",
    "    bias=\"none\", # バイアスのパラメータ更新\n",
    "    task_type=\"CAUSAL_LM\", # タスクの種別\n",
    "    target_modules=target_modules # LoRAを適用するモジュールのリスト\n",
    ")\n",
    "\n",
    "# 学習パラメータを設定\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./train_logs\", # ログの出力ディレクトリ\n",
    "    fp16=True, # fp16を使用\n",
    "    logging_strategy='epoch', # 各エポックごとにログを保存（デフォルトは\"steps\"）\n",
    "    save_strategy='epoch', # 各エポックごとにチェックポイントを保存（デフォルトは\"steps\"）\n",
    "    num_train_epochs=3, # 学習するエポック数\n",
    "    per_device_train_batch_size=1, # （GPUごと）一度に処理するバッチサイズ\n",
    "    gradient_accumulation_steps=4, # 勾配を蓄積するステップ数\n",
    "    optim=\"paged_adamw_32bit\", # 最適化アルゴリズム\n",
    "    learning_rate=5e-4, # 初期学習率\n",
    "    lr_scheduler_type=\"cosine\", # 学習率スケジューラの種別\n",
    "    max_grad_norm=0.3, # 勾配の最大ノルムを制限（クリッピング）\n",
    "    warmup_ratio=0.03, # 学習を増加させるウォームアップ期間の比率\n",
    "    weight_decay=0.001, # 重み減衰率\n",
    "    group_by_length=True,# シーケンスの長さが近いものをまとめてバッチ化\n",
    "    report_to=\"tensorboard\" # TensorBoard使用してログを生成（\"./train_logs\"に保存）\n",
    ")\n",
    "\n",
    "# SFTパラメータの設定\n",
    "trainer = SFTTrainer(\n",
    "    model=model, # モデルをセット\n",
    "    tokenizer=tokenizer, # トークナイザーをセット\n",
    "    train_dataset=train_dataset, # データセットをセット\n",
    "    dataset_text_field=\"text\", # 学習に使用するデータセットのフィールド\n",
    "    peft_config=Lora_config, # LoRAのConfigをセット\n",
    "    args=training_arguments, # 学習パラメータをセット\n",
    "    max_seq_length=512, # 入力シーケンスの最大長を設定\n",
    ")\n",
    "\n",
    "# 正規化層をfloat32に変換(学習を安定させるため)\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "# モデルの学習\n",
    "trainer.train()\n",
    "\n",
    "# 学習したアダプターを保存\n",
    "trainer.model.save_pretrained(\"./QLoRA_sample_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# huggingfaceトークンの設定（gemma2を使用するのに必要なため）\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# アダプターのパス\n",
    "adapter_path = \"./QLoRA_sample_model\"\n",
    "\n",
    "# モデルのリポジトリIDを設定\n",
    "repo_id = \"google/gemma-2-2b-jpn-it\"\n",
    "\n",
    "# ベースモデルとアダプターの読み込み\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=adapter_path, \n",
    "    device_map={\"\": \"cuda\"}, \n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# トークナイザーの準備\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=repo_id, \n",
    ")\n",
    "\n",
    "# パディングトークンが設定されていない場合、EOSトークンを設定\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# パディングを右側に設定(fp16を使う際のオーバーフロー対策)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "question_list = [\n",
    "    \"名前を教えてください\",\n",
    "    \"日本の首都はどこですか\", \n",
    "    \"ジョークを言ってください\", \n",
    "    \"東北の観光地について教えてください\" \n",
    "]\n",
    "\n",
    "# 各質問に対して回答を生成\n",
    "for i, question in enumerate(question_list, 1):\n",
    "    print(f\"\\nchat_{i}----------------------------------------------------\")\n",
    "    print(f\"質問: {question}\")\n",
    "    \n",
    "    # チャットメッセージの設定\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    # トークナイザーのチャットテンプレートを適用\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # プロンプトをトークン化してテンソルに変換（GPUに転送）\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # 回答を生成\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        attention_mask=model_inputs.attention_mask,\n",
    "        max_new_tokens=300\n",
    "    )\n",
    "    \n",
    "    # 生成された回答を抽出\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    # トークンIDを文字列に変換\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"回答: {response}\")\n",
    "    print(\"----------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
