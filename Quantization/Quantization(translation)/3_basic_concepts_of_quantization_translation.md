# A Survey of Quantization Methods for Efficient Neural Network Inference(効率的なニューラルネットワーク推論のための量子化手法に関する調査) 


## BASIC CONCEPTS OF QUANTIZATION(量子化の基本概念)  
このセクションでは、まずセクションIII-Aで一般的な記法と問題の設定を簡単に紹介し、その後セクションIII-BからIII-Fで基本的な量子化の概念と方法について説明します。その後、セクションIII-Gで異なるファインチューニング方法について議論し、セクションIII-Hで確率的量子化について説明します。

図1: 一様量子化（左）と非一様量子化（右）の比較。連続ドメインの実値 $r$ が、量子化ドメインの離散的で低精度の値 $Q$ にマッピングされており、これらの値はオレンジの点で示されています。一様量子化では量子化レベル間の距離は同じですが、非一様量子化では距離が異なることがあります。

### A. 問題設定と表記法
ニューラルネットワーク(NN)が $L$ 層の学習可能なパラメータ $\{W_{1}, W_{2}, \ldots, W_{L}\}$ を持ち、$\theta$ がこれら全てのパラメータの組み合わせを示すと仮定します。一般性を失わずに、ここでは教師あり学習の問題に焦点を当てます。その目的は、次の経験リスク最小化関数を最適化することです。

$$
\mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^{N}\mathcal{l}(x_{i}, y_{i};\theta)
$$

ここで、$(x, y)$ は入力データと対応するラベルを示し、$\mathcal{l}(x, y; θ)$ は損失関数（例えば、平均二乗誤差や交差エントロピー損失）を示します。$N$ はデータポイントの総数です。また、$i^{th}$ 番目の層の入力隠れアクティベーションを $h_{i}$、対応する出力隠れアクティベーションを $a_{i}$ と表記します。トレーニング済みのモデルパラメータ $\theta$ は浮動小数点精度で保存されていると仮定します。量子化の目的は、パラメータ $(\theta)$ と中間アクティベーションマップ（すなわち、$h_{i}$ と $a_{i}$）の両方の精度を低精度に減少させ、モデルの一般化能力や精度に最小限の影響を与えることです。これを実現するためには、浮動小数点値を量子化された値にマッピングする量子化演算子を定義する必要があります。これについては次に説明します。

### B. 一様量子化
まず、ニューラルネットワーク(NN)の重みとアクティベーションを有限の値の集合に量子化できる関数を定義する必要があります。この関数は、浮動小数点の実数値を取り、それを低精度の範囲にマッピングします。これについては図1で示されています。量子化関数の一般的な選択肢は以下の通りです。

$$
Q(r) = \text{Int}(r/S) - Z
$$

ここで、$Q$ は量子化演算子、$r$ は実数値の入力（アクティベーションまたは重み）、$S$ は実数値のスケーリングファクター、$Z$ は整数のゼロポイントです。さらに、$\text{Int}$ 関数は実数値を整数値に変換するための丸め操作（例えば、最も近い整数への丸めや切り捨て）を行います。基本的に、この関数は実数値 $r$ をいくつかの整数値にマッピングします。この量子化の方法は一様量子化としても知られており、結果として得られる量子化値（すなわち量子化レベル）が均等に間隔を空けて配置されます（図1、左）。また、量子化値が必ずしも均等に配置されていない非一様量子化の方法もあり（図1、右）、これについてはセクション III-F で詳しく説明されます。量子化された値 $Q(r)$ から実数値 r を回復するための操作は、一般にデ量子化と呼ばれます。

注意してください。回復された実数値 $\tilde{r}$ は、丸め操作のために元の実数値 $r$ と正確には一致しないことがあります。

図2: 対称的量子化と非対称的量子化の図解。対称的量子化では制限された範囲が実数値を [-127, 127] にマッピングし、フルレンジでは [-128, 127] にマッピングします（8ビット量子化の場合）。

### C. 対称的および非対称的量子化
一様量子化において重要な要素の1つは、式2におけるスケーリングファクター $S$ の選択です。このスケーリングファクターは、実数値 r の与えられた範囲を複数の区間に分割します（[113, 133] で議論されています）。

$$
S = \frac{\beta - \alpha}{2^{b} - 1}
$$

ここで、$[\alpha, \beta]$ はクリッピング範囲を示し、これは実数値をクリッピングするための有界範囲です。また、$b$ は量子化ビット幅を示します。したがって、スケーリングファクターを定義するためには、まずクリッピング範囲 $[\alpha, \beta]$ を決定する必要があります。クリッピング範囲を選択するプロセスは、一般的にキャリブレーションと呼ばれます。

簡単な選択肢としては、信号の最小値および最大値をクリッピング範囲として使用する方法があります。すなわち、$\alpha = r_{min}、\beta = r_{max}$ です。このアプローチは非対称的な量子化方式であり、クリッピング範囲が原点に対して対称でないため、$-\alpha \neq \beta$ になります（図2、右参照）。対称的な量子化方式を使用することも可能で、$\alpha = -\beta$ の対称的なクリッピング範囲を選択します。一般的な選択肢としては、信号の最小値および最大値に基づいて選ぶ方法があります: $−\alpha = \beta = max(|r_{max}|, |r_{min}|)$。非対称的な量子化は、対称的な量子化に比べてクリッピング範囲がより狭くなることがよくあります。これは特に、$\text{ReLU}$ 後のように常に非負の値を持つアクティベーションのようにターゲットの重みやアクティベーションが不均衡な場合に重要です。しかし、対称的な量子化を使用すると、式2の量子化関数が簡略化され、ゼロポイントを $Z = 0$ に置き換えることができます。

$$
Q(r) = \text{Int}(\frac{r}{S})
$$

ここでは、スケーリングファクターに対して2つの選択肢があります。「フルレンジ」対称量子化では、スケーリングファクター $S$ は $\frac{2 \cdot \max(|r|)}{2^{n-1}}$（切り捨て丸めモード）として選ばれ、$\text{INT8}$ のフルレンジ $[-128, 127]$ が使用されます。しかし、「制限範囲」では、スケーリングファクター $S$ は $\frac{\max(|r|)}{2^{n-1}-1}$ として選ばれ、範囲は $[-127, 127]$ のみが使用されます。予想通り、フルレンジアプローチの方がより正確です。

対称的量子化は、ゼロポイントをゼロにすることで推論中の計算コストを削減できるため、重みの量子化において広く採用されています [255]。また、実装がより簡単になるという利点もあります。ただし、アクティベーションについては、非対称アクティベーションのオフセットによって生じる交差項は静的なデータ非依存項であり、バイアスに吸収することができます（またはアキュムレータの初期化に使用することができます） [15]。

信号の最小値/最大値を使用する方法は、対称的および非対称的量子化で一般的ですが、このアプローチはアクティベーションにおける外れ値に対して脆弱です。外れ値が範囲を不必要に広げ、その結果、量子化の解像度が低下する可能性があります。この問題に対処するための $1$ つのアプローチは、信号の最小値/最大値の代わりにパーセンタイルを使用することです [172]。つまり、最大値/最小値の代わりに、$i$ 番目に大きい/小さい値を $\beta/\alpha$ として使用します。別のアプローチは、実値と量子化された値の間の $\text{KL}$ ダイバージェンス（情報損失）を最小化するように $\alpha$ と $\beta$ を選択することです [176]。興味のある読者には、さまざまなモデルで異なるキャリブレーション方法が評価されている [255] を参照することをお勧めします。

要約（対称的量子化 vs 非対称的量子化）。対称的量子化は対称的な範囲を使用してクリッピングを分割します。これにより、式2で $Z = 0$ となるため、実装が容易になります。ただし、範囲が歪んで対称でない場合には最適ではありません。このような場合には、非対称的量子化が推奨されます。

図3: 異なる量子化の粒度の図解。レイヤーごとの量子化では、同じレイヤーに属するすべてのフィルタに対して同じクリッピング範囲が適用されます。これにより、分布が狭いチャネル（例えば、図中のフィルタ1）に対しては量子化の解像度が低くなる可能性があります。チャネルごとの量子化を使用すると、異なるクリッピング範囲を異なるチャネルに割り当てることで、より良い量子化解像度を達成することができます。

### D. 範囲キャリブレーションアルゴリズム: 静的 vs 動的量子化
次に、$[α, β]$ のクリッピング範囲を決定するためのさまざまなキャリブレーション方法について議論しました。量子化方法のもう一つの重要な違いは、クリッピング範囲が決定されるタイミングです。この範囲は、重み（ウェイト）については静的に計算することができます。ほとんどの場合、推論時にはパラメータが固定されているためです。しかし、活性化マップ（式1における $x$ ）は各入力サンプルごとに異なります。そのため、活性化関数の量子化には、動的量子化と静的量子化という2つのアプローチがあります。

動的量子化では、この範囲が実行時に各活性化マップごとに動的に計算されます。このアプローチでは、信号の統計情報（最小値、最大値、パーセンタイルなど）をリアルタイムで計算する必要があり、非常に大きなオーバーヘッドが発生する可能性があります。しかし、動的量子化は各入力に対して信号範囲を正確に計算するため、より高い精度を得ることがよくあります。

もう一つの量子化アプローチは静的量子化です。この方法では、推論時にクリッピング範囲が事前に計算され、固定されます。このアプローチは計算負荷を追加することはありませんが、動的量子化と比較して通常は精度が低くなります。一般的な事前計算方法として、キャリブレーション用の入力データを一連に渡して、典型的な活性化範囲を計算することがあります [113, 267]。最適な範囲を見つけるために提案されているさまざまな指標には、元の非量子化重み分布と対応する量子化値との間の平均二乗誤差 (MSE) を最小化することが含まれています [40, 221, 229, 281]。他にもエントロピー [189] などの指標を使うことも考えられますが、MSEが最も一般的に使用される方法です。もう一つのアプローチとして、ニューラルネットワーク (NN) のトレーニング中にこのクリッピング範囲を学習または適用することもあります [36, 146, 276, 287]。ここで注目すべき研究として、LQNet [276]、PACT [36]、LSQ [56]、およびLSQ+ [15] が挙げられ、これらはクリッピング範囲とNN内の重みをトレーニング中に同時に最適化します。

要約（動的量子化 vs 静的量子化）。動的量子化は各活性化のクリッピング範囲を動的に計算し、しばしば最高の精度を達成します。しかし、信号の範囲を動的に計算することは非常にコストが高いため、実務では多くの場合、全ての入力に対してクリッピング範囲が固定される静的量子化が使用されます。

### E. 量子化の粒度  
ほとんどのコンピュータビジョンのタスクでは、層への活性化入力が多数の異なる畳み込みフィルタと畳み込まれます（図3に示されています）。これらの畳み込みフィルタのそれぞれが異なる値の範囲を持つことがあります。そのため、量子化方法の区別点の一つは、重みに対してクリッピング範囲[α, β]がどのように計算されるかという粒度です。これを次のように分類しました。  
a) 層単位の量子化：このアプローチでは、層内のすべての畳み込みフィルタの重みを考慮してクリッピング範囲を決定します [133]。図3の3列目に示されているように、層内の全パラメータの統計情報（例えば、最小値、最大値、パーセンタイルなど）を調べ、その後、すべての畳み込みフィルタに対して同じクリッピング範囲を使用します。この方法は非常にシンプルに実装できますが、各畳み込みフィルタの範囲が大きく異なる場合が多いため、精度が最適でないことがよくあります。例えば、比較的狭いパラメータ範囲を持つ畳み込みカーネルが、同じ層内でより広い範囲を持つ別のカーネルのために量子化解像度を失う可能性があります。
b) グループ単位の量子化：層内の複数の異なるチャンネルをグループ化して、（活性化マップや畳み込みカーネルの）クリッピング範囲を計算する方法があります。これは、単一の畳み込みまたは活性化内でパラメータの分布が大きく異なる場合に役立ちます。例えば、このアプローチは、完全に接続された注意層を含むTransformerモデル [243] の量子化において、Q-BERT [219] で有用であることが示されています。しかし、この方法は異なるスケーリングファクターを考慮する必要があるため、追加のコストが伴います。
c) チャンネル単位の量子化：一般的な選択肢として、各畳み込みフィルタに対して、他のチャンネルとは無関係に固定された値のクリッピング範囲を使用する方法があります [105, 113, 133, 222, 276, 285]。これは図3の最後の列に示されています。つまり、各チャンネルに専用のスケーリングファクターが割り当てられます。この方法は、より良い量子化解像度を確保し、しばしばより高い精度をもたらします。
d) サブチャンネル単位の量子化：前述のアプローチをさらに徹底し、畳み込み層や全結合層のパラメータグループに対してクリッピング範囲を決定することもできます。しかし、この方法では、単一の畳み込み層や全結合層を処理する際に異なるスケーリングファクターを考慮する必要があるため、かなりのオーバーヘッドが発生する可能性があります。そのため、グループ単位の量子化は、量子化の精度と計算コストの間で良い妥協点を提供する可能性があります。
要約（量子化の粒度）。チャンネル単位の量子化は、現在、畳み込みカーネルの量子化に使用される標準的な方法です。これは、各カーネルに対してクリッピング範囲を個別に調整でき、オーバーヘッドがほとんど発生しません。それに対して、サブチャンネル単位の量子化は、かなりのオーバーヘッドを引き起こす可能性があり、現在の標準的な選択肢ではありません（これらの設計選択に関連するトレードオフについては、[68] を参照してください）。

### F. 非一様量子化

文献の中には、非一様量子化を探求した研究もあります [25, 38, 62, 74, 79, 99, 118, 125, 153, 159, 179, 189, 190, 238, 248, 256, 264, 266, 276, 284]。ここでは、量子化ステップと量子化レベルが非一様に配置されることが許されています。非一様量子化の正式な定義は式6に示されており、ここで $X_i$ は離散的な量子化レベル、$Δ_i$ は量子化ステップ（しきい値）を表します。

$$
Q(r) = X_{i}, \text{if} r \in[Δ_{i}, Δ_{i+1})
$$

具体的には、実数rの値が量子化ステップ $Δ_i$ と $Δ_{i+1}$ の間にある場合、量子化器 $Q$ はそれを対応する量子化レベル $X_i$ に投影します。なお、$X_{i}$ も $Δ_{i}$ も一様には配置されていません。非一様量子化は、固定されたビット幅でより高い精度を達成する可能性があります。これは、重要な値の領域に焦点を当てたり、適切な動的範囲を見つけたりすることで、分布をより正確に捉えることができるためです。たとえば、多くの非一様量子化手法は、ベル型の重みや活性化の分布に対して設計されており、しばしば長い尾部を伴います [12, 25, 61, 115, 147, 179]。

典型的なルールベースの非一様量子化の一例として、対数分布を使用する方法があります [179, 283]。ここでは、量子化ステップとレベルが線形ではなく指数的に増加します。もう一つの一般的な手法は、バイナリコードベースの量子化です [78, 107, 118, 258, 276]。ここでは、実数ベクトル $r ∈ R^n$ が、スケーリング係数 $α_i ∈ R$ およびバイナリベクトル $b_i ∈ {−1, +1}^n$ を用いて

$$
r ≈ \sum_{i=1}^{N} α_{i}b_{i}
$$

と表され、$m$ 個のバイナリベクトルに量子化されます。$r$ と $\sum_{i=1}^{N} α_{i}b_{i}$ との誤差を最小化するための閉形式の解は存在しないため、従来の研究ではヒューリスティックな解に依存していました。量子化器のさらなる改善のために、最近の研究 [78, 234, 258] では、非一様量子化を最適化問題として定式化しています。式7に示されているように、量子化器Q内の量子化ステップやレベルは、元のテンソルと量子化された対応物との誤差を最小化するように調整されます。

$$
\text{min}\|Q(r)-r\|^2
$$

さらに、量子化器自体もモデルのパラメータと一緒に学習することができます。これらの手法は学習可能な量子化器と呼ばれ、量子化ステップやレベルは通常、反復的な最適化 [258, 276] や勾配降下法 [125, 158, 264] で学習されます。ルールベースや最適化ベースの非一様量子化に加えて、クラスタリングも量子化による情報損失を軽減するために役立つことがあります。いくつかの研究 [74, 256] では、異なるテンソルに対してk-means法を使用して量子化ステップとレベルを決定し、他の研究 [38] では、パフォーマンスの低下を最小限に抑えるためにヘシアン重み付きk-meansクラスタリングを重みに適用しています。さらに詳しい議論はセクションIV-Fで説明されています。
要約（均一量子化 vs 非均一量子化）
一般に、非一様量子化はパラメータの範囲を非均一に分離し、ビットを割り当てることで、信号情報をより良く捉えることができます。しかし、非一様量子化方式は、GPUやCPUなどの一般的な計算ハードウェア上で効率的に展開することが難しいです。そのため、均一量子化は、そのシンプルさとハードウェアへの効率的な適応性から、現在デファクトスタンダードとなっています。

### G. ファインチューニング方法

量子化後には、ニューラルネットワーク（NN）のパラメータを調整する必要があることがよくあります。これには、モデルを再訓練する「量子化対応トレーニング（QAT）」と、再訓練を行わない「ポストトレーニング量子化（PTQ）」の2つの方法があります。これら2つのアプローチの概略的な比較が図4に示されており、以下でさらに説明されています（このトピックに関する詳細な議論は[183]を参照）。

1) 量子化対応トレーニング（QAT）:
訓練済みモデルでは、量子化によりモデルパラメータに摂動が生じることがあり、これはモデルを浮動小数点精度で訓練されたときに収束した点から離れさせる可能性があります。これに対処するために、量子化されたパラメータでニューラルネットワークモデルを再訓練し、より良い損失関数で収束できるようにすることが可能です。一般的な手法は量子化対応トレーニング（QAT）で、通常のフォワードパスおよびバックワードパスが浮動小数点で量子化モデルに対して行われ、各勾配更新後にモデルパラメータが量子化されます（投影勾配降下に類似）。特に、浮動小数点精度で重みの更新を行った後に、この投影を行うことが重要です。量子化された精度で勾配を累積すると、ゼロ勾配や誤差の大きい勾配が生じることがあり、特に低精度の場合はその影響が顕著です [42, 80, 81, 107, 159, 186, 204, 231]。

バックプロパゲーションにおける重要な微妙な点は、非微分可能な量子化演算子（式2）の扱い方です。近似なしでは、この演算子の勾配はほぼ全ての場所でゼロになります。これは、式2の丸め操作が区分的に平坦な演算子であるためです。この問題に対処するために、STE（ストレートスルー推定器） [13] と呼ばれる手法でこの演算子の勾配を近似するのが一般的です。STEは基本的に丸め操作を無視し、それを恒等関数で近似します（図5に示されています）。

STEの大まかな近似にもかかわらず、実際には多くの場合うまく機能します。ただし、バイナリ量子化のような超低精度の量子化では例外です [8]。[271] の研究では、この現象の理論的な正当化が提供されており、STEの粗い勾配近似が、適切なSTEの選択をすれば、期待値で集団勾配と相関することがわかっています。歴史的な観点から見ると、STEの元々のアイデアは [209, 210] の初期の研究にまで遡り、そこでバイナリニューロンの勾配を近似するために恒等演算子が使用されました。
STE（ストレートスルー推定器）が主流のアプローチである一方 [226, 289]、他のアプローチも文献で検討されています [2, 25, 31, 59, 144, 164]。まず、[13] ではSTEの代替として確率的ニューロンアプローチも提案されていることを挙げるべきです（これはセクションIII-Hで簡単に説明されています）。他にも、組み合わせ最適化 [65]、ターゲット伝播 [140]、あるいはGumbel-softmax [116] を使用するアプローチが提案されています。さらに、別のクラスの代替手法として、正則化演算子を使用して重みを量子化することを強制する方法もあります。これにより、式2の非微分可能な量子化演算子を使用する必要がなくなります。これらはしばしば非STEメソッドと呼ばれます [4, 8, 39, 99, 144, 184, 283]。

この分野の最新の研究には、量子化式の式2における丸め操作を削除し、代わりに「W型」の非滑らかな正則化関数を使用して重みを量子化された値に強制するProxQuant [8] があります。他の注目すべき研究には、不連続点の導関数を近似するためにパルストレーニングを使用する方法 [45] や、量子化された重みを浮動小数点パラメータと量子化されたパラメータのアフィン結合に置き換える方法 [165] があります。最近の研究 [181] では、最も近い値への丸めに代わる適応的丸め法であるAdaRoundが提案されています。

この分野の興味深い研究にもかかわらず、これらの方法は多くの調整を必要とすることが多く、これまでのところSTEアプローチが最も一般的に使用されている方法です。

モデルパラメータの調整に加えて、QAT中に量子化パラメータを学習することが効果的であると判明した研究もいくつかあります。PACT [36] では、均一な量子化の下で活性化のクリッピング範囲を学習し、QIT [125] は非均一な量子化設定を拡張して、量子化ステップとレベルを学習します。LSQ [56] はQAT中に非負の活性化（例：ReLU）のスケーリング係数を学習するための新しい勾配推定を導入し、LSQ+ [15] はこのアイデアをさらに拡張して、swish [202] やh-swish [100] などの負の値を生成する一般的な活性化関数にも対応します。

まとめ（QAT）
QATは、STEの粗い近似にもかかわらず効果があることが示されています。ただし、QATの主な欠点は、ニューラルネットワークモデルの再トレーニングにかかる計算コストです。特に低ビット精度の量子化では、精度を回復するために数百エポックにわたる再トレーニングが必要になることがあります。量子化されたモデルが長期間にわたってデプロイされる場合、効率と精度が特に重要であるならば、この再トレーニングへの投資は価値があると考えられます。しかし、すべてのモデルがそうではなく、いくつかのモデルは比較的短命であることもあります。次に、このオーバーヘッドを持たない代替アプローチについて説明します。

2) **Post-Training Quantization (PTQ)**: 高価なQATメソッドに代わる手法として、再トレーニングを行わずに量子化と重みの調整を行う **Post-Training Quantization (PTQ)** があります [11, 24, 40, 60, 61, 68, 69, 89, 108, 142, 148, 174, 182, 223, 281]。このため、PTQのオーバーヘッドは非常に低く、しばしば無視できるほどです。QATとは異なり、再トレーニングに十分な量のトレーニングデータが必要ではなく、PTQはデータが限られている場合やラベル付けされていない場合にも適用できるという利点があります。しかし、特に低精度の量子化では、QATと比較して精度が低下することが多いです。

このため、PTQの精度低下を軽減するために、さまざまなアプローチが提案されています。例えば、[11, 63] では量子化後の重みの値に固有のバイアス（平均値や分散）を観察し、バイアス補正方法を提案しています。また、[174, 182] では異なる層やチャネル間で重み範囲（および暗黙的に活性化範囲）を均等化することで、量子化エラーを減らせることを示しています。ACIQ [11] は、PTQのために最適なクリッピング範囲とチャネルごとのビット幅設定を解析的に計算します。ACIQは精度の低下を抑えることができますが、ACIQで使用されるチャネルごとの活性化量子化はハードウェア上で効率的に展開するのが困難です。これに対応するために、OMSEメソッド [40] では、活性化のチャネルごとの量子化を削除し、量子化テンソルと対応する浮動小数点テンソル間のL2距離を最適化することでPTQを実施することを提案しています。

さらに、PTQにおける外れ値の悪影響をより軽減するために、[281] では外れ値を含むチャネルを複製して半分に分割する外れ値チャネル分割 (OCS) メソッドが提案されています。もう1つ注目すべき研究はAdaRound [181]で、量子化のためにナイーブな「最も近い整数に丸める」方法が直感に反して最適な結果をもたらさない場合があることを示し、損失をよりよく減らす適応的な丸め方法を提案しています。AdaRoundは、量子化された重みの変化を浮動小数点精度の値から±1以内に制限するのに対し、AdaQuant [108] は量子化された重みが必要に応じて変化できるより一般的な方法を提案しています。

PTQスキームは極端な状況にも適用され、量子化中にトレーニングデータもテストデータも使用しない場合（ゼロショットシナリオとして知られる）については、次のセクションで議論されます。

まとめ（PTQ）
PTQでは、ニューラルネットワークモデルの再トレーニングなしで、すべての重みと活性化の量子化パラメータが決定されます。そのため、PTQはニューラルネットワークモデルを量子化する非常に迅速な方法です。ただし、QATと比較して精度が低下することが多いです。

3) **Zero-shot Quantization (ZSQ)**: これまでに議論されたように、量子化後の精度劣化を最小限に抑えるためには、トレーニングデータ全体またはその一部にアクセスする必要があります。まず、活性化の範囲を知る必要があり、これにより値をクリップして適切なスケーリング係数を決定します（文献ではこれをキャリブレーションと呼びます）。第二に、量子化されたモデルは、モデルパラメータを調整して精度の劣化を回復するために、しばしば微調整が必要です。しかし、多くの場合、量子化手順中に元のトレーニングデータにアクセスすることができません。これは、トレーニングデータセットが大きすぎて配布できない場合や、機密性がある（例えば、GoogleのJFT-300M）、またはセキュリティやプライバシーの懸念からアクセスが制限されている（例: 医療データ）場合です。この課題に対処するためにいくつかの異なる方法が提案されており、これを**ゼロショット量子化 (Zero-shot Quantization, ZSQ)** と呼びます。 [182] に触発されて、ここではZSQの2つの異なるレベルを説明します。

- **レベル 1**: データなし、微調整なし（ZSQ + PTQ）。
- **レベル 2**: データなし、微調整が必要（ZSQ + QAT）。

レベル1では、微調整なしでより速く簡単に量子化を行うことができます。微調整は一般に時間がかかり、追加のハイパーパラメータの探索が必要となることが多いです。しかし、レベル2では通常、より高い精度が得られます。微調整により、特に超低ビット精度の設定で量子化モデルの精度劣化が回復します [85]。[182] の研究では、レベル1アプローチを使用して、データや微調整を行わずに、重みの範囲を均等化し、バイアスエラーを修正して量子化に適したニューラルネットワーク（NN）モデルを作成しています。しかし、この方法は（部分的に）線形の活性化関数のスケール不変性の特性に基づいているため、BERT [46] のGELU [94] 活性化やMobileNetV3 [100] のswish活性化 [203] のような非線形活性化を持つNNには最適ではありません。

ゼロショット量子化（ZSQ）の人気のある研究分野は、ターゲットの事前学習モデルがトレーニングされた実際のデータに類似した合成データを生成することです。この合成データは、量子化モデルのキャリブレーションや微調整に使用されます。この分野の初期の研究 [28] では、合成データ生成のためにGAN（生成敵対ネットワーク）[75] を活用しています。事前学習モデルを識別器として使用し、その出力をうまく分類できるように生成器をトレーニングします。次に、生成器から収集された合成データサンプルを使用して、量子化モデルを全精度モデルからの知識蒸留で微調整します（詳細はセクションIV-Dを参照）。しかし、この方法では、実際のデータの内部統計（中間層の活性化の分布など）を正確に捉えることができません。これは、モデルの最終出力のみを使用してデータが生成されるためです。内部統計を考慮しない合成データは、実際のデータ分布を適切に表現できない可能性があります [85]。これに対処するため、後続の研究では、Batch Normalization（バッチ正規化）[112] に保存された統計、つまりチャネルごとの平均と分散を使用して、より現実的な合成データを生成しています。特に [85] では、内部統計のKLダイバージェンスを直接最小化することでデータを生成し、合成データを使用して量子化モデルをキャリブレーションし、微調整します。さらに、ZeroQ [24] は、合成データを感度測定やキャリブレーションに使用できることを示しており、これにより、トレーニング/検証データにアクセスせずに混合精度の後処理量子化が可能になります。ZeroQ はまた、データ生成時に出力ラベルに依存しないため、ZSQを物体検出タスクにも拡張しています。 [85] と [24] の両方は、入力画像をトレーニング可能なパラメータとして設定し、内部統計が実際のデータの統計に類似するまで直接バックプロパゲーションを行います。さらに進んだ研究では、[37, 90, 259] が、実際のデータ分布をよりよく捉え、より現実的な合成データを生成できる生成モデルのトレーニングと活用が効果的であることを発見しています。

**まとめ（ZSQ）**: ゼロショット（データなし）量子化は、トレーニング/検証データに一切アクセスせずに、量子化を完全に実行します。これは、Machine Learning as a Service（MLaaS）プロバイダーにとって、顧客のワークロードの展開を迅速化し、データセットにアクセスする必要がないため、特に重要です。また、セキュリティやプライバシーの問題でトレーニングデータへのアクセスが制限される場合にも重要です。

### H. 確率的量子化

推論時には、通常、量子化方式は決定論的（デターミニスティック）です。しかし、これが唯一の方法というわけではなく、一部の研究では、量子化に対応したトレーニングや低精度トレーニングのために、確率的量子化が探求されています [13, 79]。高レベルの直感としては、確率的量子化により、決定論的量子化と比べてニューラルネットワーク（NN）がより多く探索できる可能性があるというものです。支持されている一般的な主張の1つは、小さな重みの更新が、丸め操作によって常に同じ重みを返すため、重みの変化をもたらさないかもしれないというものです。しかし、確率的な丸めを有効にすることで、NNがその制約を逃れてパラメータを更新する機会が提供されるかもしれません。

より正式には、確率的量子化は、重みの更新の大きさに関連する確率に基づいて浮動小数点数を切り上げまたは切り下げします。例えば、[29, 79] では、式2の Int 演算子が以下のように定義されています。

$$
\begin{equation}
\text{Int}(x) =
    \begin{cases}
      \lfloor x\rfloor  \text{with probability} \lceil x\rceil - x\\
      \lceil x\rceil \text{with probability} x - \lfloor x\rfloor
    \end{cases}
\end{equation}
$$

しかし、この定義はバイナリ量子化には使用できません。そのため、[42] はこれを次のように拡張しています。

$$
\begin{equation}
\text{Binary}(x) =
    \begin{cases}
      -1  \text{with probability} 1 - \sigma(x) \\
      +1 \text{with probability} \sigma(x)
    \end{cases}
\end{equation}
$$

ここで、Binaryは実数値xをバイナリ化する関数であり、σ(·) はシグモイド関数です。

最近では、QuantNoise [59] で別の確率的量子化手法が導入されました。QuantNoiseは、各フォワードパスで異なるランダムな重みのサブセットを量子化し、バイアスのない勾配でモデルをトレーニングします。これにより、多くのコンピュータビジョンおよび自然言語処理モデルにおいて、精度の大きな低下を伴わずに低ビット精度量子化が可能になります。しかし、確率的量子化手法の主要な課題は、すべての重み更新に対して乱数を生成するオーバーヘッドであり、そのため、実際にはまだ広く採用されていません。