# A Survey of Quantization Methods for Efficient Neural Network Inference(効率的なニューラルネットワーク推論のための量子化手法に関する調査) 

## 概要
デジタルコンピュータ上で数値計算を効率的に行うためには, 数値の表現方法や計算精度の問題が重要です. 特に, 連続する実数値を限られたビット数で離散化する **量子化** は, メモリや計算リソースが限られた状況で重要な課題です. 最近では, ニューラルネットワークの効率化のために低精度な量子化が注目されており, メモリ使用量や処理時間の大幅な削減が可能です. この記事は, この量子化の技術を整理し, 現状と今後の研究課題を評価するための情報を提供します. 

この調査の構成に関しては, まずセクションIIで量子化の簡単な歴史を提供し, その後セクションIIIで量子化の基本的な概念を紹介します. これらの基本的な概念はほとんどの量子化アルゴリズムと共有されており, 既存の方法を理解し展開するために必要です. 次に, セクションIVでより高度なトピックについて議論します. これらは主に最新の最先端の方法, 特に低精度/混合精度量子化に関するものです. 次に, セクションVでハードウェアアクセラレータにおける量子化の影響について, 特にエッジプロセッサに焦点を当てて議論します. 最後に, セクションVIIで要約と結論を提供します. 

## 導入(introduction)
過去10年間でニューラルネットワーク(NN)の精度は大幅に向上しましたが, 非常に多くのパラメータを持つ大規模モデルが主に使われています. これにより, リソース制約のある環境でのリアルタイム推論や低エネルギー消費が難しくなり, 効率的な展開が課題となっています. 特に, ヘルスケアモニタリングや自動運転, 音声認識などのアプリケーションにおいては, 最適な精度と効率性を両立するNNの設計やトレーニングが求められています. この問題に対処するため, 多くの研究が進められており, 大きく以下のように分類できます. 

### 1. 効率的なアーキテクチャ設計
この研究の一つの流れは, ニューラルネットワーク(NN)モデルのアーキテクチャを最適化することに焦点を当てています. 具体的には, **マイクロアーキテクチャ**(例：深層畳み込み, 低ランク因数分解などのカーネル設計)や**マクロアーキテクチャ**(例：残差モジュール, インセプションモジュールなどのモジュール設計)の観点から最適化が行われています. 従来は手動で最適なアーキテクチャを探す方法が主流でしたが, これは効率が悪いため, 新しい流れとして **AutoML(自動機械学習)** や **ニューラルアーキテクチャ検索(NAS)** の手法が登場しています. これらは, モデルのサイズ, 深さ, 幅などの制約の下で, 最適なNNアーキテクチャを自動的に見つけることを目的としています. 
- **AutoML(自動機械学習)**：データ収集から機械学習モデルの生成までを自動化し最適化する技術. 例えば, IBMではAutoAIを提供しており, 特徴量エンジニアリングと選択, 機械学習アルゴリズムのタイプの選択, ハイパーパラメータの最適化などを自動化してくれる.
- **ニューラルアーキテクチャ検索(NAS)**：NNの構造を自動で探索・設計する技術で, 最適なアーキテクチャを発見するプロセスを効率化します. 

[参考サイト](https://cloud-ace.jp/column/detail318/)

### 2. NNアーキテクチャとハードウェアの共同設計
この研究の流れでは, **ニューラルネットワーク(NN)アーキテクチャ** と **ハードウェア** を **共同設計(コーデザイン)** し, 特定のハードウェアプラットフォームに最適化することが重要とされています. これは, NNの処理速度やエネルギー効率が使用するハードウェアに大きく依存するためです. 例えば, あるハードウェアはメモリ帯域幅に依存する操作が得意ですが, 別のハードウェアではエネルギー効率や処理速度に優れた特性を持っています. これに対応するため, NNモデルの設計とハードウェアの設計を合わせることで, リアルタイム処理やエネルギー効率を最大限に引き出すことができます. 初期のアプローチは手動で行われていましたが, 現在では **AutoML** や **ニューラルアーキテクチャ検索(NAS)** を用いた自動化技術が登場し, モデルのサイズ, 深さ, 幅, 計算コストを制約として設定し, 最適なNNアーキテクチャを自動的に探索・設計することが可能になっています. 

### 3. プルーニング
プルーニングは, ニューラルネットワーク(NN)のメモリ使用量や計算コストを削減するために使われ, 感度の低いニューロンを削除して, 計算グラフをスパース(疎)化する技術です. 感度が低いニューロンは, 削除してもモデルの性能にほとんど影響を与えないものです. プルーニングには2つのタイプがあります.

![](2024-09-13-13-26-24.png)
![](2024-09-13-13-34-30.png)

1. **非構造化プルーニング**：感度の低い個々のニューロンを削除し, 多くのパラメータを取り除けますが, 疎行列操作が必要になり, 効率的な計算が難しい. 
2. **構造化プルーニング**：畳み込みフィルターなどのパラメータのグループを削除し, 計算の効率性を保ちますが, 積極的にプルーニングを行うと精度が大幅に低下することがあります. 

プルーニングを効果的に行い, スパース性を保ちながら高精度を維持することは依然として難しい課題です. 

[参考サイト](https://iblog.ridge-i.com/entry/2021/02/24/115105)

### 4. 知識蒸留
大規模なモデルを「教師」として使用し, それをもとによりコンパクトな「生徒」モデルをトレーニングする手法です. 従来の「ハード」なクラスラベルではなく, 教師モデルが生成する「ソフト」な確率を活用することで, 入力に関するより多くの情報を生徒モデルに伝えます. しかし, 知識蒸留単独ではモデルの大幅な圧縮は難しく, 量子化やプルーニングと比較すると, 圧縮に伴う精度低下が問題になります. しかし, 知識蒸留を量子化やプルーニングと組み合わせることで, 成功例が報告されています. 

![](2024-09-13-14-55-51.png)

1.	**教師モデルのトレーニング**
大規模で高精度なモデル(教師モデル)をデータでトレーニングします. このモデルは, 最終的に生徒モデルに知識を伝える役割を果たします. 
2.	**ソフトターゲットの使用**
通常の「ハード」なラベル（例：クラスの正解）ではなく, 教師モデルの出力する「ソフト」な確率（各クラスに対する確率分布）を生徒モデルに学習させます. このソフトな確率は, 教師モデルが出した推論結果をより多くの情報として含んでいます. 
例えば, 私たちは「犬」というと「目があって耳があって足があって尻尾があって、、、」と思い浮かべることができますが, 学習データは入力画像と「これは犬である」という情報のみで,そういった情報は得られませんが, 未知のデータに対しても良い認識精度を持つ教師モデルであれば、こういった知識を持っているはずである. 「犬とは、、」といった一般的な概念を得ていることが期待でき, 学習ずみの教師モデルの持つ入力画像のどこに注目すれば良いかといった知識が学習データには含まれない重要な部分である. 教師モデルの出力であるスコア分布にはこういった構造化された知識が反映されているはずなので, それを生徒モデルに学習させる.
3.	**生徒モデルのトレーニング**
教師モデルからのソフトターゲットを使って, より小さい生徒モデルをトレーニングします. 具体的には, 教師出力をソフトターゲットとして, 生徒の出力の分布がこれと近くなるような損失(soft target loss)を学習に利用する.
4.	**損失関数の設定**
生徒モデルの損失関数は, ソフトターゲットと実際の「ハード」な正解ラベルの両方を使用します. これにより, 生徒モデルが精度を保ちながら圧縮されます. 

[参考サイト](https://codecrafthouse.jp/p/2018/01/knowledge-distillation/)

### 5. 量子化
量子化は, ニューラルネットワーク(NN)モデルのトレーニングと推論の効率化に大きな成功を収めている技術です. 特に, 半精度や混合精度のトレーニングにより, AIアクセラレーターのスループット(ニューラルネットワークのトレーニングや推論において, アクセラレーターがどれだけ多くのデータを処理できるか, またはどれだけ速く計算を完了できるか)を大幅に向上させました. ただし, 半精度以下への移行は難しく, 最近の研究は主に推論に焦点を当てています. 
- 半精度（16ビット）や混合精度は, 計算コストを抑えつつモデルの性能を維持できる技術です. 

### 6. 量子化と神経科学
ニューラルネットワーク(NN)の量子化に関連する神経科学の研究では, 人間の脳が情報を連続的ではなく, 離散的(量子化された)形式で保存することを表しているものがあります. 理由としては, 連続的な情報はノイズ(熱やシナプスのノイズなど)によって破損しやすい一方, 離散的な情報表現はノイズに対してより頑健であることが挙げられています. また, 離散的表現は高い一般化能力や効率的な情報処理を可能にするとも考えられています.