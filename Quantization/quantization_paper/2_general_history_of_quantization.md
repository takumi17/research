# A Survey of Quantization Methods for Efficient Neural Network Inference(効率的なニューラルネットワーク推論のための量子化手法に関する調査)  

## GENERAL HISTORY OF QUANTIZATION(量子化の一般的な歴史 )
GrayとNeuhoffは, 1998年までの量子化の歴史について優れた調査を行いました. 量子化は, 大きな(連続的な)入力値を小さな(有限の)出力値にマッピングする方法であり, 丸めや切り捨てが典型例です. 量子化は微積分に関連しており, その技術は1800年代初期から見られます. 1867年には積分の計算に離散化が使用され, 1897年にはShappardが丸め誤差の影響を調査しました. 近年, 量子化はデジタル信号処理で重要な役割を果たし, デジタル形式で信号を表現する際に丸めが使われています. 

1948年にShannonがデジタルコンピュータの出現とともに, コミュニケーションの数学的理論に関する重要な論文を発表しました. 彼は, ロスレス符号化理論において, 事象の確率に応じてビット数を変えることが最適であると提案し, これは可変レート量子化の概念に繋がりました. 特にハフマン符号化がこの考えに基づいています. また, 1959年の研究では, Shannonは歪み率関数とベクトル量子化の概念を紹介し, それが通信アプリケーションに実用化されました. 量子化に関する他の重要な研究には, パルス符号変調(PCM)や高解像度量子化に関するものが含まれます. 

>- **可変レート量子化** とは, データや信号を量子化する際に, 信号の重要性や特性に応じて異なる精度(ビット数)を使用する方法. 例えば, 画像圧縮では画像の重要な部分(エッジや細部)には高い精度で量子化し, テクスチャが少ない部分には低いビットレートで量子化します.JPEG画像サッシュくがこの考えに基づいています.
>- **ハフマン符号化** は, データの各シンボル(テキストデータを構成する単位)が出現する頻度に基づいて, 各シンボルに異なる長さのビット列(符号)を割り当てます. 頻繁に出現するシンボルには短いコードを, まれにしか出現しないシンボルには長いコードを割り当てます.
>- **歪み率関数**は, 圧縮における歪み(圧縮されたデータと元のデータの間に生じる誤差や違い)の量と, それに必要なビット数(圧縮率)との関係を表す関数. 圧縮アルゴリズムがどれだけのビット数を使用してどれだけの歪みを受け入れるかを評価するために用いる. 損失関数は, 機械学習モデルの性能評価や最適化にすようされ, 歪み率関数はデータ圧縮や信号処理における圧縮後の品質評価に使用される. 
>- **ベクトル量子化** は, データ圧縮技術の一つで, 連続的なデータを離散的なコードブック(事前に定義された離散的なベクトルの集合)を使用して圧縮する方法です. 

量子化は, 連続的な量に対して数値近似を使うアルゴリズムにも現れ, 特にデジタルコンピュータの登場以降, 重要視されるようになりました. 数値解析では, 解が存在し, 一意であり, データに対して連続的に依存する「良条件問題」が重要ですが, 丸め誤差や切り捨て誤差がアルゴリズムの性能に悪影響を与えることがあります. 特に, 実数を有限のビット数で表すことに起因する誤差(量子化)は問題になります. これらの誤差により, アルゴリズムの数値安定性が重要な概念となりました. 

>- **数値解析**は, コンピュータを使って連続的な数学の問題を近似的に解く技術. 
>- **良条件問題**とは, 解が安定しており, 入力データに対して解が連続的に依存する問題のことである. 特徴として解が必ず存在してその解が一意であることと入力データが少し変わっても, 解が少ししか変わらない解の連続性がある. 線形方程式系 $A\bm{x} = \bm{b}$ が両条件であるためには, 逆行列 $A^{-1}$ が存在し, 解 $\bm{x}$ が一意であり, 入力ベクトル $\bm{b}$ の小さな変化が解 $\bm{x}$ に小さな変化を起こす場合. 
>- **丸め誤差**は, コンピュータが有限の桁数でしか数を表せないために発生する誤差で, **IEEE浮動小数点標準**などの規則によって定められています. **切り捨て誤差**は, 反復アルゴリズムで繰り返しの回数が有限なために生じる誤差です. 
>- **数値安定性**は, アルゴリズムが誤差に対してどれだけ強いか, つまり, 誤差が増幅されずに正確な結果に近づけるかを示す概念. 

[参考サイト](https://www.u-tokai.ac.jp/uploads/sites/12/2024/03/Vol.16_2023_PP26-29.pdf)

数値アルゴリズムでは, 入力データ $x$ を「真の」解 $y$ にマッピングする関数 $f(x)$ を実行しようとしますが, 丸め誤差や切り捨て誤差のために, 実際のアルゴリズムの出力は $y^*$ になります. このとき, **前方誤差**は以下のように定義されます：

$$
\Delta y = y^* - y
$$

これは, アルゴリズムの出力 $y^*$ と真の解 $y$ との差を示しています. 

一方, **後方誤差**は, $f(x + \Delta x) = y^*$ を満たす最小の $\Delta x$ で表されます. これは, アルゴリズムが実際にどの入力データ $x + \Delta x$ を解いたのかを示します. 

前方誤差と後方誤差は, **条件数** $K$ によって次のように関連付けられています：

$$
\Delta y \leq K \cdot \Delta x
$$

この式は, 条件数が大きいほど, 入力データのわずかな誤差が出力に大きく影響することを示しています. 

>- **前方誤差**は, アルゴリズムの出力と理論上の正しい解との差です. これによって, どれだけ解がズレているかを測定します. 
>- **後方誤差**は, アルゴリズムが解いたデータが, 実際の入力データからどれだけズレているかを示します. 
>- **条件数**は, 問題がどれだけ誤差に敏感かを表す指標で, 高い条件数は誤差が増幅しやすいことを意味します. 

ニューラルネットワーク(NN)の量子化に関する最近の研究は, 過去の研究と関連性があるものの, NNに特有の課題があります. NNの推論とトレーニングは計算量が多く, 効率的な数値表現が重要です. また, 過剰パラメータ化されたモデルが多いため, ビット精度を下げても性能に大きな影響を与えずに量子化できる可能性があります. さらに, NNは積極的な量子化や離散化にも強い耐性を持っています. 

ニューラルネットアプリケーションでは, 単一の良条件問題を解くのではなく, 分類品質やパープレキシティといった前方誤差に基づく指標に焦点を当てています. 過剰パラメータ化のおかげで, 量子化による誤差が大きくても, 良い一般化性能を維持できます. 従来の研究は信号の圧縮や数値計算の正確さに重きを置いていましたが, ニューラルネット量子化では新たな技術が必要とされています. また, ニューラルネットの階層構造により, 層ごとに異なる精度を適用する「混合精度アプローチ」が可能です. 

>- **過剰パラメータ化**により, ニューラルネットは量子化による誤差に強く, 異なるモデルでも同じような精度を達成できる自由度があります. 
>- **混合精度アプローチ**とは, ニューラルネットの異なる層に異なる量子化精度を適用し, 計算コストを下げながらも性能を維持する方法です. 