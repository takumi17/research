# A Survey of Quantization Methods for Efficient Neural Network Inference(効率的なニューラルネットワーク推論のための量子化手法に関する調査) 

# 目次
- [A Survey of Quantization Methods for Efficient Neural Network Inference(効率的なニューラルネットワーク推論のための量子化手法に関する調査)](#a-survey-of-quantization-methods-for-efficient-neural-network-inference効率的なニューラルネットワーク推論のための量子化手法に関する調査)
- [目次](#目次)
  - [BASIC CONCEPTS OF QUANTIZATION(量子化の基本概念)](#basic-concepts-of-quantization量子化の基本概念)
    - [A. Problem Setup and Notations (問題設定と表記法)](#a-problem-setup-and-notations-問題設定と表記法)
    - [B. Uniform Quantization (一様量子化)](#b-uniform-quantization-一様量子化)
    - [C. . Symmetric and Asymmetric Quantization (対称的および非対称的量子化)](#c--symmetric-and-asymmetric-quantization-対称的および非対称的量子化)
      - [要約(対称的量子化 vs 非対称的量子化)](#要約対称的量子化-vs-非対称的量子化)
    - [D. Range Calibration Algorithms: Static vs Dynamic Quantization (範囲キャリブレーションアルゴリズム: 静的 vs 動的量子化)](#d-range-calibration-algorithms-static-vs-dynamic-quantization-範囲キャリブレーションアルゴリズム-静的-vs-動的量子化)
      - [要約(動的量子化 vs 静的量子化)](#要約動的量子化-vs-静的量子化)
    - [E. Quantization Granularity(量子化の粒度)](#e-quantization-granularity量子化の粒度)
      - [要約(量子化の粒度)](#要約量子化の粒度)
    - [F. Non-Uniform Quantization(非一様量子化)](#f-non-uniform-quantization非一様量子化)
    - [G. Fine-tuning Methods(ファインチューニング方法)](#g-fine-tuning-methodsファインチューニング方法)
    - [H. Stochastic Quantization(確率的量子化)](#h-stochastic-quantization確率的量子化)
      - [参考サイト](#参考サイト)


## BASIC CONCEPTS OF QUANTIZATION(量子化の基本概念) 
このセクションでは, まずセクションIII-Aで一般的な記法と問題の設定を簡単に紹介し, その後セクションIII-BからIII-Fで基本的な量子化の概念と方法について説明します. その後, セクションIII-Gで異なるファインチューニング方法について議論し, セクションIII-Hで確率的量子化について説明します.

### A. Problem Setup and Notations (問題設定と表記法)
ニューラルネットワーク(NN)が $L$ 層の学習可能なパラメータ $\{W_{1}, W_{2}, \ldots, W_{L}\}$ を持ち, $\theta$ がこれら全てのパラメータの組み合わせを示すと仮定します. 一般性を失わずに, ここでは教師あり学習の問題に焦点を当てます. その目的は, 次の経験損失最小化関数を最適化することです. 

$$
\mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^{N}\mathcal{l}(x_{i}, y_{i};\theta)
$$

- $(x, y)$ は入力データと対応するラベル
- $\mathcal{l}(x, y; θ)$ は損失関数(例えば, 平均二乗誤差や交差エントロピー損失) 
- $N$ はデータポイントの総数
- $i^{th}$ 番目の層の入力隠れアクティベーションを $h_{i}$, 対応する出力隠れアクティベーションを $a_{i}$ 
トレーニング済みのモデルパラメータ $\theta$ は浮動小数点精度で保存されていると仮定します. 量子化の目的は, パラメータ $(\theta)$ と中間アクティベーションマップ(すなわち, $h_{i}$ と $a_{i}$)の両方の精度を低精度に減少させ, モデルの一般化能力や精度に最小限の影響を与えることです. これを実現するためには, 浮動小数点値を量子化された値にマッピングする量子化演算子を定義する必要があります. これについては次に説明します. 

>**期待損失と経験損失の違い**
>| 項目 | 期待損失最小化 | 経験損失最小化 |
>|:---|:---|:---|
>| **目的** | 真のデータ分布$P(x, y)$に基づいて損失を最小化 | 訓練データに基づいて損失を最小化 |
>| **数式** | $\mathcal{R}(\theta) = \mathbb{E}_{(x, y) \sim P(x, y)}[\mathcal{l}(x, y; \theta)]$ | $\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \mathcal{l}(x_i, y_i; \theta)$ |
>| **データ** | 真のデータ分布全体 | 限定された訓練データ |
>| **実現可能性** | 理想的だが現実には不可能(真の分布が不明) | 実際に用いられる方法 |
>| **目的の違い** | 理論上の最適化 | 実務上の最適化 |

### B. Uniform Quantization (一様量子化)
まず, ニューラルネットワーク(NN)の重みとアクティベーションを有限の値の集合に量子化できる関数を定義する必要があります. この関数は, 浮動小数点の実数値を取り, それを低精度の範囲にマッピングします. これについては図1で示されています. 

![](2024-09-26-16-34-58.png)
**図1** : 一様量子化(左)と非一様量子化(右)の比較. 連続ドメインの実値 $r$ が, 量子化ドメインの離散的で低精度の値 $Q$ にマッピングされており, これらの値はオレンジの点で示されています. 一様量子化では量子化レベル間の距離は同じですが, 非一様量子化では距離が異なることがあります.

量子化関数の一般的な選択肢は以下の通り

$$
Q(r) = \text{Int}(r/S) - Z
$$

- $Q$ は量子化演算子
- $r$ は実数値の入力(アクティベーションまたは重み)
- $S$ は実数値のスケーリングファクター 
- $Z$ は整数のゼロポイント
- $\text{Int}$ 関数は実数値を整数値に変換するための丸め操作(例えば, 最も近い整数への丸めや切り捨て)
基本的に, この関数は実数値 $r$ をいくつかの整数値にマッピングします. この量子化の方法は一様量子化としても知られており, 結果として得られる量子化値(すなわち量子化レベル)が均等に間隔を空けて配置されます(図1, 左). また, 量子化値が必ずしも均等に配置されていない非一様量子化の方法もあり(図1, 右), これについてはセクション III-F で詳しく説明されます. 量子化された値 $Q(r)$ から実数値 r を回復するための操作は, 一般に dequantization(逆量子化)と呼ばれます. 

$$
\tilde{r} = S(Q(r) + Z)
$$

注意 : 回復された実数値 $\tilde{r}$ は, 丸め操作のために元の実数値 $r$ と正確には一致しないことがあります.


### C. . Symmetric and Asymmetric Quantization (対称的および非対称的量子化)
一様量子化において重要な要素の1つは, 式2におけるスケーリングファクター $S$ の選択です. このスケーリングファクターは, 実数値 r の与えられた範囲を複数の区間に分割します. 

$$
S = \frac{\beta - \alpha}{2^{b} - 1}
$$

- $[\alpha, \beta]$ はクリッピング範囲を示し, これは実数値をクリッピングするための有界範囲 
- $b$ は量子化ビット幅
したがって, スケーリングファクターを定義するためには, まずクリッピング範囲 $[\alpha, \beta]$ を決定する必要があります. クリッピング範囲を選択するプロセスは, 一般的にキャリブレーションと呼ばれます. 

![](2024-09-26-16-36-26.png)
**図2** : 対称的量子化と非対称的量子化の図解. 対称的量子化では制限された範囲が実数値を [-127, 127] にマッピングし, フルレンジでは [-128, 127] にマッピングします(8ビット量子化の場合). 

**非対称的な量子化方式**
簡単な選択肢としては, 信号の最小値および最大値をクリッピング範囲として使用する方法があります. すなわち, $\alpha = r_{min}, \beta = r_{max}$ です. このアプローチは非対称的な量子化方式であり, クリッピング範囲が原点に対して対称でないため, $-\alpha \neq \beta$ になります(図2, 右参照). 

**対称的な量子化方式**
対称的な量子化方式を使用することも可能で, $\alpha = -\beta$ の対称的なクリッピング範囲を選択します. 一般的な選択肢としては, 信号の最小値および最大値に基づいて選ぶ方法があります: $−\alpha = \beta = max(|r_{max}|, |r_{min}|)$. 

非対称的な量子化は, 対称的な量子化に比べてクリッピング範囲がより狭くなることがよくあります. これは, 特にターゲットとなる重みやアクティベーションが不均衡である場合(例えば, ReLUの後のアクティベーションが常に非負の値を持つ場合)に重要です. しかし, 対称的な量子化を使用すると, 式2の量子化関数が簡略化され, ゼロポイントを $Z = 0$ に置き換えることができます. 

$$
Q(r) = \text{Int}(\frac{r}{S})
$$

>ReLU関数を通過したアクティベーションでは, 値は常に非負. もしこのような場合で対称量子化を適用すると, 負の範囲に無駄な領域が割り当てられます. つまり, 対称量子化では不必要に広い範囲をカバーしてしまうのに対し, 非対称量子化では最小限の範囲で済みます.

ここでは, スケーリングファクターに対して2つの選択肢があります. $\text{full range}$ 対称量子化では, スケーリングファクター $S$ は $\frac{2 \cdot \max(|r|)}{2^{n-1}}$(切り捨て丸めモード)として選ばれ, $\text{INT8}$ の全範囲 $[-128, 127]$ が使用されます. しかし, $\text{restricted range}$ では, スケーリングファクター $S$ は $\frac{\max(|r|)}{2^{n-1}-1}$ として選ばれ, 範囲は $[-127, 127]$ のみが使用されます. 予想通り, フルレンジアプローチの方がより正確です. 

>**対称的量子化と非対称的量子化と $\text{full range}$ と $\text{restricted range}$ の関係性**
**対称的量子化**
量子化の範囲がゼロを中心に対称に設定されます.
$−\alpha = \beta = max(|r_{max}|, |r_{min}|)$
例: 実数値を $\text{INT8}$ の範囲でスケーリングする場合
$\text{full range}$ 対称量子化は, データの最大値と最小値をそのまま量子化範囲に使用します.
$S = \frac{2 \cdot \max(|r|)}{2^{n-1}}$ の範囲は $[-128, 127]$
$\text{restricted range}$ 対称量子化は, 数値の範囲をさらに制限します.
$S = \frac{2 \cdot \max(|r|)}{2^{n-1} -1}$ の範囲は $[-127, 127]$
**非対称的量子化**
量子化範囲はゼロを中心に対称である必要はなく, データの最小値と最大値に基づいて決定されます.
$\alpha = r_{min}, \beta = r_{max}$
$\text{full range}$ 非対称量子化は, データの最小値と最大値を量子化範囲としてそのまま使う場合を指します.
$S = \frac{2 \cdot \max(|r|)}{2^{n-1}}$ の最大範囲は $[-128, 127]$
$\text{restricted range}$ 非対称量子化は, 範囲を制限する.
$S = \frac{2 \cdot \max(|r|)}{2^{n-1} -1}$ の範囲は $[-127, 127]$
元のデータ範囲(たとえば $[-1000, 1000]$ など)を $[-128, 127]$ に収めるようにスケーリングしますが, スケーリングされた結果, マッピングされる値が常に $-128$ や $127$ になるわけではありません. むしろ, $r$ の最大値が $127$ に近い値になり, 最小値が $-128$ に近い値に変換されます. 


対称的量子化は, ゼロポイントをゼロにすることで推論中の計算コストを削減できるため, 重みの量子化において広く採用されています [255]. また, 実装がより簡単になるという利点もあります. ただし, アクティベーションについては, 非対称アクティベーションのオフセットによって生じる交差項は静的なデータ非依存項であり, バイアスに吸収することができます(またはアキュムレータの初期化に使用することができます) [15]. 

>非対称的量子化では, ゼロポイントが $0$ ではなくなるため, データ全体に「オフセット」と呼ばれる調整値が加わります. このオフセットにより計算の複雑さが増すように思えますが, 実際には問題はそれほど大きくありません. その理由は, 非対称量子化のオフセットによって生じる「交差項」(つまり計算式における余分な項)は, データに依存せず静的なものです. このため, 計算の途中で新たに調整するのではなく, ネットワークのバイアス(ニューラルネットワークの重み以外に加えられる値)に吸収させることができるという意味です. また, アキュムレータの初期化にこのオフセットを使って計算を最適化することもできます.
**ゼロポイント**
ゼロポイントとは, 量子化された値の中で元のデータの「 $0$ 」に対応する値のことです. 通常, 量子化の範囲(例えば, $8$ ビット量子化なら $0$ から $255$ の間)内で, どの値が「 $0$ 」に対応するかを定める基準点です. 
**オフセット**
オフセットは, ゼロポイントが「 $0$ 」でない場合に, ゼロからのずれを表すものです. つまり, オフセットが導入されるのは非対称的量子化のケースです.
オフセットとゼロポイントは, 基本的に同じものであり, 非対称量子化においてゼロポイントがゼロでない場合に, そのずれをオフセットと呼ぶことがあります.
**アキュムレータ**
ニューラルネットワークや量子化の文脈では, アキュムレーターは, 通常, 推論や学習中に演算結果を一時的に保持する役割を担います. 特に量子化の場合, 低精度の数値(例えば, $8$ ビット整数)を使って演算を行うため, 累積誤差や計算精度の管理が重要です. このとき, アキュムレーターはより高い精度(通常は $32$ ビットや $64$ ビットの精度)で保持することで, 量子化による精度の損失を防ぎながら計算を進めます.

信号の最小値/最大値を使用する方法は, 対称的および非対称的量子化で一般的ですが, このアプローチはアクティベーションにおける外れ値に対して脆弱です. 外れ値が範囲を不必要に広げ, その結果, 量子化の解像度が低下する可能性があります. この問題に対処するための $1$ つのアプローチは, 信号の最小値/最大値の代わりにパーセンタイルを使用することです [172]. つまり, 最大値/最小値の代わりに, $i$ 番目に大きい/小さい値を $\beta/\alpha$ として使用します. 別のアプローチは, 実値と量子化された値の間の $\text{KL}$ ダイバージェンス(情報損失)を最小化するように $\alpha$ と $\beta$ を選択することです [176]. 興味のある読者には, さまざまなモデルで異なるキャリブレーション方法が評価されている [255] を参照することをお勧めします. 

#### 要約(対称的量子化 vs 非対称的量子化)
対称的量子化は対称的な範囲を使用してクリッピングを分割します. これにより, 式2で $Z = 0$ となるため, 実装が容易になります. ただし, 範囲が歪んで対称でない場合には最適ではありません. このような場合には, 非対称的量子化が推奨されます. 

### D. Range Calibration Algorithms: Static vs Dynamic Quantization (範囲キャリブレーションアルゴリズム: 静的 vs 動的量子化)
$[α, β]$ のクリッピング範囲を決定するためのさまざまな調整方法について議論しました. 量子化方法のもう一つの重要な違いは, クリッピング範囲が決定されるタイミングです. この範囲は, 重み(ウェイト)については静的に計算することができます. ほとんどの場合, 推論時にはパラメータが固定されているためです. しかし, 活性化マップ(式1における $x$ )は各入力サンプルごとに異なります. そのため, 活性化関数の量子化には, 動的量子化と静的量子化という2つのアプローチがあります. 

**動的量子化**
動的量子化では, この範囲が実行時に各活性化マップごとに動的に計算されます. このアプローチでは, 信号の統計情報(最小値, 最大値, パーセンタイルなど)をリアルタイムで計算する必要があり, 非常に大きなオーバーヘッドが発生する可能性があります. しかし, 動的量子化は各入力に対して信号範囲を正確に計算するため, より高い精度を得ることがよくあります. 

**静的量子化**
もう一つの量子化アプローチは静的量子化です. この方法では, 推論時にクリッピング範囲が事前に計算され, 固定されます. このアプローチは計算負荷を追加することはありませんが, 動的量子化と比較して通常は精度が低くなります. 一般的な事前計算方法として, キャリブレーション用の入力データを一連に渡して, 典型的な活性化範囲を計算することがあります [113, 267]. 最適な範囲を見つけるために提案されているさまざまな指標には, 元の非量子化重み分布と対応する量子化値との間の平均二乗誤差 (MSE) を最小化することが含まれています [40, 221, 229, 281]. 他にもエントロピー [189] などの指標を使うことも考えられますが, MSEが最も一般的に使用される方法です. もう一つのアプローチとして, ニューラルネットワーク (NN) のトレーニング中にこのクリッピング範囲を学習または適用することもあります [36, 146, 276, 287]. ここで注目すべき研究として, LQNet [276], PACT [36], LSQ [56], およびLSQ+ [15] が挙げられ, これらはクリッピング範囲とNN内の重みをトレーニング中に同時に最適化します. 


>| **項目**                | **静的量子化(Static Quantization)**                                                                                              | **動的量子化(Dynamic Quantization)**                                                                            |
>|-------------------------|-------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|
>| **量子化のタイミング**   | 推論前に**事前に量子化**される. スケールとゼロポイントがキャリブレーションデータに基づいて決定される.                                     | **推論時に動的に量子化**される. 重みは事前に量子化されているが, アクティベーションは推論中に量子化される.             |
>| **対象となる要素**       | 重みとアクティベーションの両方が量子化され, 事前に固定されている.                                                                      | 重みは事前に量子化されるが, アクティベーションは推論時に量子化される.                                                |
>| **キャリブレーション**   | キャリブレーションデータを使用して, スケールとゼロポイントを推定し, 固定する必要がある.                                                  | キャリブレーションデータは不要で, モデルの再トレーニングも必要ない.                                                    |
>| **演算効率**             | 重みもアクティベーションも8ビット整数で計算されるため, **非常に高速**.                                                                  | アクティベーションは32ビット浮動小数点で計算され, 重みは8ビット整数. **速度はやや遅い**.                              |
>| **推論時の精度**         | アクティベーションも事前に量子化されているため, **精度はやや低下**することがある.                                                          | アクティベーションが浮動小数点のままなので, **精度の劣化が比較的少ない**.                                               |                            |
>| **トレーニングの必要性**  | トレーニング済みモデルにキャリブレーションが必要だが, 量子化対応トレーニングは不要.                                                          | キャリブレーションも量子化対応トレーニングも不要.                                                                       |
>| **適用の容易さ**         | キャリブレーションデータの準備やモデル調整が必要で, やや手間がかかる.                                                                       | 事前の準備が少なく, **簡単に適用可能**.                                                                              |
>| **量子化の粒度**         | アクティベーションはレイヤー単位で量子化される. **各レイヤーにスケールとゼロポイントが固定**される.                                             | アクティベーションの量子化は, レイヤー間で動的に行われる. **スケールとゼロポイントが動的に適用**される.                     |


>| **手順**                | **静的量子化の手順**                                                                                                          | **動的量子化の手順**                                                                                                     |
>|-------------------------|------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|
>| **1. モデルのトレーニング** | 通常のフル精度(32ビット浮動小数点)のモデルをトレーニングする.                                                                  | 通常のフル精度(32ビット浮動小数点)のモデルをトレーニングする.                                                              |
>| **2. キャリブレーション**   | キャリブレーションデータセットを用意し, 各レイヤーのアクティベーションの範囲(最小値と最大値)を推定. 推論時に使用するためのスケールとゼロポイントを計算する.   | キャリブレーションデータの準備や計算は不要.                                                                                     |
>| **3. 重みの量子化**        | トレーニング済みモデルの**重み**を8ビット整数に量子化.                                                                                | トレーニング済みモデルの**重み**を8ビット整数に量子化.                                                                             |
>| **4. アクティベーションの量子化** | **推論前に**各レイヤーのアクティベーションを8ビット整数に量子化し, スケールとゼロポイントを固定.                                                  | **推論時に**各レイヤーのアクティベーションを32ビット浮動小数点から動的に量子化し, スケールとゼロポイントをその都度適用.               |
>| **5. 推論**               | 量子化された重みとアクティベーションを使い, 効率的に推論を実行. 整数演算のみで実行されるため, **高速かつメモリ効率が良い**.                           | 量子化された重みと, 推論中に動的に量子化されるアクティベーションを使って推論を実行. **静的量子化ほど高速ではないが, 精度の低下が少ない**.  |

#### 要約(動的量子化 vs 静的量子化)
動的量子化は各活性化のクリッピング範囲を動的に計算し, しばしば最高の精度を達成します. しかし, 信号の範囲を動的に計算することは非常にコストが高いため, 実務では多くの場合, 全ての入力に対してクリッピング範囲が固定される静的量子化が使用されます. 

### E. Quantization Granularity(量子化の粒度)
ほとんどのコンピュータビジョンのタスクでは, 層への活性化入力が多数の異なる畳み込みフィルタと畳み込まれます(図3に示されています). 

![](2024-09-29-16-56-52.png)
図3: 異なる量子化の粒度の図解. レイヤーごとの量子化では, 同じレイヤーに属するすべてのフィルタに対して同じクリッピング範囲が適用されます. これにより, 分布が狭いチャネル(例えば, 図中のフィルタ1)に対しては量子化の解像度が低くなる可能性があります. チャネルごとの量子化を使用すると, 異なるクリッピング範囲を異なるチャネルに割り当てることで, より良い量子化解像度を達成することができます. 

これらの畳み込みフィルタのそれぞれが異なる値の範囲を持つことがあります. そのため, 量子化方法の区別点の一つは, 重みに対してクリッピング範囲 $[α, β]$ がどのように計算されるかという粒度です. これを次のように分類しました. 

**a) 層単位の量子化**
このアプローチでは, 層内のすべての畳み込みフィルタの重みを考慮してクリッピング範囲を決定します [133]. 図3の3列目に示されているように, 層内の全パラメータの統計情報(例えば, 最小値, 最大値, パーセンタイルなど)を調べ, その後, すべての畳み込みフィルタに対して同じクリッピング範囲を使用します. この方法は非常にシンプルに実装できますが, 各畳み込みフィルタの範囲が大きく異なる場合が多いため, 精度が最適でないことがよくあります. 例えば, 比較的狭いパラメータ範囲を持つ畳み込みカーネルが, 同じ層内でより広い範囲を持つ別のカーネルのために量子化解像度を失う可能性があります. 

**b) グループ単位の量子化**
層内の複数の異なるチャンネルをグループ化して, (活性化マップや畳み込みカーネルの)クリッピング範囲を計算する方法があります. これは, 単一の畳み込みまたは活性化内でパラメータの分布が大きく異なる場合に役立ちます. 例えば, このアプローチは, 完全に接続された注意層を含むTransformerモデル [243] の量子化において, Q-BERT [219] で有用であることが示されています. しかし, この方法は異なるスケーリングファクターを考慮する必要があるため, 追加のコストが伴います. 

**c) チャンネル単位の量子化**
一般的な選択肢として, 各畳み込みフィルタに対して, 他のチャンネルとは無関係に固定された値のクリッピング範囲を使用する方法があります [105, 113, 133, 222, 276, 285]. これは図3の最後の列に示されています. つまり, 各チャンネルに専用のスケーリングファクターが割り当てられます. この方法は, より良い量子化解像度を確保し, しばしばより高い精度をもたらします. 

**d) サブチャンネル単位の量子化** 
前述のアプローチをさらに徹底し, 畳み込み層や全結合層のパラメータグループに対してクリッピング範囲を決定することもできます. しかし, この方法では, 単一の畳み込み層や全結合層を処理する際に異なるスケーリングファクターを考慮する必要があるため, かなりのオーバーヘッドが発生する可能性があります. そのため, グループ単位の量子化は, 量子化の精度と計算コストの間で良い妥協点を提供する可能性があります. 

#### 要約(量子化の粒度)
チャンネル単位の量子化は, 現在, 畳み込みカーネルの量子化に使用される標準的な方法です. これは, 各カーネルに対してクリッピング範囲を個別に調整でき, オーバーヘッドがほとんど発生しません. それに対して, サブチャンネル単位の量子化は, かなりのオーバーヘッドを引き起こす可能性があり, 現在の標準的な選択肢ではありません(これらの設計選択に関連するトレードオフについては, [68] を参照してください). 

### F. Non-Uniform Quantization(非一様量子化)

### G. Fine-tuning Methods(ファインチューニング方法)

### H. Stochastic Quantization(確率的量子化)








#### 参考サイト
[浮動小数点数](https://hwb.ecc.u-tokyo.ac.jp/hwb2023/information/coding/number/floating_number/)
[符号付き整数_1](https://www.tohoho-web.com/ex/numerical-format.html)
[符号付き整数_2](https://hwb.ecc.u-tokyo.ac.jp/hwb2023/information/coding/number/integer/)
[期待損失最小化問題](https://orsj.org/wp-content/corsj/or65-12/or65_12_643.pdf)
[損失関数とクロスエントロピー](https://ieyasu03.web.fc2.com/Deep_Learning/5-loss_function.html)
[深層学習ライブラリと量子化](https://lab.mo-t.com/blog/quantization-frameworks)
[カルバック・ライブラリー情報量](https://qiita.com/shuva/items/81ad2a337175c035988f)
